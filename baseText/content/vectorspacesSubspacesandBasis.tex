\section{Subspaces and basis}

\begin{outcome}
  \begin{enumerate}
  \item Utilize the subspace test to determine if a set is a subspace
    of a given vector space.
  \item Extend a linearly independent set and shrink a spanning set to
    a basis of a given vector space.
  \end{enumerate}
\end{outcome}

In this section we will examine the concept of subspaces introduced earlier in terms of $\R^n$. Here, we will discuss these concepts in terms of abstract vector spaces. 

Consider the definition of a subspace.

\begin{definition}{Subspace}{subspace}
Let $V$ be a vector space. A subset $W\subseteq V$ is said to be a \textbf{subspace}\index{subspace} of $V$ if $a\vect{x}+b\vect{y}
\in W$ whenever $a,b\in \R$ and $\vect{x},\vect{y}\in W$.
\end{definition}

The span of a set of vectors as described in Definition \ref{def:span} is an example of a subspace. The following fundamental result says that subspaces are subsets of a
vector space which are themselves vector spaces.

\begin{theorem}{Subspaces are vector spaces}{subspaces-are-vector-spaces}
Let $W$ be a nonempty collection of vectors in a vector space $V$. Then $W$
is a subspace if and only if $W$ satisfies the vector space axioms, using the same
operations as those defined on $V$.
\end{theorem}

\begin{proof}
Suppose first that $W$ is a subspace. It is obvious that
all the algebraic laws hold on $W$ because it is a subset of $V$ and they
hold on $V$. Thus $\vect{u}+\vect{v}=\vect{v}+\vect{u}$ along with the other axioms. Does $W$
contain $\vect{0}$? Yes because it contains $0\vect{u}=\vect{0}$. See
Theorem \ref{thm:axiom-uniqueness}.

 Are the operations of $V$ defined on $W$? That is,
when you add vectors of $W$ do you get a vector in $W$? When you multiply a
vector in $W$ by a scalar, do you get a vector in $W$? Yes. This is
contained in the definition. Does every vector in $W$ have an additive
inverse? Yes by Theorem \ref{thm:axiom-uniqueness} because $-\vect{v}=\tup{
-1} \vect{v}$ which is given to be in $W$ provided $\vect{v}\in W$.

Next suppose $W$ is a vector space. Then by definition, it is closed with
respect to linear combinations. Hence it is a subspace. 
\end{proof}

Consider the following useful Corollary.

\begin{corollary}{Span is a subspace}{span-subspace}
Let $V$ be a vector space with $W \subseteq V$. If $W = \func{span} \set{\vect{v}_1, \cdots,  \vect{v}_n }$ then $W$ is a subspace of $V$.
\end{corollary}

When determining spanning sets the following theorem proves useful.

\begin{theorem}{Spanning set}{spanning-set}
Let $W \subseteq V$ for a vector space $V$ and suppose $W = \func{span} \set{\vect{v}_1, \vect{v}_2, \cdots, \vect{v}_n }$. 

Let $U \subseteq V$ be a subspace such that $\vect{v}_1, \vect{v}_2, \cdots, \vect{v}_n \in U$. Then it follows that $W \subseteq U$. 
\end{theorem}

In other words, this theorem claims that any subspace that contains a set of vectors must also contain the span of these vectors. 

The following example will show that two spans, described differently, can in fact be equal. 

\begin{example}{Equal span}{equal-span}
Let $p(x), q(x)$ be polynomials and suppose $U = \func{span}\set{2p(x) - q(x), p(x) + 3q(x)} $ and $W =  \func{span}\set{p(x), q(x) }$. Show that $U = W$. 
\end{example}

\begin{solution}
We will use Theorem \ref{thm:spanning-set} to show that $U \subseteq W$ and $W \subseteq U$. It will then follow that $U=W$. 
\begin{enumerate}
\item $U \subseteq W$

Notice that $2p(x) - q(x)$ and $p(x) + 3q(x)$  are both in $W = \func{span} \set{p(x), q(x) }$. Then by Theorem \ref{thm:spanning-set} $W$ must contain the span of these polynomials and so $U \subseteq W$. 

\item $W \subseteq U$

Notice that 
\begin{eqnarray*}
p(x) &=& \frac{3}{7} \tup{2p(x) - q(x) }  + \frac{2}{7} \tup{p(x) + 3q(x)} \\
q(x) &=& -\frac{1}{7} \tup{2p(x) - q(x) }  + \frac{2}{7} \tup{p(x) + 3q(x)}
\end{eqnarray*}
Hence $p(x), q(x)$ are in $\func{span} \set{2p(x) - q(x), p(x) + 3q(x) }$. By Theorem \ref{thm:spanning-set} $U$ must contain the span of these polynomials and so $W \subseteq U$. 
\end{enumerate}
\end{solution}

To prove that a set is a vector space, one must verify each of the axioms given in Definition \ref{def:vector-space-axioms-addition} and \ref{def:vector-space-axioms-scalar-mult}. This is a cumbersome task, and therefore a shorter procedure is used to verify a subspace. 

\begin{procedure}{Subspace test}{subspace-test}
Suppose $W$ is a subset of a vector space $V$. To determine if $W$ is a subspace of $V$, it is sufficient to determine if the following three conditions hold, using the operations of $V$: 
\begin{enumerate}
\item
The additive identity $\vect{0}$ of $V$ is contained in $W$.
\item
For any vectors $\vect{w}_1, \vect{w}_2$ in $W$, $\vect{w}_1 + \vect{w}_2$ is also in $W$.
\item
For any vector $\vect{w}_1$ in $W$ and scalar $a$,  the product $a\vect{w}_1$ is also in $W$. 
\end{enumerate}
\end{procedure}

Therefore it suffices to prove these three steps to show that a set is a subspace. 

Consider the following example.

\begin{example}{Improper subspaces}{improper-subspaces}
Let $V$ be an arbitrary vector space. Then $V$ is a subspace of itself. Similarly, the set $\set{\vect{0} }$ containing only the zero vector is also a subspace. 
\end{example}

\begin{solution}
Using the subspace test in Procedure \ref{proc:subspace-test} we can show that $V$ and $\set{\vect{0} }$ are subspaces of $V$. 

Since $V$ satisfies the vector space axioms it also satisfies the three steps of the subspace test. Therefore $V$ is a subspace.

Let's consider the set $\set{\vect{0} }$. 
\begin{enumerate}
\item
The vector $\vect{0}$ is clearly contained in  $\set{\vect{0} }$, so the first condition is satisfied.

\item
Let $\vect{w}_1, \vect{w}_2$ be in  $\set{\vect{0} }$. Then $\vect{w}_1 = \vect{0}$ and $\vect{w}_2 = \vect{0}$ and so 
\[
\vect{w}_1 + \vect{w}_2 = \vect{0} + \vect{0} = \vect{0}
\]
It follows that the sum is contained in $\set{\vect{0} }$ and the second condition is satisfied. 

\item
Let $\vect{w}_1$ be in  $\set{\vect{0} }$ and let $a$ be an arbitrary scalar. Then
\[
a\vect{w}_1  = a\vect{0} = \vect{0}
\]
Hence the product is contained in  $\set{\vect{0} }$ and the third condition is satisfied. 
\end{enumerate}

It follows that  $\set{\vect{0} }$ is a subspace of $V$. 
\end{solution}

The two subspaces described above are called \textbf{improper subspaces}\index{improper subspace}. Any subspace of a vector space $V$ which is not equal to $V$ or  $\set{\vect{0} }$ is called a \textbf{proper subspace}\index{proper subspace}. 

Consider another example.

\begin{example}{Subspace of polynomials}{poly-subspace}
Let $\Poly_2$ be the vector space of polynomials of degree two or less. Let $W \subseteq \Poly_2$ be all polynomials of degree two or less which have $1$ as a root. Show that $W$ is a subspace of $\Poly_2$. 
\end{example}

\begin{solution}
First, express $W$ as follows:
\[
W = \set{p(x) = ax^2 +bx +c, a,b,c, \in \R | p(1)  = 0 }
\]

We need to show that $W$ satisfies the three conditions of Procedure \ref{proc:subspace-test}. 
\begin{enumerate}
\item
The zero polynomial of $\Poly_2$ is given by $0(x) = 0x^2 + 0x + 0 = 0$. Clearly $0(1) = 0$ so $0(x)$ is contained in $W$. 

\item
Let $p(x), q(x)$ be polynomials in $W$.  It follows that $p(1) = 0 $ and $q(1) = 0$. Now consider $p(x) + q(x)$. Let $r(x)$ represent this sum.
\begin{eqnarray*}
r(1) &=& p(1) + q(1) \\
&=& 0 + 0 \\
&=& 0
\end{eqnarray*}

Therefore the sum is also in $W$ and the second condition is satisfied. 

\item
Let $p(x)$ be a polynomial in $W$ and let $a$ be a scalar. It follows that $p(1) = 0$. Consider the product $ap(x)$. 
\begin{eqnarray*}
ap(1) &=& a(0) \\
&=& 0
\end{eqnarray*}

Therefore the product is in $W$ and the third condition is satisfied.
\end{enumerate}

It follows that $W$ is a subspace of $\Poly_2$. 
\end{solution} 

Recall the definition of basis, considered now in the context of vector spaces.

\begin{definition}{Basis}{basis-vector-space}
Let $V$ be a vector space. Then $\{\vect{v}_{1},\cdots ,\vect{v}_{n}\}$ is called a basis\index{basis} for $V$ if the following conditions hold.
\begin{enumerate}
\item
$\func{span}\set{\vect{v}_{1},\cdots ,\vect{v}_{n}} = V$
\item
$\{\vect{v}_{1},\cdots ,\vect{v}_{n}\}$ is linearly independent
\end{enumerate}
\end{definition}

Consider the following example.

\begin{example}{Polynomials of degree two}{poly-degree-two}
Let $\Poly_2$ be the set polynomials of degree no more than 2. We can write
$\Poly_2=\func{span}\set{x^{2}, x, 1}$. Is $\set{x^{2}, x, 1} $ a
basis for $\Poly_2$?
\end{example}

\begin{solution}
It can be verified that $\Poly_2$ is a vector space defined under the usual addition and scalar multiplication of polynomials. 

Now, since $\Poly_2=\func{span}\set{x^{2},x, 1}$, the set  $\set{x^{2}, x, 1} $ is a basis if it is linearly independent. Suppose then that 
\begin{equation*}
ax^{2}+bx+c=0x^2 + 0x + 0 
\end{equation*}
where $a,b,c$ are real numbers. It is clear that this can only occur if $a=b=c=0$. Hence the set is linearly independent and forms a basis of $\Poly_2$.
\end{solution}

The next theorem is an essential result in linear algebra and is called the exchange theorem\index{exchange theorem}.

\begin{theorem}{Exchange theorem}{exchange-theorem}
Let $\set{\vect{x}_{1},\cdots ,\vect{x}_{r}} $
be a linearly independent set of vectors such that each $\vect{x}_{i}$ is
contained in span$\set{\vect{y}_{1},\cdots ,\vect{y}_{s}}$. Then $
r\leq s$.
\end{theorem}

\begin{proof} The proof will proceed as follows. First, we set up the necessary steps for the proof. Next, we will assume that $r > s$ and show that this leads to a contradiction, thus requiring that $r \leq s$. 

Define span$\set{\vect{y}_{1},\cdots ,\vect{y}_{s}} = V$. Since each $\vect{x}_i$ is in  span$\set{\vect{y}_{1},\cdots ,\vect{y}_{s}}$, it follows there exist scalars $c_{1},\cdots ,c_{s}$
such that 
\begin{equation}
\vect{x}_{1}=\sum_{i=1}^{s}c_{i}\vect{y}_{i}  \label{lin-comb}
\end{equation}
Note that not all of these scalars $c_i$ can equal zero. Suppose that all the $c_i=0$. Then it
would follow that $\vect{x}_{1}=\vect{0}$ and so $\set{\vect{x}
_{1},\cdots ,\vect{x}_{r}} $ would not be linearly independent.
Indeed, if $\vect{x}_{1}=\vect{0}$, $1\vect{x}_{1}+\sum_{i=2}^{r}0
\vect{x}_{i}=\vect{x}_{1}=\vect{0}$ and so there would exist a
nontrivial linear combination of the vectors $\set{\vect{x}_{1},\cdots ,
\vect{x}_{r}} $ which equals zero. Therefore at least one $c_i$ is non-zero. 

Say $c_{k}\neq 0$. Then solve \ref{lin-comb} for $\vect{y}_{k}$ and obtain 
\begin{equation*}
\vect{y}_{k}\in \func{span}\set{\vect{x}_{1},\overset{\text{s-1
vectors here}}{\overbrace{\vect{y}_{1},\cdots ,\vect{y}_{k-1},\vect{y}
_{k+1},\cdots ,\vect{y}_{s}}}} .
\end{equation*}
Define $\set{\vect{z}_{1},\cdots ,\vect{z}_{s-1}} $ to be
\begin{equation*}
\set{\vect{z}_{1},\cdots ,\vect{z}_{s-1}} = \set{
\vect{y}_{1},\cdots ,\vect{y}_{k-1},\vect{y}_{k+1},\cdots ,\vect{y}
_{s}}
\end{equation*}
Now we can write 
\begin{equation*}
\vect{y}_{k}\in \func{span}\set{\vect{x}_{1}, \vect{z}_{1},\cdots, \vect{z}_{s-1}} 
\end{equation*}
Therefore, $\func{span}\set{\vect{x}_{1},\vect{z}_{1},\cdots ,\vect{z
}_{s-1}}=V$. To see this, suppose $\vect{v}\in V$. Then there exist constants $
c_{1},\cdots ,c_{s}$ such that 
\begin{equation*}
\vect{v}=\sum_{i=1}^{s-1}c_{i}\vect{z}_{i}+c_{s}\vect{y}_{k}.
\end{equation*}
Replace this $\vect{y}_{k}$ with a linear combination of the
vectors $\set{\vect{x}_{1},\vect{z}_{1},\cdots ,\vect{z}_{s-1}}$
to obtain $\vect{v}\in \func{span}\set{\vect{x}_{1},\vect{z}
_{1},\cdots ,\vect{z}_{s-1}}$. The vector $\vect{y}_{k}$, in the
list $\set{\vect{y}_{1},\cdots ,\vect{y}_{s}}$, has now been
replaced with the vector $\vect{x}_{1}$ and the resulting modified list of
vectors has the same span as the original list of vectors, $\set{\vect{y
}_{1},\cdots ,\vect{y}_{s}} $.

We are now ready to move on to the proof. Suppose that $r>s$ and that 
\[
\func{span}\set{\vect{x}_{1},\cdots ,
\vect{x}_{l},\vect{z}_{1},\cdots ,\vect{z}_{p}} =V
\]
 where the process established above has continued. In other words, the vectors $\vect{z}_{1},\cdots ,\vect{z}_{p}$ are each taken from the
set $\set{\vect{y}_{1},\cdots ,\vect{y}_{s}} $ and $l+p=s$.
This was done for $l=1$ above. Then since $r>s$, it follows that $
l\leq s<r$ and so $l+1\leq r$. Therefore, $\vect{x}_{l+1}$ is a vector not
in the list, $\set{\vect{x}_{1},\cdots ,\vect{x}_{l}} $ and
since 
\[
\func{span}\set{\vect{x}_{1},\cdots ,\vect{x}_{l},\vect{z}
_{1},\cdots ,\vect{z}_{p}} =V
\]
 there exist scalars, $c_{i}$ and $
d_{j}$ such that 
\begin{equation}
\vect{x}_{l+1}=\sum_{i=1}^{l}c_{i}\vect{x}_{i}+\sum_{j=1}^{p}d_{j}
\vect{z}_{j}.  \label{lin-comb2}
\end{equation}
Not all the $d_{j}$ can equal zero because if this were so, it would follow
that $\set{\vect{x}_{1},\cdots ,\vect{x}_{r}} $ would be a
linearly dependent set because one of the vectors would equal a linear
combination of the others. Therefore, \ref{lin-comb2} can be solved for one of the 
$\vect{z}_{i}$, say $\vect{z}_{k}$, in terms of $\vect{x}_{l+1}$ and
the other $\vect{z}_{i}$ and just as in the above argument, replace that $
\vect{z}_{i}$ with $\vect{x}_{l+1}$ to obtain 
\begin{equation*}
\func{span}\set{\vect{x}_{1},\cdots \vect{x}_{l},\vect{x}_{l+1},
\overset{\text{p-1 vectors here}}{\overbrace{\vect{z}_{1},\cdots \vect{z}
_{k-1},\vect{z}_{k+1},\cdots ,\vect{z}_{p}}}} =V
\end{equation*}
Continue this way, eventually obtaining 
\begin{equation*}
\func{span}\set{\vect{x}_{1},\cdots ,\vect{x}_{s}} =V.
\end{equation*}
But then $\vect{x}_{r}\in $ $\func{span}\set{\vect{x}_{1},\cdots ,
\vect{x}_{s}} $ contrary to the assumption that $\set{\vect{x}
_{1},\cdots ,\vect{x}_{r}} $ is linearly independent. Therefore, $
r\leq s$ as claimed.
\end{proof}

The following corollary follows from the exchange theorem.

\begin{corollary}{Two bases of the same length}{bases-length}
Let $B_1$, $B_2$ be two bases of a vector space $V$. Suppose $B_1$ contains $m$ vectors and $B_2$ contains $n$ vectors. Then $m = n$.\index{basis!any two same size}
\end{corollary}

\begin{proof} By Theorem \ref{thm:exchange-theorem}, $m\leq n$ and $n\leq m$. Therefore $m=n$. 
\end{proof}

This corollary is very important so we provide another proof independent of the exchange theorem above.

\begin{proof}Suppose $n > m$. Then since the vectors $\set{\vect{u}
_{1},\cdots ,\vect{u}_{m}} $ span $V$, there exist scalars $c_{ij}$
such that 
\begin{equation*}
\sum_{i=1}^{m}c_{ij}\vect{u}_{i}=\vect{v}_{j}.
\end{equation*}
Therefore, 
\begin{equation*}
\sum_{j=1}^{n}d_{j}\vect{v}_{j}=\vect{0}
\text{ if and only if }\sum_{j=1}^{n}\sum_{i=1}^{m}c_{ij}d_{j}\vect{u}_{i}=
\vect{0}
\end{equation*}
if and only if 
\begin{equation*}
\sum_{i=1}^{m}\tup{\sum_{j=1}^{n}c_{ij}d_{j}} \vect{u}_{i}=\vect{
0}
\end{equation*}
Now since $\{\vect{u}_{1},\cdots ,\vect{u}_{n}\}$ is independent, this
happens if and only if 
\begin{equation*}
\sum_{j=1}^{n}c_{ij}d_{j}=0,\;i=1,2,\cdots ,m.
\end{equation*}
However, this is a system of $m$ equations in $n$ variables, $d_{1},\cdots
,d_{n}$ and $m<n$. Therefore, there exists a solution to this system of
equations in which not all the $d_{j}$ are equal to zero. Recall why this is
so. The augmented matrix for the system is of the form 
$\begin{mymatrix}{c|c}
C & \vect{0}
\end{mymatrix} $ where $C$ is a matrix which has more columns than rows. Therefore,
there are free variables and hence non-zero solutions to the system of
equations. However, this contradicts the linear independence of $\set{
\vect{u}_{1},\cdots ,\vect{u}_{m}}$. Similarly it cannot happen
that $m > n$.
\end{proof}

Given the result of the previous corollary, the following definition follows.

\begin{definition}{Dimension}{dimension-vector-space}
 A vector space $V$ is of dimension $n$ if it has a basis consisting of $n$ vectors\index{dimension of vector space}\index{vector space!dimension}.
\end{definition}

Notice that the dimension is well defined by Corollary \ref{cor:bases-length}. It is assumed here
that $n<\infty $ and therefore such a vector space is said to be \textbf{finite
dimensional}\index{finite dimensional}.

\begin{example}{Dimension of a vector space}{dimension}
Let $\Poly_2$ be the set of all polynomials of degree at most $2$. Find the dimension of $\Poly_2$. 
\end{example}

\begin{solution}
If we can find a basis of $\Poly_2$ then the number of vectors in the basis will give the dimension. Recall from Example \ref{exa:poly-degree-two} that a basis of $\Poly_2$ is given by 
\[
S  = \set{x^2, x, 1 }
\]
There are three polynomials in $S$ and hence the dimension of $\Poly_2$ is three. 
\end{solution}

It is important to note that a basis for a vector space is not unique. A vector space can have many bases. Consider the following example.

\begin{example}{A different basis for polynomials of degree two}{poly-deg-two-different-basis}
Let $\Poly_2$ be the polynomials of degree no more than 2. Is $\set{
x^{2}+x+1,2x+1,3x^{2}+1} $ a basis for $\Poly_2$?
\end{example}

\begin{solution}
Suppose these vectors are linearly independent but do not form a spanning set for $\Poly_2$. Then by Lemma \ref{lem:adding-linearly-independent}, we could find a fourth polynomial in $\Poly_2$ to create a new linearly independent 
set containing four polynomials. However this would imply that we could find a basis of $\Poly_2$ of more than three polynomials. This contradicts the result of Example \ref{exa:dimension} in which we determined the dimension of $\Poly_2$ is three.  Therefore if these vectors are linearly independent they must also form a spanning set and thus a basis for $\Poly_2$. 

Suppose then that 
\begin{eqnarray*}
a\tup{x^{2}+x+1} +b\tup{2x+1} +c\tup{3x^{2}+1} &=& 0\\
\tup{a+3c} x^{2}+\tup{a+2b} x+\tup{a+b+c} &=& 0 
\end{eqnarray*}
We know that $\set{x^2, x, 1 }$ is linearly independent, and so it follows that  
\begin{eqnarray*}
a+3c &=& 0 \\
a+2b &=& 0 \\
a+b+c &=& 0
\end{eqnarray*}
and there is only one solution to this system of equations, $a=b=c=0$.
Therefore, these are linearly independent and form a basis for $\Poly_2$.
\end{solution}

Consider the following theorem. 

\begin{theorem}{Every subspace has a basis}{every-subspace-basis}
Let $W$ be a non-zero subspace of a finite dimensional vector
space $V$. Suppose $V$ has dimension $n$.
Then $W$ has a basis\index{subspace!has a basis}
with no more than $n$ vectors.
\end{theorem}

\begin{proof}
Let $\vect{v}_{1}\in V$ where $\vect{v}_{1}\neq 0$. If $
\func{span}\set{\vect{v}_{1}} =V$, then it follows that $\set{\vect{v}
_{1}} $ is a basis for $V$. Otherwise, there exists $\vect{v}
_{2}\in V$ which is not in $\func{span}\set{\vect{v}_{1}}$. By
Lemma \ref{lem:adding-linearly-independent} $\set{\vect{v}_{1},\vect{v}_{2}} $ is a
linearly independent set of vectors. Then $\set{\vect{v}_{1},\vect{v}
_{2}} $ is a basis for $V$ and we are done. If $\func{span}\set{\vect{v}_{1},
\vect{v}_{2}} \neq V$, then there exists $\vect{v}_{3}\notin \func{
span}\set{\vect{v}_{1},\vect{v}_{2}} $ and $\set{\vect{v}
_{1},\vect{v}_{2},\vect{v}_{3}} $ is a larger linearly
independent set of vectors. Continuing this way, the process must stop
before $n+1$ steps because if not, it would be possible to obtain $n+1$
linearly independent vectors contrary to the exchange theorem, Theorem \ref{thm:exchange-theorem}. 
\end{proof}

If in fact $W$ has $n$ vectors, then it follows that $W=V$. 

\begin{theorem}{Subspace of same dimension}{subspace-vector-space}
Let $V$ be a vector space of dimension $n$ and let $W$ be a
subspace. Then $W=V$ if and only if the dimension of $W$ is also $n$.
\end{theorem}

\begin{proof}First suppose $W=V$. Then obviously the dimension of $W=n$.

Now suppose that the dimension of $W$ is $n$. Let a basis for $W$ be $
\set{\vect{w}_{1},\cdots ,\vect{w}_{n}}$. If $W$ is not equal to $V$
, then let $\vect{v}$ be a vector of $V$ which is not contained in $W$. Thus $
\vect{v}$ is not in $\func{span}\set{\vect{w}_{1},\cdots ,\vect{w}
_{n}} $ and by Lemma \ref{lem:bases-isomorphism}, $\set{\vect{w}_{1},\cdots ,\vect{w}_{n},\vect{v}} $ is linearly independent which contradicts
Theorem \ref{thm:exchange-theorem} because it would be an independent set of $n+1$
vectors even though each of these vectors is in a spanning set of $n$
vectors, a basis of $V$. 
\end{proof}

Consider the following example.

\begin{example}{Basis of a subspace}{basis-subspace}
Let $U=\set{A\in\Mat_{22} ~\abs{~
A\begin{mymatrix}{rr}
1 & 0 \\ 1 & -1 \end{mymatrix}\right. 
= \begin{mymatrix}{rr}
1 & 1 \\ 0 & -1 \end{mymatrix} A }$. 
Then $U$ is a subspace of $\Mat_{22}$ 
Find a basis of $U$, and hence $\dim(U)$.
\end{example}

\begin{solution}
Let $A=\begin{mymatrix}{rr} a & b \\ c & d \end{mymatrix}
\in\Mat_{22}$.
Then
\[ A\begin{mymatrix}{rr} 1 & 0 \\ 1 & -1 \end{mymatrix}
= \begin{mymatrix}{rr} a & b \\ c & d \end{mymatrix}
\begin{mymatrix}{rr} 1 & 0 \\ 1 & -1 \end{mymatrix}
=\begin{mymatrix}{rr} a+b & -b \\ c+d & -d \end{mymatrix}\]
and
\[ \begin{mymatrix}{rr} 1 & 1 \\ 0 & -1 \end{mymatrix} A
= \begin{mymatrix}{rr} 1 & 1 \\ 0 & -1 \end{mymatrix}
\begin{mymatrix}{rr} a & b \\ c & d \end{mymatrix}
=\begin{mymatrix}{cc} a+c & b+d \\ -c & -d \end{mymatrix}.\]
If $A\in U$, then
$\begin{mymatrix}{cc} a+b & -b \\ c+d & -d \end{mymatrix}=
\begin{mymatrix}{cc} a+c & b+d \\ -c & -d \end{mymatrix}$.

Equating entries leads to a system of four equations in the four
variables $a,b,c$ and $d$.
\[ \begin{array}{ccc}
a+b & = & a + c \\
-b & = & b + d \\
c + d & = & -c \\
-d & = & -d \end{array} \hspace*{.2in}\mbox{ or }\hspace*{.2in}
\begin{array}{rcc}
b - c & = & 0 \\
-2b - d & = & 0 \\
2c + d & = & 0 
\end{array}.  \] 

The solution to this system is
$a=s$, $b=-\frac{1}{2}t$, $c=-\frac{1}{2}t$,  $d=t$ for any $s,t\in\R$, 
and thus 
\[ A=\begin{mymatrix}{cc} s & \frac{t}{2} \\
-\frac{t}{2} & t \end{mymatrix}
= s\begin{mymatrix}{cc} 1 & 0 \\ 0 & 0 \end{mymatrix}
+ t\begin{mymatrix}{rr} 0  & -\frac{1}{2} \\ 
-\frac{1}{2} & 1 \end{mymatrix} .\]
Let 
\[ B=\set{
\begin{mymatrix}{cc} 1 & 0 \\ 0 & 0 \end{mymatrix},
\begin{mymatrix}{rr} 0  & -\frac{1}{2} \\
-\frac{1}{2} & 1 \end{mymatrix}}.\]
Then $\func{span}(B)=U$, and it is routine to verify that $B$ is
an independent subset of $\Mat_{22}$.  
Therefore $B$ is a basis of $U$, and $\dim(U)=2$.
\end{solution}

The following theorem claims that a spanning set of a vector space $V$ can be shrunk down to a basis of $V$. Similarly, a linearly independent set within $V$ can be enlarged to create a basis of $V$.

\begin{theorem}{Basis of $V$}{basis-from-spanning-lin-ind}
 If $V=\func{span}\set{\vect{u}_{1},\cdots ,\vect{u}
_{n}} $ is a vector space, then some subset of $\{\vect{u}_{1},\cdots ,\vect{u}_{n}\}$
is a basis for $V$. Also, if $\{\vect{u}_{1},\cdots ,\vect{u}
_{k}\}\subseteq V$ is linearly independent and the vector space is finite
dimensional,
then the set $\{
\vect{u}_{1},\cdots ,\vect{u}_{k}\}$, can be enlarged to obtain a basis
of $V$\index{linear independence!enlarging to form a basis}.
\end{theorem}

\begin{proof}Let 
\begin{equation*}
S=\{E\subseteq \{\vect{u}_{1},\cdots ,\vect{u}_{n}\}\text{ such that }%
\func{span}\set{E} =V\}.
\end{equation*}
For $E\in S$, let $\abs{E}$ denote the number of elements
of $E$. Let 
\begin{equation*}
m= \min \{\abs{E}\text{ such that }E\in S\}.
\end{equation*}
Thus there exist vectors 
\begin{equation*}
\{\vect{v}_{1},\cdots ,\vect{v}_{m}\}\subseteq \{\vect{u}_{1},\cdots ,%
\vect{u}_{n}\}
\end{equation*}
such that 
\begin{equation*}
\func{span}\set{\vect{v}_{1},\cdots ,\vect{v}_{m}} =V
\end{equation*}
and $m$ is as small as possible for this to happen. If this set is linearly
independent, it follows it is a basis for $V$ and the theorem is proved. On
the other hand, if the set is not linearly independent, then there exist
scalars, $c_{1},\cdots ,c_{m}$ such that 
\begin{equation*}
\vect{0}=\sum_{i=1}^{m}c_{i}\vect{v}_{i}
\end{equation*}
and not all the $c_{i}$ are equal to zero. Suppose $c_{k}\neq 0$. Then solve for the
vector $\vect{v}_{k}$ in terms of the other vectors.
Consequently, 
\begin{equation*}
V=\func{span}\set{\vect{v}_{1},\cdots ,\vect{v}_{k-1},\vect{v}
_{k+1},\cdots ,\vect{v}_{m}}
\end{equation*}
contradicting the definition of $m$. This proves the first part of the
theorem.

To obtain the second part, begin with $\{\vect{u}_{1},\cdots ,\vect{u}
_{k}\}$ and suppose a basis for $V$ is 
\begin{equation*}
\set{\vect{v}_{1},\cdots ,\vect{v}_{n}} 
\end{equation*}
If 
\begin{equation*}
\func{span}\set{\vect{u}_{1},\cdots ,\vect{u}_{k}} =V,
\end{equation*}
then $k=n$. If not, there exists a vector 
\begin{equation*}
\vect{u}_{k+1}\notin \func{span}\set{\vect{u}_{1},\cdots ,\vect{u}
_{k}}
\end{equation*}
Then from Lemma \ref{lem:adding-linearly-independent}, $\{\vect{u}_{1},\cdots ,\vect{u}_{k},
\vect{u}_{k+1}\}$ is also linearly independent. Continue adding vectors in
this way until $n$ linearly independent vectors have been obtained. Then 
\begin{equation*}
\func{span}\set{\vect{u}_{1},\cdots ,\vect{u}_{n}} =V
\end{equation*}
because if it did not do so, there would exist $\vect{u}_{n+1}$ as just
described and $\set{\vect{u}_{1},\cdots ,\vect{u}_{n+1}} $
would be a linearly independent set of vectors having $n+1$ elements. This contradicts the fact that $\set{\vect{v}_{1},\cdots ,\vect{v}_{n}} $ is a basis.
 In turn this would contradict Theorem \ref{thm:exchange-theorem}. Therefore, this list is a
basis. 
\end{proof}

Recall Example \ref{exa:adding-lin-ind} in which we added a matrix to a linearly independent set to create a larger linearly independent set. By Theorem \ref{thm:basis-from-spanning-lin-ind} we can extend a linearly independent set to a basis.  

\begin{example}{Adding to a linearly independent set}{adding-linear-ind-basis}
Let $S \subseteq M_{22}$ be a linearly independent set given by 
\[
S  = \set{\begin{mymatrix}{rr}
1 & 0 \\
0 & 0 
\end{mymatrix}, \begin{mymatrix}{rr}
0 & 1 \\
0 & 0 
\end{mymatrix} }
\]
Enlarge $S$ to a basis of $M_{22}$. 
\end{example}

\begin{solution}
Recall from the solution of Example \ref{exa:adding-lin-ind} that the set  $R \subseteq M_{22}$ given by 
\[
R = \set{\begin{mymatrix}{rr}
1 & 0 \\
0 & 0 
\end{mymatrix}, \begin{mymatrix}{rr}
0 & 1 \\
0 & 0 
\end{mymatrix}, \begin{mymatrix}{rr}
0 & 0 \\
1 & 0 
\end{mymatrix} }
\]
is also linearly independent.
However this set is still not a basis for $M_{22}$ as it is not a spanning set. In particular, $\begin{mymatrix}{rr}
0 & 0 \\
0 & 1 
\end{mymatrix}$ is not in $\func{span} R$. Therefore, this matrix can be added to the set by Lemma \ref{lem:adding-linearly-independent} to obtain a new linearly independent set given by 
\[
T = \set{\begin{mymatrix}{rr}
1 & 0 \\
0 & 0 
\end{mymatrix}, \begin{mymatrix}{rr}
0 & 1 \\
0 & 0 
\end{mymatrix}, \begin{mymatrix}{rr}
0 & 0 \\
1 & 0 
\end{mymatrix}, \begin{mymatrix}{rr}
0 & 0 \\
0 & 1 
\end{mymatrix} }
\]

This set is linearly independent and now spans $M_{22}$. Hence $T$ is a basis. 
\end{solution}

Next we consider the case where you have a
spanning set and you want a subset which is a basis. The above discussion involved adding vectors to a set. The next theorem involves removing vectors. 

\begin{theorem}{Basis from a spanning set}{}
Let $V$ be a vector space and let $W$ be a subspace. Also
suppose that $W=\func{span}\set{\vect{w}_{1},\cdots ,\vect{w}
_{m}}$. Then there exists a subset of $\set{\vect{w}_{1},\cdots ,
\vect{w}_{m}} $ which is a basis for $W$.
\end{theorem}

\begin{proof}
Let $S$ denote the set of positive integers such that for $
k\in S$, there exists a subset of $\set{\vect{w}_{1},\cdots ,\vect{w}
_{m}} $ consisting of exactly $k$ vectors which is a spanning set for 
$W$. Thus $m\in S$. Pick the smallest positive integer in $S$. Call it $k$.
Then there exists $\set{\vect{u}_{1},\cdots ,\vect{u}_{k}} \subseteq
\set{\vect{w}_{1},\cdots ,\vect{w}_{m}} $ such that $\limfunc{span}
\set{\vect{u}_{1},\cdots ,\vect{u}_{k}} =W$. If 
\begin{equation*}
\sum_{i=1}^{k}c_{i}\vect{w}_{i}=\vect{0}
\end{equation*}
and not all of the $c_{i}=0$, then you could pick $c_{j}\neq 0$, divide by
it and solve for $\vect{u}_{j}$ in terms of the others. 
\begin{equation*}
\vect{w}_{j}=\sum_{i\neq j}\tup{-\frac{c_{i}}{c_{j}}} \vect{w}_{i}
\end{equation*}
Then you could delete $\vect{w}_{j}$ from the list and have the same span.
In any linear combination involving $\vect{w}_{j}$, the linear
combination would equal one in which $\vect{w}_{j}$ is replaced with the
above sum, showing that it could have been obtained as a linear combination
of $\vect{w}_{i}$ for $i\neq j$. Thus $k-1\in S$ contrary to the choice of $k$
. Hence each $c_{i}=0$ and so $\set{\vect{u}_{1},\cdots ,\vect{u}
_{k}} $ is a basis for $W$ consisting of vectors of $\set{\vect{w}
_{1},\cdots ,\vect{w}_{m}}$. 
\end{proof}

Consider the following example of this concept. 

\begin{example}{Basis from a spanning set}{}
Let $V$ be the vector space of polynomials of degree no more than 3,
denoted earlier as $\Poly_{3}$. Consider the following vectors in $V$.
\begin{eqnarray*}
&&2x^{2}+x+1,x^{3}+4x^{2}+2x+2,2x^{3}+2x^{2}+2x+1, \\
&&x^{3}+4x^{2}-3x+2,x^{3}+3x^{2}+2x+1
\end{eqnarray*}
Then, as mentioned above, $V$ has dimension 4 and so clearly these vectors
are not linearly independent. A basis for $V$ is $\set{
1,x,x^{2},x^{3}}$. Determine a linearly independent subset of these
which has the same span. Determine whether this subset is a basis for $V$.
\end{example}

\begin{solution}
Consider an isomorphism which maps $\R%
^{4}$ to $V$ in the obvious way. Thus 
\begin{equation*}
\begin{mymatrix}{c}
1 \\ 
1 \\ 
2 \\ 
0
\end{mymatrix}
\end{equation*}
corresponds to $2x^{2}+x+1$ through the use of this isomorphism. Then
corresponding to the above vectors in $V$ we would have the following
vectors in $\R^{4}$. 
\begin{equation*}
\begin{mymatrix}{c}
1 \\ 
1 \\ 
2 \\ 
0
\end{mymatrix} ,\begin{mymatrix}{c}
2 \\ 
2 \\ 
4 \\ 
1
\end{mymatrix} ,\begin{mymatrix}{c}
1 \\ 
2 \\ 
2 \\ 
2
\end{mymatrix} ,\begin{mymatrix}{r}
2 \\ 
-3 \\ 
4 \\ 
1
\end{mymatrix} ,\begin{mymatrix}{c}
1 \\ 
2 \\ 
3 \\ 
1
\end{mymatrix}
\end{equation*}
Now if we obtain a subset of these which has the same span but which is
linearly independent, then the corresponding vectors from $V$ will also be
linearly independent. If there are four
in the list, then the resulting vectors from $V$ must be a basis for $V$.
The {\rref} for the matrix which has the above vectors as
columns is 
\begin{equation*}
\begin{mymatrix}{rrrrr}
1 & 0 & 0 & -15 & 0 \\ 
0 & 1 & 0 & 11 & 0 \\ 
0 & 0 & 1 & -5 & 0 \\ 
0 & 0 & 0 & 0 & 1
\end{mymatrix}
\end{equation*}
Therefore, a basis for $V$ consists of the vectors
\begin{eqnarray*}
&&2x^{2}+x+1,x^{3}+4x^{2}+2x+2,2x^{3}+2x^{2}+2x+1, \\
&&x^{3}+3x^{2}+2x+1.
\end{eqnarray*}
Note how this is a subset of the original set of vectors. If there had been
only three pivot columns in this matrix, then we would not have had a basis
for $V$ but we would at least have obtained a linearly independent subset of
the original set of vectors in this way. 

Note also that, since all linear relations are preserved by an isomorphism,
\begin{eqnarray*}
&&-15\tup{2x^{2}+x+1} +11\tup{x^{3}+4x^{2}+2x+2} +\tup{
-5} \tup{2x^{3}+2x^{2}+2x+1} \\
&=&x^{3}+4x^{2}-3x+2
\end{eqnarray*}

\end{solution}

Consider the following example.

\begin{example}{Shrinking a spanning set}{shrink-spanning}
Consider the set $S \subseteq \Poly_2$ given by 
\[
S = \set{1, x, x^2, x^2 + 1 }
\]
Show that $S$ spans $\Poly_2$, then remove vectors from $S$ until it creates a basis. 
\end{example}

\begin{solution}
First we need to show that $S$ spans $\Poly_2$. Let $ax^2 + bx + c$ be an arbitrary polynomial in $\Poly_2$. Write 
\[
ax^2 + bx + c = r(1) + s(x) + t(x^2) + u (x^2 + 1)
\]
Then,
\begin{eqnarray*}
ax^2 +bx + c &=& r(1) + s(x) + t(x^2) + u (x^2 + 1) \\
&=& (t+u) x^2 + s(x) + (r+u) 
\end{eqnarray*}

It follows that 
\begin{eqnarray*}
a &=& t + u \\
b &=& s \\
c &=& r + u 
\end{eqnarray*}

Clearly a solution exists for all $a,b,c$ and so $S$ is a spanning set for $\Poly_2$. By Theorem \ref{thm:basis-from-spanning-lin-ind}, some subset of $S$ is a basis for $\Poly_2$. 

Recall that a basis must be both a spanning set and a linearly independent set.
Therefore we must remove a vector from $S$ keeping this in mind. Suppose we remove $x$ from $S$. The resulting set would be $\set{1, x^2, x^2 + 1 }$. This set is clearly linearly dependent (and also does not span $\Poly_2$) and so is not a basis. 

Suppose we remove $x^2 + 1$ from $S$. The resulting set is $\set{1, x, x^2 }$ which is both linearly independent and spans $\Poly_2$. Hence this is a basis for $\Poly_2$. Note that removing any one of $1, x^2$, or $x^2 + 1$ will result in a basis.
\end{solution}

Now the following is a fundamental result about subspaces.

\begin{theorem}{Basis of a vector space}{basis-vector-space}
Let $V$ be a finite dimensional vector space and let $W$ be
a non-zero subspace. Then $W$ has a basis. That is, there exists a linearly
independent set of vectors $\set{\vect{w}_{1},\cdots ,\vect{w}_{r}} $
such that 
\begin{equation*}
\limfunc{span}\set{\vect{w}_{1},\cdots ,\vect{w}_{r}} =W
\end{equation*}
Also if $\set{\vect{w}_{1},\cdots ,\vect{w}_{s}} $ is a linearly
independent set of vectors, then $W$ has a basis of the form $\set{\vect{w}
_{1},\cdots ,\vect{w}_{s},\cdots ,\vect{w}_{r}} $ for $r\geq s$.
\end{theorem}

\begin{proof}
Let the dimension of $V$ be $n$. Pick $\vect{w}_{1}\in W$
where $\vect{w}_{1}\neq \vect{0}$. If $\vect{w}_{1},\cdots ,\vect{w}_{s}$ have
been chosen such that $\set{\vect{w}_{1},\cdots ,\vect{w}_{s}} $ is
linearly independent, if $\limfunc{span}\set{\vect{w}_{1},\cdots ,\vect{w}
_{r}} =W$, stop. You have the desired basis. Otherwise, there exists $
\vect{w}_{s+1}\notin \limfunc{span}\set{\vect{w}_{1},\cdots ,\vect{w}
_{s}} $ and $\set{\vect{w}_{1},\cdots ,
\vect{w}_{s},\vect{w}_{s+1}} $ is linearly independent. Continue this
way until the process stops. It must stop since otherwise, you could obtain a
linearly independent set of vectors having more than $n$ vectors which is
impossible.

The last claim is proved by following the above procedure starting with $
\set{\vect{w}_{1},\cdots ,\vect{w}_{s}} $ as above. 
\end{proof}

This also proves the following corollary. Let $V$ play the role of $
W$ in the above theorem and begin with a basis for $W$, enlarging it to form
a basis for $V$ as discussed above.

\begin{corollary}{Basis extension}{}
Let $W$ be any non-zero subspace of a vector space $V$.
Then every basis of $W$ can be extended to a basis for $V$.
\end{corollary}

Consider the following example.

\begin{example}{Basis extension}{}
Let $V=\R^{4}$ and let 
\begin{equation*}
W=\func{span}\set{\begin{mymatrix}{c}
1 \\ 
0 \\ 
1 \\ 
1
\end{mymatrix} ,\begin{mymatrix}{c}
0 \\ 
1 \\ 
0 \\ 
1
\end{mymatrix} }
\end{equation*}
Extend this basis of $W$ to a basis of $V$.
\end{example}

\begin{solution}
An easy way to do this is to take the {\rref} of the matrix 
\begin{equation}
\begin{mymatrix}{cccccc}
1 & 0 & 1 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 1 & 0 & 0 \\ 
1 & 0 & 0 & 0 & 1 & 0 \\ 
1 & 1 & 0 & 0 & 0 & 1
\end{mymatrix}  \label{vector-space-eq1}
\end{equation}
Note how the given vectors were placed as the first two and then the matrix
was extended in such a way that it is clear that the span of the columns of
this matrix yield all of $\R^{4}$. Now determine the pivot columns.
The {\rref} is 
\begin{equation}
\begin{mymatrix}{rrrrrr}
1 & 0 & 0 & 0 & 1 & 0 \\ 
0 & 1 & 0 & 0 & -1 & 1 \\ 
0 & 0 & 1 & 0 & -1 & 0 \\ 
0 & 0 & 0 & 1 & 1 & -1
\end{mymatrix}  \label{vector-space-eq2}
\end{equation}
These are 
\begin{equation*}
\begin{mymatrix}{c}
1 \\ 
0 \\ 
1 \\ 
1
\end{mymatrix} ,\begin{mymatrix}{c}
0 \\ 
1 \\ 
0 \\ 
1
\end{mymatrix} ,\begin{mymatrix}{c}
1 \\ 
0 \\ 
0 \\ 
0
\end{mymatrix} ,\begin{mymatrix}{c}
0 \\ 
1 \\ 
0 \\ 
0
\end{mymatrix}
\end{equation*}
and now this is an extension of the given basis for $W$ to a basis for $
\R^{4}$.

Why does this work? The columns of \ref{vector-space-eq1} obviously span $\R
^{4}$  the span of the first four is the same as the span of all
six.
\end{solution}
