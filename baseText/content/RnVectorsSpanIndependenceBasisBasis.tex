% ----------------------------------------------------------------------
\section{Basis and dimension}

% ----------------------------------------------------------------------
\subsection{Definition of basis}

We saw in Proposition~\ref{prop:span-subspace} that spans are
subspaces of\/ $\R^n$. Interestingly, the converse is also true: every
subspace of\/ $\R^n$ is the span of some finite set of vectors.

\begin{theorem}{Subspaces are spans}{subspaces-are-spans}
  Let $V$ be a subspace of\/ $\R^n$. Then there exist linearly
  independent vectors $\set{\vect{u}_{1},\ldots,\vect{u}_{k}}$ in $V$
  such that
  \begin{equation*}
    V= \sspan\set{\vect{u}_{1},\ldots ,\vect{u}_{k}}. 
  \end{equation*}%
  \index{subspace!subspace is a span}
  \vspace{-3ex}
\end{theorem}

\begin{proof}
  We proceed as follows.
  \begin{enumerate}
  \item[0.] If $V=\set{\vect{0}}$, then $V$ is the empty span, and we
    are done.
  \item[1.] Otherwise, $V$ contains some non-zero vector.  Pick a
    non-zero vector $\vect{u}_{1}$ in $V$. If
    $V=\sspan\set{\vect{u}_{1}}$, we are done.
  \item[2.] Otherwise, pick a vector $\vect{u}_{2}$ in $V$ that is not
    in $\sspan\set{\vect{u}_{1}}$. If
    $V=\sspan\set{\vect{u}_{1},\vect{u}_{2}}$, we are done.
  \item[3.] Otherwise, pick a vector $\vect{u}_{3}$ in $V$ that is not
    in $\sspan\set{\vect{u}_{1},\vect{u}_{2}}$. If
    $V=\sspan\set{\vect{u}_{1},\vect{u}_{2},\vect{u}_{3}}$, we are done.
  \item[4.] Otherwise, pick a vector $\vect{u}_{4}$ in $V$ that is not
    in $\sspan\set{\vect{u}_{1},\vect{u}_{2},\vect{u}_{4}}$, and so on.
  \end{enumerate}
  Continue in this way. Note that after the $j\th$ step of this
  process, the vectors $\vect{u}_1,\ldots,\vect{u}_j$ are linearly
  independent. This is because, by construction, no vector is in the
  span of the previous vectors, and therefore no vector is redundant.
  By
  Theorem~\ref{thm:properties-linear-independence}(\ref{properties-linear-independence-c}),
  there can be at most $n$ linearly independent vectors in $\R^n$.
  Therefore the process must stop after $k$ steps for some $k\leq
  n$. But then $V=\sspan\set{\vect{u}_{1},\ldots ,\vect{u}_{k}}$, as
  desired.
\end{proof}

In summary, every subspace of\/ $\R^{n}$ is spanned by a finite,
linearly independent collection of vectors.  Such a collection of
vectors is called a \textbf{basis} of the subspace.

\begin{definition}{Basis of a subspace}{subspace-basis}
  Let $V$ be a subspace of\/ $\R^{n}$. Then
  $\set{\vect{u}_{1},\ldots ,\vect{u}_{k}}$ is a \textbf{basis} for
  $V$ if the following two conditions hold:%
  \index{basis}%
  \index{subspace!basis|see{basis}}%
  \index{vector!basis|see{basis}}%
  \begin{enumerate}
  \item $\sspan\set{\vect{u}_{1},\ldots,\vect{u}_{k}}=V$, and
  \item $\vect{u}_{1},\ldots,\vect{u}_{k}$ are linearly independent.
  \end{enumerate}
\end{definition}

Note that the plural of basis is \textbf{bases}.

% ----------------------------------------------------------------------
\subsection{Examples of bases}

\begin{proposition}{Standard basis of\/ $\R^n$}{standard-basis}
  Let $\vect{e}_i$ be the vector in $\R^n$ whose $i\th$ entry is $1$
  and all of whose other entries are $0$. In other words, $\vect{e}_i$
  is the $i\th$ column of the identity matrix.
  \begin{equation*}
    \vect{e}_1 = \begin{mymatrix}{c} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{mymatrix},\quad
    \vect{e}_2 = \begin{mymatrix}{c} 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{mymatrix},\quad
    \vect{e}_3 = \begin{mymatrix}{c} 0 \\ 0 \\ 1 \\ \vdots \\ 0 \end{mymatrix},\quad
    \ldots~,\quad
    \vect{e}_n = \begin{mymatrix}{c} 0 \\ 0 \\ 0 \\ \vdots \\ 1\end{mymatrix}.
  \end{equation*}
  Then $\set{\vect{e}_1,\vect{e}_2,\ldots,\vect{e}_n}$ is a basis for
  $\R^n$. It is called the \textbf{standard basis}%
  \index{standard basis}%
  \index{basis!standard}
  of\/ $\R^n$.
\end{proposition}

\begin{proof}
  To see that it is a basis of\/ $\R^n$, first notice that the vectors
  $\vect{e}_1,\vect{e}_2,\ldots,\vect{e}_n$ span $\R^n$. Indeed, every
  vector $\vect{v} = \mat{x_1,\ldots,x_n}^T\in\R^n$ can be written as
  $\vect{v} = x_1\vect{e}_1+\ldots+x_n\vect{e}_n$. Second, the vectors
  $\vect{e}_1,\vect{e}_2,\ldots,\vect{e}_n$ are evidently linearly
  independent, because none of these vectors can be written as a
  linear combination of previous vectors. Since the vectors span
  $\R^n$ and are linearly independent, they form a basis of\/ $\R^n$.
\end{proof}

\begin{example}{A non-standard basis of\/ $\R^3$}{non-standard-basis}
  Check that the vectors
  \begin{equation*}
    \vect{u}_1 = \begin{mymatrix}{r} 1 \\ 2 \\ 1 \end{mymatrix},\quad
    \vect{u}_2 = \begin{mymatrix}{r} 0 \\ 1 \\ 0 \end{mymatrix},\quad
    \mbox{and}\quad
    \vect{u}_3 = \begin{mymatrix}{r} -1 \\ 0 \\ 1 \end{mymatrix}
  \end{equation*}
  form a basis of\/ $\R^3$.
\end{example}

\begin{solution}
  We must check that the vectors $\vect{u}_1,\vect{u}_2,\vect{u}_3$
  are linearly independent and span $\R^3$. To check linear
  independence, we use the casting-out algorithm.
  \begin{equation*}
    \begin{mymatrix}{rrr}
      1 & 0 & -1 \\
      2 & 1 & 0 \\
      1 & 0 & 1 \\
    \end{mymatrix}
    \quad\sim\quad
    \begin{mymatrix}{rrr}
      \circled{1} & 0 & -1 \\
      0 & \circled{1} & 2 \\
      0 & 0 & \circled{2} \\
    \end{mymatrix}.
  \end{equation*}
  Since all columns are pivot columns, there are no redundant vectors,
  so $\vect{u}_1,\vect{u}_2,\vect{u}_3$ are linearly independent.
  To check that they span all of\/ $\R^3$, let $\vect{w}=\mat{x,y,z}^T$
  be an arbitrary element of\/ $\R^3$. We must show that $\vect{w}$ is a linear
  combination of $\vect{u}_1,\vect{u}_2,\vect{u}_3$. This amounts to
  solving the system of equations
  \begin{equation*}
    a_1\,\vect{u}_1+a_2\,\vect{u}_2+a_3\,\vect{u}_3 = \vect{w},
  \end{equation*}
  or in augmented matrix form,
  \begin{equation*}
    \begin{mymatrix}{rrr|c}
      1 & 0 & -1 & x \\
      2 & 1 & 0  & y \\
      1 & 0 & 1  & z \\
    \end{mymatrix}
    \quad\sim\quad
    \begin{mymatrix}{rrr|c}
      1 & 0 & -1 & x    \\
      0 & 1 & 2  & y-2x \\
      0 & 0 & 2  & z-x  \\
    \end{mymatrix}.
  \end{equation*}
  The system is clearly consistent, so it has a solution, and
  therefore $\vect{w}$ is indeed a linear combination of
  $\vect{u}_1,\vect{u}_2,\vect{u}_3$. Since $\vect{w}$ was an
  arbitrary vector of\/ $\R^3$, it follows that
  $\vect{u}_1,\vect{u}_2,\vect{u}_3$ span $\R^3$.
\end{solution}

Generalizing the last example, we find that a set of $n$ vectors forms
a basis of\/ $\R^n$ if and only if the matrix having those vectors as
its columns is invertible. This is the content of the following
proposition.

\begin{proposition}{Invertible matrices and bases of\/ $\R^n$}{invertible-matrices}
  Let $A$ be an $n\times n$-matrix. Then the columns of $A$ form a
  basis of\/ $\R^n$ if and only if $A$ is invertible%
  \index{basis!of Rn@of\/ $\R^n$}.
\end{proposition}

We turn to the question of finding bases for subspaces of\/ $\R^n$.

\begin{example}{Basis of a span}{basis-of-span}
  Let
  \begin{equation*}
    \vect{u}_1 = \begin{mymatrix}{r} 2 \\ 0 \\ -2 \end{mymatrix},
    \quad
    \vect{u}_2 = \begin{mymatrix}{r} -1 \\ 0 \\ 1 \end{mymatrix},
    \quad
    \vect{u}_3 = \begin{mymatrix}{r} 1 \\ 3 \\ 5 \end{mymatrix},
    \quad
    \vect{u}_4 = \begin{mymatrix}{r} 3 \\ 5 \\ 7 \end{mymatrix},
    \quad
    \vect{u}_5 = \begin{mymatrix}{r} -1 \\ 1 \\ 3 \end{mymatrix}.
  \end{equation*}
  Find a basis of $\sspan\set{\vect{u}_1,\ldots,\vect{u}_5}$%
  \index{basis!of a span}.
\end{example}

\begin{solution}
  Let $S=\sspan\set{\vect{u}_1,\ldots,\vect{u}_5}$.  By
  Theorem~\ref{thm:linearly-independent-subset}, we know that if we
  remove the redundant vectors from
  $\set{\vect{u}_1,\ldots,\vect{u}_5}$, then the remaining vectors
  will be linearly independent and will still span $S$. In other
  words, the remaining vectors will be a basis for $S$. We use the
  casting-out algorithm to identity the redundant vectors:
  \begin{equation*}
    \begin{mymatrix}{rrrrr}
      2 & -1 & 1 & 3 & -1 \\
      0 & 0 & 3 & 5 & 1 \\
      -2 & 1 & 5 & 7 & 3 \\
    \end{mymatrix}
    ~\sim~
    \begin{mymatrix}{rrrrr}
      2 & -1 & 1 & 3 & -1 \\
      0 & 0 & 3 & 5 & 1 \\
      0 & 0 & 6 & 10 & 2 \\
    \end{mymatrix}
    ~\sim~
    \begin{mymatrix}{rrrrr}
      \circled{2} & -1 & 1 & 3 & -1 \\
      0 & 0 & \circled{3} & 5 & 1 \\
      0 & 0 & 0 & 0 & 0 \\
    \end{mymatrix}.
  \end{equation*}
  Since columns $2$, $4$, and $5$ are the non-pivot columns, it
  follows that the vectors $\vect{u}_2$, $\vect{u}_4$, and
  $\vect{u}_5$ are redundant. Therefore, the desired basis is
  $\set{\vect{u}_1,\vect{u}_3}$. 
\end{solution}

\begin{example}{Basis of the solution space of a homogeneous system of equations}{basis-solution-space}
  Find a basis for the solution space of the system of equations%
  \index{basis!of a solution space}
  \begin{equation*}
    \begin{array}{r@{~}r@{~}r@{~}r@{~}r@{~}r@{~}r@{~}r@{~}r@{~}r@{~}r}
     x &+& y  &-& z &+& 3w &-& 2v &=& 0, \\
     x &+& y &+& z &-& 11w &+& 8v &=& 0, \\
    4x &+& 4y &-& 3z &+& 5w &-& 3v &=& 0. \\
    \end{array}
  \end{equation*}
\end{example}

\begin{solution}
  We solve the system of equations in the usual way:
  \begin{equation*}
    \begin{mymatrix}{rrrrr|r}
      1 & 1 & -1 & 3 & -2 & 0 \\
      1 & 1 &  1 & -11 & 8 & 0 \\
      4 & 4 & -3 & 5 & -3 & 0 \\
    \end{mymatrix}
    ~\sim~
    \begin{mymatrix}{rrrrr|r}
      1 & 1 & -1 & 3 & -2 & 0 \\
      0 & 0 & 2 & -14 & 10 & 0 \\
      0 & 0 & 1 & -7 & 5 & 0 \\
    \end{mymatrix}
    ~\sim~
    \begin{mymatrix}{rrrrr|r}
      \circled{1} & 1 & 0 & -4 & 3 & 0 \\
      0 & 0 & \circled{1} & -7 & 5 & 0 \\
      0 & 0 & 0 &  0 & 0 & 0 \\
    \end{mymatrix}.
  \end{equation*}
  From the {\rref}, we see that $y$, $w$, and $v$ are free
  variables. The general solution is:
  \begin{equation*}
    \begin{mymatrix}{c} x \\ y \\ z \\ w \\ v \end{mymatrix}
    \quad=\quad
    t \begin{mymatrix}{r} -3 \\ 0 \\ -5 \\ 0 \\ 1 \end{mymatrix}
    ~+~ s \begin{mymatrix}{r} 4 \\ 0 \\ 7 \\ 1 \\ 0 \end{mymatrix}
    ~+~ r \begin{mymatrix}{r} -1 \\ 1 \\ 0 \\ 0 \\ 0 \end{mymatrix}.
  \end{equation*}
  Thus, the solution space is spanned by the vectors
  \begin{equation*}
    \set{\begin{mymatrix}{r} -3 \\ 0 \\ -5 \\ 0 \\ 1 \end{mymatrix},
    \begin{mymatrix}{r} 4 \\ 0 \\ 7 \\ 1 \\ 0 \end{mymatrix},
    \begin{mymatrix}{r} -1 \\ 1 \\ 0 \\ 0 \\ 0 \end{mymatrix}
    }.
  \end{equation*}
  Moreover, these vectors are evidently linearly independent, because
  each vector contains a $1$ in a position where all the previous
  vectors have $0$ (and therefore, none of the vectors can be written
  as a linear combination of previous vectors). It follows that the
  above three vectors form a basis of the solution space.
\end{solution}

Note that the basis vectors of the solution space are exactly what we
called the \textbf{basic solutions}%
\index{basic solution}%
\index{solution!basic}%
\index{solution!basis of}
in Section~\ref{sec:homogeneous-systems}. 

% ----------------------------------------------------------------------
\subsection{Bases and coordinate systems}

Let $V$ be a subspace of\/ $\R^n$. A basis of $V$ is essentially the
same thing as a coordinate system for $V$. To see why, let
$B=\set{\vect{u}_1,\ldots,\vect{u}_k}$ be some basis of $V$. This
means that the vectors $\vect{u}_1,\ldots,\vect{u}_k$ are linearly
independent and span $V$. Because the basis vectors are spanning,
every vector $\vect{v}\in V$ can be written as a linear combination of
basis vectors
\begin{equation*}
  \vect{v} = a_1\,\vect{u}_1 + \ldots + a_k\,\vect{u}_k.
\end{equation*}
Moreover, because the basis vectors are linearly independent, it
follows by Theorem~\ref{thm:unique-linear-combination} that the
coefficients $a_1,\ldots,a_k$ are unique. We say that $a_1,\ldots,a_k$
are the \textbf{coordinates of $\vect{v}$ with respect to the basis
$B$}%
\index{coordinate!with respect to basis}%
\index{coordinate system!and basis}, and we write
\begin{equation*}
  \coord{\vect{v}}_B = \begin{mymatrix}{c} a_1 \\ a_2 \\ a_3 \end{mymatrix}.
\end{equation*}
\begin{center}
  \begin{tikzpicture}
    \begin{scope}[scale=2.5,x={(1.2cm,-0.2cm)},y={(0.5cm,0.4cm)},z={(0cm,0.8cm)}]
      \draw(-2.5,0,0) -- (2.5,0,0);
      \draw(0,-2.3,0) -- (0,2.3,0);
      \draw(0,0,-1.3) -- (0,0,1.3);
      \draw[->, thick, blue](0,0,0) -- node[below] {$\vect{u}_1$} +(1,0,0);
      \draw[->, thick, blue](0,0,0) -- node[above left=-1ex] {$\vect{u}_2$} +(0,1,0);
      \draw[->, thick, blue](0,0,0) -- node[left] {$\vect{u}_3$} +(0,0,1);
      \draw(-2,0,0) -- +(0,0,-0.1) node[below] {$-2$};
      \draw(-1,0,0) -- +(0,0,-0.1) node[below] {$-1$};
      \draw(0,0,0) -- +(0,0,-0.1) node[below] {$0$};
      \draw(1,0,0) -- +(0,0,-0.1) node[below] {$1$};
      \draw(2,0,0) -- +(0,0,-0.1) node[below] {$2$};
      \draw(0,-2,0) -- +(0,0,-0.1) node[below] {$-2$};
      \draw(0,-1,0) -- +(0,0,-0.1) node[below] {$-1$};
      \draw(0,1,0) -- +(0,0,-0.1) node[below] {$1$};
      \draw(0,2,0) -- +(0,0,-0.1) node[below] {$2$};
      \draw(0,0,-1) -- +(-0.1,0,0) node[left] {$-1$};
      \draw(0,0,1) -- +(-0.1,0,0) node[left] {$1$};
    \end{scope}
    \path(0,-3.2) node {Basis as coordinate system};
  \end{tikzpicture}
\end{center}

\begin{example}{Find a vector from its coordinates in a basis}{vector-from-coordinates}
  Find the vector $\vect{v}$ that has coordinates
  \begin{equation*}
    \coord{\vect{v}}_B = \begin{mymatrix}{r} 1 \\ -1 \\ 2 \end{mymatrix}
  \end{equation*}
  with respect to the basis $B=\set{\vect{u}_1,\vect{u}_2,\vect{u}_3}$
  of\/ $\R^3$, where
  \begin{equation*}
    \vect{u}_1 = \begin{mymatrix}{r} 1 \\ 2 \\ 1 \end{mymatrix},\quad
    \vect{u}_2 = \begin{mymatrix}{r} 0 \\ 1 \\ 0 \end{mymatrix},\quad
    \mbox{and}\quad
    \vect{u}_3 = \begin{mymatrix}{r} -1 \\ 0 \\ 1\end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  This simply means that $\vect{v} = 1\vect{u}_1 - 1\vect{u}_2 +
  2\vect{u}_3$. We calculate
  \begin{equation*}
    \vect{v} =
    1\begin{mymatrix}{r} 1 \\ 2 \\ 1 \end{mymatrix}
    - 1\begin{mymatrix}{r} 0 \\ 1 \\ 0 \end{mymatrix}
    + 2\begin{mymatrix}{r} -1 \\ 0 \\ 1 \end{mymatrix}
    = \begin{mymatrix}{r} -1 \\ 1 \\ 3  \end{mymatrix}.
  \end{equation*}
\end{solution}

In case the basis is the standard basis, the coordinates are just the
usual ones, as the following example illustrates:

\begin{example}{Find a vector from its coordinates in the standard basis}{vector-from-coorinates-standard}
  Find the vector $\vect{v}$ that has coordinates
  \begin{equation*}
    \coord{\vect{v}}_B = \begin{mymatrix}{r} 1 \\ -1 \\ 2 \end{mymatrix},
  \end{equation*}
  where $B$ is the standard basis of\/ $\R^3$.
\end{example}

\begin{solution}
  The standard basis is
  \begin{equation*}
    \vect{e}_1 = \begin{mymatrix}{r} 1 \\ 0 \\ 0 \end{mymatrix},\quad
    \vect{e}_2 = \begin{mymatrix}{r} 0 \\ 1 \\ 0 \end{mymatrix},\quad
    \mbox{and}\quad
    \vect{e}_3 = \begin{mymatrix}{r} 0 \\ 0 \\ 1\end{mymatrix},
  \end{equation*}
  and in this case, we simply have
  \begin{equation*}
    \vect{v} = 1\vect{e}_1 - 1\vect{e}_2 + 2\vect{e}_3
        = \begin{mymatrix}{r} 1 \\ -1 \\ 2  \end{mymatrix}.
  \end{equation*}
\end{solution}

We can also ask to find the coordinates of a given vector in a given
basis.

\begin{example}{Find the coordinates of a vector with respect to a basis}{coordinates-from-vector}
  Find the coordinates of the vector
  \begin{equation*}
    \vect{v} = \begin{mymatrix}{r} 1 \\ 2 \\ 3 \end{mymatrix}
  \end{equation*}
  with respect to the basis $B=\set{\vect{u}_1,\vect{u}_2,\vect{u}_3}$
  of\/ $\R^3$, where
  \begin{equation*}
    \vect{u}_1 = \begin{mymatrix}{r} 1 \\ 2 \\ 1 \end{mymatrix},\quad
    \vect{u}_2 = \begin{mymatrix}{r} 0 \\ 1 \\ 0 \end{mymatrix},\quad
    \mbox{and}\quad
    \vect{u}_3 = \begin{mymatrix}{r} -1 \\ 0 \\ 1\end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  To find the coordinates, we must solve the system of equations
  $\vect{v} = a_1\,\vect{u}_1+a_2\,\vect{u}_2+a_3\,\vect{u}_3$. We
  solve:
  \begin{equation*}
    \begin{mymatrix}{rrr|r}
      1 & 0 & -1 & 1 \\
      2 & 1 & 0 & 2 \\
      1 & 0 & 1 & 3 \\
    \end{mymatrix}
    \quad\sim\quad
    \begin{mymatrix}{rrr|r}
      1 & 0 & -1 & 1 \\
      0 & 1 & 2 & 0 \\
      0 & 0 & 2 & 2 \\
    \end{mymatrix}
    \quad\sim\quad
    \begin{mymatrix}{rrr|r}
      1 & 0 & 0 & 2 \\
      0 & 1 & 0 & -2 \\
      0 & 0 & 1 & 1 \\
    \end{mymatrix}.
  \end{equation*}
  Therefore, the unique solution is $(a_1,a_2,a_3) = (2,-2,1)$. The
  coordinates of $\vect{v}$ with respect to the basis $B$ are
  \begin{equation*}
    \coord{\vect{v}}_B
    = \begin{mymatrix}{r} a_1 \\ a_2 \\ a_3 \end{mymatrix}
    = \begin{mymatrix}{r} 2 \\ -2 \\ 1 \end{mymatrix}.
  \end{equation*}
\end{solution}

% ----------------------------------------------------------------------
\subsection{Dimension}

One of the most important properties of bases is that any two bases
for the same space must be of the same size. To show this, we will
need the the following fundamental result, called the Exchange
Theorem. This theorem states that spanning sets have at least as many
vectors as linearly independent sets.

\begin{theorem}{Exchange Theorem}{exchange-theorem}
  \index{exchange theorem}%
  Suppose $\vect{u}_{1},\ldots,\vect{u}_{r}$ are linearly independent
  elements of $\sspan\set{\vect{v}_{1},\ldots,\vect{v}_{s}}$. Then
  $r\leq s$.
\end{theorem}

\begin{proof}
  Since each $\vect{u}_j$ is an element of
  $\sspan\set{\vect{v}_{1},\ldots,\vect{v}_{s}}$, there exist
  scalars $a_{ij}$ such that
  \begin{equation*}
    \vect{u}_{j} = a_{1j}\,\vect{v}_1 + \ldots + a_{sj}\,\vect{v}_s.
  \end{equation*}
  Let $A = \mat{a_{ij}}$. Note that this matrix has $s$ rows and $r$
  columns, i.e., it is an $s\times r$-matrix. Now suppose, for the
  sake of obtaining a contradiction, that $r>s$. Then by
  Theorem~\ref{thm:rank-homogeneous-solutions}, the system
  $A\vect{x}=\vect{0}$ has a non-trivial solution $\vect{x}$, i.e.,
  there exists $\vect{x}\neq\vect{0}$ such that $A\vect{x}=\vect{0}$.
  In other words, for all $i=1,\ldots,s$, 
  \begin{equation*}
    a_{i1}x_1 + \ldots + a_{ir}x_r = 0.
  \end{equation*}
  Therefore,
  \begin{eqnarray*}
    x_1\,\vect{u}_1 + \ldots + x_r\,\vect{u}_r
    &=&
        x_1(a_{11}\,\vect{v}_1 + \ldots + a_{s1}\,\vect{v}_s)
        + \ldots
        + x_r(a_{1r}\,\vect{v}_1 + \ldots + a_{sr}\,\vect{v}_s)
    \\
    &=&
        (a_{11}x_1 + \ldots + a_{1r}x_r)\vect{v}_1
        + \ldots
        + (a_{s1}x_1 + \ldots + a_{sr}x_r)\vect{v}_s
    \\
    &=& 0\,\vect{v}_1 + \ldots + 0\,\vect{v}_s
    \\
    &=& 0.
  \end{eqnarray*}
  This contradicts the assumption that
  $\vect{u}_{1},\ldots,\vect{u}_{r}$ are linearly independent. Since
  we assumed $r>s$ and obtained a contradiction, it follows that
  $r\leq s$, as desired.
\end{proof}

Armed with the Exchange Theorem, we are now ready to show that any two
bases of a space are of the same size.

\begin{theorem}{Bases are of the same size}{bases-same-size}
  \index{basis!size of}%
  Let $V$ be a subspace of\/ $\R^{n}$, and let $B_1$ and $B_2$ be
  bases of $V$. Suppose $B_1$ contains $s$ vectors and $B_2$ contains
  $r$ vectors. Then $s=r$.
\end{theorem}

\begin{proof}
  This follows right away from the Exchange Theorem. Indeed, observe
  that $B_1 = \set{ \vect{u}_{1},\ldots ,\vect{u}_{s}} $ is a spanning
  set for $V$ while $ B_2 = \set{\vect{v}_{1},\ldots ,\vect{v}_{r}} $
  is linearly independent, so $s \geq r$. Similarly
  $B_2 = \set{\vect{v}_{1},\ldots ,\vect{v} _{r}} $ is a spanning set
  for $V$ while $B_1 = \set{\vect{u}_{1},\ldots , \vect{u}_{s}} $ is
  linearly independent, so $r\geq s$.
\end{proof}

Because every basis of $V$ has the same number of vectors, we give
this number a special name. It is called the \textbf{dimension} of
$V$.

\begin{definition}{Dimension of a subspace}{dimension}
  Let $V$ be a subspace of\/ $\R^{n}$. Then the \textbf{dimension}%
  \index{dimension}%
  \index{subspace!dimension} of $V$, written $\dim(V)$, is
  defined to be the number of vectors in a basis.
\end{definition}

\begin{example}{Dimension of\/ $\R^n$}{dimension-Rn}
  What is the dimension of $\R^{n}$?
\end{example}

\begin{solution}
  The standard basis of $\R^{n}$ is
  $\set{\vect{e}_{1},\ldots,\vect{e}_{n}}$. Since it has $n$ vectors,
  so $\dim(\R^n) = n$.
\end{solution}

\begin{example}{Dimension of a subspace}{dimension-subspace}
  Let 
  \begin{equation*}
    V=\set{\left.
      \begin{mymatrix}{c} x\\ y\\ z\end{mymatrix}\in\R^3 ~\right\vert~
      x-y+2z = 0
    }.
  \end{equation*}
  What is the dimension of\/ $V$?
\end{example}

\begin{solution}
  We know that $V$ is a subspace of $\R^3$, because it is the solution
  space of a system of a homogeneous system of equations (in this
  case, one equation in three variables). We can take $y=t$ and $z=s$
  as the free variables and solve for $x=y-2z=t-2s$. Therefore, a
  general element of $V$ is of the form
  \begin{equation*}
    \begin{mymatrix}{c} x\\ y\\ z\end{mymatrix}
    = \begin{mymatrix}{c} t-2s \\ t \\ s \end{mymatrix}
    = t \begin{mymatrix}{c} 1 \\ 1 \\ 0 \end{mymatrix}
    + s \begin{mymatrix}{c} -2 \\ 0 \\ 1 \end{mymatrix}.
  \end{equation*}
  Thus,
  \begin{equation*}
    V = \sspan\set{
      \begin{mymatrix}{c} 1 \\ 1 \\ 0 \end{mymatrix},~
      \begin{mymatrix}{c} -2 \\ 0 \\ 1 \end{mymatrix}
    }.
  \end{equation*}
  Since the two spanning vectors are linearly independent, they form a
  basis of $V$, and thus $\dim(V)=2$. 
\end{solution}

Note that the dimension of the solution space of a system of equations
is equal to the number of parameters in the general solution, which is
equal to the number of free variables. For this reason, the dimension
is also sometimes called the number of \textbf{degrees of freedom}%
\index{degree of freedom}%
\index{freedom!degree of}.

\begin{example}{Dimension of a span}{dimension-basis}
  Let 
  \begin{equation*}
    W = \sspan\set{
      \begin{mymatrix}{r} 1 \\ 2 \\ -1 \\ 1 \end{mymatrix},
      \begin{mymatrix}{r} 1 \\ 3 \\ -1 \\ 1 \end{mymatrix},
      \begin{mymatrix}{r} 8 \\ 19 \\ -8 \\ 8 \end{mymatrix},
      \begin{mymatrix}{r} -6 \\ -15 \\ 6 \\ -6 \end{mymatrix},
      \begin{mymatrix}{r} 1 \\ 3 \\ 0 \\ 1 \end{mymatrix},
      \begin{mymatrix}{r} 1 \\ 5 \\ 0 \\ 1 \end{mymatrix}
    }.
  \end{equation*}
  What is the dimension of $W$?
\end{example}

\begin{solution}
  Let
  \begin{equation*}
    \vect{u}_1 = \begin{mymatrix}{r} 1 \\ 2 \\ -1 \\ 1 \end{mymatrix},~~
    \vect{u}_2 = \begin{mymatrix}{r} 1 \\ 3 \\ -1 \\ 1 \end{mymatrix},~~
    \vect{u}_3 = \begin{mymatrix}{r} 8 \\ 19 \\ -8 \\ 8 \end{mymatrix},~~
    \vect{u}_4 = \begin{mymatrix}{r} -6 \\ -15 \\ 6 \\ -6 \end{mymatrix},~~
    \vect{u}_5 = \begin{mymatrix}{r} 1 \\ 3 \\ 0 \\ 1 \end{mymatrix},~~
    \vect{u}_6 = \begin{mymatrix}{r} 1 \\ 5 \\ 0 \\ 1 \end{mymatrix},
  \end{equation*}
  so that $W=\sspan\set{\vect{u}_1,\ldots,\vect{u}_6}$.  We use the
  casting-out algorithm to remove any redundant vectors from
  $\vect{u}_1,\ldots,\vect{u}_6$. The remaining vectors will be
  linearly independent, and therefore a basis of the span.
  \begin{equation*}
    \begin{mymatrix}{rrrrrr}
      1 & 1 & 8 & -6 & 1 & 1 \\ 
      2 & 3 & 19 & -15 & 3 & 5 \\ 
      -1 & -1 & -8 & 6 & 0 & 0 \\ 
      1 & 1 & 8 & -6 & 1 & 1
    \end{mymatrix}
    \quad\sim\quad
    \begin{mymatrix}{rrrrrr}
      \circled{1} & 0 & 5 & -3 & 0 & -2 \\ 
      0 & \circled{1} & 3 & -3 & 0 & 2 \\ 
      0 & 0 & 0 & 0 & \circled{1} & 1 \\ 
      0 & 0 & 0 & 0 & 0 & 0
    \end{mymatrix}.
  \end{equation*}
  Therefore, the vectors $\vect{u}_3$, $\vect{u}_4$, and $\vect{u}_6$
  are redundant, and $\set{\vect{u}_1,\vect{u}_2,\vect{u}_5}$ is a
  basis of $W$. It follows that $\dim(W)=3$. 
\end{solution}

% ======================================================================
\subsection{CONTINUE HERE}

We continue by stating further properties of a set of vectors in
$\R^{n}$.

\begin{corollary}{Linearly independent and spanning sets in  $\R^{n}$}{independent-spanning-Rn}
  The following properties hold in $\R^{n}$:
  \begin{itemize}
  \item Suppose $\set{\vect{u}_{1},\ldots ,\vect{u}_{n}} $ is linearly
    independent. Then $\set{\vect{u}_{1},\ldots ,\vect{u}_{n}} $ is a
    basis for $\R^{n}$.
  \item Suppose $\set{\vect{u}_{1},\ldots ,\vect{u}_{m}} $ spans
    $\R^{n}$. Then $m\geq n$.
  \item If $\set{\vect{u}_{1},\ldots ,\vect{u}_{n}} $ spans $\R^{n}$,
    then $\set{\vect{u}_{1},\ldots ,\vect{u}_{n}} $ is linearly
    independent.
  \end{itemize}
\end{corollary}

\begin{proof}
  Assume first that $\set{\vect{u}_{1},\ldots ,\vect{u}_{n}} $ is
  linearly independent, and we need to show that this set spans
  $\R^{n}$. To do so, let $\vect{v}$ be a vector of\/ $\R^{n}$, and we
  need to write $\vect{v}$ as a linear combination of $\vect{u}_i$'s.
  Consider the matrix $A$ having the vectors $\vect{u}_i$ as columns:
  \begin{equation*}
    A = 
    \begin{mymatrix}{rrr}
      \vect{u}_{1} & \ldots & \vect{u}_{n} 
    \end{mymatrix}
  \end{equation*}
  By linear independence of the $\vect{u}_i$'s, the {\rref} of $A$ is
  the identity matrix.  Therefore the system $A\vect{x}= \vect{v}$ has
  a (unique) solution, so $\vect{v}$ is a linear combination of the
  $\vect{u}_i$'s.

  To establish the second claim, suppose that $m<n$. Then letting
  $\vect{u}_{i_{1}},\ldots ,\vect{u}_{i_{k}}$ be the pivot columns of
  the matrix
  \begin{equation*}
    \begin{mymatrix}{ccc}
      \vect{u}_{1} & \ldots & \vect{u}_{m}
    \end{mymatrix}
  \end{equation*}
  it follows $k\leq m<n$ and these $k$ pivot columns would be a basis
  for $\R^{n}$ having fewer than $n$ vectors, contrary to
  Corollary~\ref{cor:dimension-Rn}.

  Finally consider the third claim. If $\set{\vect{u}_{1},\ldots
    ,\vect{u}_{n}} $ is not linearly independent, then replace this
  list with $\set{\vect{u}_{i_{1}},\ldots ,\vect{u}_{i_{k}}} $ where these
  are the pivot columns of the matrix 
  \begin{equation*}
    \begin{mymatrix}{ccc}
      \vect{u}_{1} & \ldots & \vect{u}_{n}
    \end{mymatrix}
  \end{equation*}
  Then $\set{\vect{u}_{i_{1}},\ldots ,\vect{u}_{i_{k}}} $ spans
  $\R^{n}$ and is linearly independent, so it is a basis having
  less than $n$ vectors again contrary to Corollary~\ref{cor:dimension-Rn}.
\end{proof}

The next theorem follows from the above claim.

\begin{theorem}{Existence of basis}{existence-basis}
  Let $V$ be a subspace of\/ $\R^n$. Then there exists a basis of $V$ with 
  $\dim(V)\leq n$.
\end{theorem}

Consider Corollary~\ref{cor:independent-spanning-Rn} together with Theorem~\ref{thm:existence-basis}. Let $\dim(V) = r$. Suppose there exists an independent set of vectors in $V$. If this set contains $r$ vectors, then it is a basis for $V$. If it contains less than $r$ vectors, then vectors can be added to the set to create a basis of $V$. Similarly, any spanning set of $V$ which contains more than $r$ vectors can have vectors removed to create a basis of $V$.

We illustrate this concept in the next example.

\begin{example}{Extending an independent set}{extend-independent}
  Consider the set $U$ given by 
  \begin{equation*}
    U=\set{\left.\begin{mymatrix}{c} a\\ b\\ c\\ d\end{mymatrix}
        \in\R^4 \right\vert a-b=d-c
    }
  \end{equation*}
  Then $U$ is a subspace of\/ $\R^4$ and $\dim(U)=3$.

  Then
  \begin{equation*}
    S=\set{
      \begin{mymatrix}{c} 1\\ 1\\ 1\\ 1\end{mymatrix},
      \begin{mymatrix}{c} 2\\ 3\\ 3\\ 2\end{mymatrix}
    },
  \end{equation*}
  is an independent subset of $U$.
  Therefore $S$ can be extended to a basis of $U$.
\end{example}

\begin{solution}
  To extend $S$ to a basis of $U$, find a vector in $U$ that is {\bf not} in
  $\sspan(S)$.
  \begin{equation*}
    \begin{mymatrix}{rrr}
      1 & 2 & ? \\
      1 & 3 & ? \\
      1 & 3 & ? \\
      1 & 2 & ? 
    \end{mymatrix}
  \end{equation*}

  \begin{equation*}
    \begin{mymatrix}{rrr}
      1 & 2 & 1 \\
      1 & 3 & 0 \\
      1 & 3 & -1 \\
      1 & 2 & 0 
    \end{mymatrix}
    \rightarrow
    \begin{mymatrix}{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1 \\
      0 & 0 & 0 
    \end{mymatrix}
  \end{equation*}

  Therefore, $S$ can be extended to the following basis of $U$:
  \begin{equation*}
    \set{
      \begin{mymatrix}{r} 1\\ 1\\ 1\\ 1\end{mymatrix},
      \begin{mymatrix}{r} 2\\ 3\\ 3\\ 2\end{mymatrix},
      \begin{mymatrix}{r} 1\\ 0\\ -1\\ 0\end{mymatrix}
    },
  \end{equation*}
\end{solution}

Next we consider the case of removing vectors from a spanning set to
result in a basis.

\begin{theorem}{Finding a basis from a span}{}
  Let $W$ be a subspace. Also suppose that
  $W=\sspan\set{\vect{w} _{1},\ldots ,\vect{w}_{m}}$. Then there
  exists a subset of $\set{ \vect{w}_{1},\ldots ,\vect{w}_{m}} $ which
  is a basis for $W$.\index{spanning set!basis}
\end{theorem}

\begin{proof}
  Let $S$ denote the set of positive integers such that for $ k\in S$,
  there exists a subset of $\set{\vect{w}_{1},\ldots ,\vect{w}_{m}} $
  consisting of exactly $k$ vectors which is a spanning set for
  $W$. Thus $m\in S$. Pick the smallest positive integer in $S$. Call
  it $k$. Then there exists
  $\set{\vect{u}_{1},\ldots , \vect{u}_{k}} \subseteq
  \set{\vect{w}_{1},\ldots ,\vect{w} _{m}} $ such that
  $\sspan\set{\vect{u}_{1},\ldots ,\vect{u} _{k}} =W$. If
  \begin{equation*}
    \sum_{i=1}^{k}c_{i}\vect{w}_{i}=\vect{0}
  \end{equation*}
  and not all of the $c_{i}=0$, then you could pick $c_{j}\neq 0$,
  divide by it and solve for $\vect{u}_{j}$ in terms of the others,
  \begin{equation*}
    \vect{w}_{j}=\sum_{i\neq j}\tup{-\frac{c_{i}}{c_{j}}} \vect{w}_{i}
  \end{equation*}
  Then you could delete $\vect{w}_{j}$ from the list and have the same
  span. Any linear combination involving $\vect{w}_{j}$ would equal
  one in which $\vect{w}_{j}$ is replaced with the above sum, showing
  that it could have been obtained as a linear combination of
  $\vect{w}_{i}$ for $i\neq j$. Thus $k-1\in S$ contrary to the choice
  of $k$ . Hence each $c_{i}=0$ and so
  $\set{\vect{u}_{1},\ldots ,\vect{u} _{k}} $ is a basis for $W$
  consisting of vectors of $\set{\vect{w} _{1},\ldots ,\vect{w}_{m}}$.
\end{proof}

The following example illustrates how to carry out this shrinking
process which will obtain a subset of a span of vectors which is
linearly independent.

Consider the following theorems regarding a subspace contained in
another subspace.

\begin{theorem}{Subset of a subspace}{subset-dimension}
  Let $V$ and $W$ be subspaces of\/ $\R^n$, and suppose that
  $W\subseteq V$.  Then $\dim(W) \leq \dim(V)$ with equality when
  $W=V$.
\end{theorem}

\begin{theorem}{Extending a basis}{extending-basis}
  Let $W$ be any non-zero subspace $\R^{n}$ and let $W\subseteq V$
  where $V$ is also a subspace of\/ $\R^{n}$. Then every basis of $W$
  can be extended to a basis for $V$.\index{extending a basis}
\end{theorem}

The proof is left as an exercise but proceeds as follows. Begin with a
basis for $W,\set{\vect{w}_{1},\ldots ,\vect{w}_{s}} $ and add in
vectors from $V$ until you obtain a basis for $V$.  Not that the
process will stop because the dimension of $V$ is no more than $n$.

Consider the following example.

\begin{example}{Extending a basis}{extending-basis}
  Let $V=\R^{4}$ and let 
  \begin{equation*}
    W=\sspan\set{
      \begin{mymatrix}{c} 1 \\ 0 \\ 1 \\ 1 \end{mymatrix},
      \begin{mymatrix}{c} 0 \\ 1 \\ 0 \\ 1 \end{mymatrix}
    }
  \end{equation*}
  Extend this basis of $W$ to a basis of\/ $\R^{n}$.
\end{example}

\begin{solution}
  An easy way to do this is to take the {\rref} of the matrix
  \begin{equation}
    \begin{mymatrix}{cccccc}
      1 & 0 & 1 & 0 & 0 & 0 \\ 
      0 & 1 & 0 & 1 & 0 & 0 \\ 
      1 & 0 & 0 & 0 & 1 & 0 \\ 
      1 & 1 & 0 & 0 & 0 & 1
    \end{mymatrix}  \label{basis-eq1}
  \end{equation}
  Note how the given vectors were placed as the first two columns and
  then the matrix was extended in such a way that it is clear that the
  span of the columns of this matrix yield all of\/ $\R^{4}$. Now
  determine the pivot columns.  The {\rref} is
  \begin{equation}
    \begin{mymatrix}{rrrrrr}
      1 & 0 & 0 & 0 & 1 & 0 \\ 
      0 & 1 & 0 & 0 & -1 & 1 \\ 
      0 & 0 & 1 & 0 & -1 & 0 \\ 
      0 & 0 & 0 & 1 & 1 & -1
    \end{mymatrix}  \label{basis-eq2}
  \end{equation}
  Therefore the pivot columns are 
  \begin{equation*}
    \begin{mymatrix}{c} 1 \\ 0 \\ 1 \\ 1 \end{mymatrix},
    \begin{mymatrix}{c} 0 \\ 1 \\ 0 \\ 1 \end{mymatrix},
    \begin{mymatrix}{c} 1 \\ 0 \\ 0 \\ 0 \end{mymatrix},
    \begin{mymatrix}{c} 0 \\ 1 \\ 0 \\ 0 \end{mymatrix}
  \end{equation*}
  and now this is an extension of the given basis for $W$ to a basis
  for $ \R^{4}$.

  Why does this work? The columns of {\eqref{basis-eq1}} obviously
  span $\R^{4}$. In fact the span of the first four is the same as the
  span of all six.
\end{solution}

Consider another example.

\begin{example}{Extending a basis}{}
  Let $W$ be the span of
  $\begin{mymatrix}{c} 1 \\ 0 \\ 1 \\ 0 \end{mymatrix}$ in
  $\R^{4}$. Let $V$ consist of the span of the vectors
  \begin{equation*}
    \begin{mymatrix}{c} 1 \\ 0 \\ 1 \\ 0 \end{mymatrix},
    \begin{mymatrix}{c} 0 \\ 1 \\ 1 \\ 1 \end{mymatrix},
    \begin{mymatrix}{r} 7 \\ -6 \\ 1 \\ -6 \end{mymatrix},
    \begin{mymatrix}{r} -5 \\ 7 \\ 2 \\ 7 \end{mymatrix},
    \begin{mymatrix}{c} 0 \\ 0 \\ 0 \\ 1 \end{mymatrix}
  \end{equation*}
  Find a basis for $V$ which extends the basis for $W$.
\end{example}

\begin{solution}
  Note that the above vectors are not linearly independent, but their
  span, denoted as $V$ is a subspace which does include the subspace
  $W$.

  Using the process outlined in the previous example, form the
  following matrix
  \begin{equation*}
    \begin{mymatrix}{rrrrr}
      1 & 0 & 7 & -5 & 0 \\ 
      0 & 1 & -6 & 7 & 0 \\ 
      1 & 1 & 1 & 2 & 0 \\ 
      0 & 1 & -6 & 7 & 1
    \end{mymatrix}
  \end{equation*}
  Next find its {\rref}
  \begin{equation*}
    \begin{mymatrix}{rrrrr}
      1 & 0 & 7 & -5 & 0 \\ 
      0 & 1 & -6 & 7 & 0 \\ 
      0 & 0 & 0 & 0 & 1 \\ 
      0 & 0 & 0 & 0 & 0
    \end{mymatrix}
  \end{equation*}
  It follows that a basis for $V$ consists of the first two vectors
  and the last.
  \begin{equation*}
    \set{\begin{mymatrix}{c} 1 \\ 0 \\ 1 \\ 0 \end{mymatrix},
      \begin{mymatrix}{c} 0 \\ 1 \\ 1 \\ 1 \end{mymatrix},
      \begin{mymatrix}{c} 0 \\ 0 \\ 0 \\ 1 \end{mymatrix}
    }
  \end{equation*}
  Thus $V$ is of dimension 3 and it has a basis which extends the
  basis for $W$.
\end{solution}

% ----------------------------------------------------------------------
\subsection{CONTINUE HERE}

% ----------------------------------------------------------------------
We are now prepared to examine the precise definition of a subspace as
follows.

\begin{definition}{Subspace}{subspace-alt}
  Let $V$ be a nonempty collection of vectors in $\R^{n}$. Then $V$ is
  called a subspace\index{subspace!of Rn@of\/ $\R^n$} if whenever $a$
  and $b$ are scalars and $\vect{u}$ and $\vect{v}$ are vectors in
  $V$, the linear combination $a \vect{u}+ b \vect{v}$ is also in $V$.
\end{definition}

More generally this means that a subspace contains the span of any
finite collection vectors in that subspace. It turns out that in
$\R^{n}$, a subspace is exactly the span of finitely many of its
vectors.


