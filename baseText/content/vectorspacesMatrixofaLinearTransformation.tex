\section{The matrix of a linear transformation}

\begin{outcome}
  \begin{enumerate}
  \item Find the matrix of a linear transformation with respect to
    general bases in vector spaces.
  \end{enumerate}
\end{outcome}

Recall that, by definition, two linear transformations $S,T:V\to W$
are equal if and only if for all $\vect{v}\in V$, we have
$S(\vect{v}) = T(\vect{v})$.  However, this is not a very practical
way of checking whether $S=T$, as it theoretically requires checking
$S(\vect{v}) = T(\vect{v})$ for each one of infinitely many vectors
$\vect{v}$. The following proposition states that it is sufficient to
check the actions of $S$ and $T$ on a spanning set of vectors.

\begin{proposition}{Equality of linear transformations}{transformation-spanning-set}
  Let $V$ and $W$ be vector spaces over a field $K$, and let
  $S,T:V\to W$ be linear transformations. Moreover, let $X\subseteq V$
  be a spanning set of $V$, i.e., such that $V=\sspan X$.
  If $S(\vect{v}) = T(\vect{v})$ for all $v\in X$, then $S=T$.%
  \index{linear transformation!equality of}%
  \index{equality!of linear transformations}
\end{proposition}

\begin{proof}
  Assume that $S(\vect{v}) = T(\vect{v})$ holds for all $v\in X$. To
  show that $S=T$, let $\vect{u}\in V$ be an arbitrary vector. Since
  $X$ is a spanning set, we can write
  $\vect{u}=a_1\vect{v}_1+\ldots+a_n\vect{v}_n$, for some
  $\vect{v}_1,\ldots,\vect{v}_n\in X$ and $a_1,\ldots,a_n\in K$.
  By assumption, $S(\vect{v}_i) = T(\vect{v}_i)$ for all $i$, because
  $\vect{v}_i\in X$. Then we have
  \begin{eqnarray*}
    S(\vect{u})
    &=& S(a_1\vect{v}_1+\ldots+a_n\vect{v}_n \\
    &=& a_1S(\vect{v}_1)+\ldots+a_nS(\vect{v}_n) \\
    &=& a_1T(\vect{v}_1)+\ldots+a_nT(\vect{v}_n) \\
    &=& T(a_1\vect{v}_1+\ldots+a_n\vect{v}_n \\
    &=& T(\vect{u}).
  \end{eqnarray*}
  Since $\vect{u}\in V$ was arbitrary, it follows that $S=T$.
\end{proof}

Therefore, if we know how a linear transformation acts on a spanning
set (and in particular, on a basis), then we know how it acts on the
entire space. There is also a kind of converse to this: given a basis
of $V$, we can map the basis vectors to any elements of $W$ we like,
and this will always determine a unique linear transformations. This
is the content of the following theorem.

\begin{theorem}{Linear transformation defined on a basis}{transformation-basis}
  Suppose $V$ and $W$ are vector spaces over a field $K$. Suppose
  $\vect{v}_1,\vect{v}_2,\ldots,\vect{v}_n$ is a basis of $V$, and
  $\vect{w}_1,\vect{w}_2,\ldots,\vect{w}_n$ are any vectors in $W$
  (which may or may not be distinct). Then there exists a unique
  linear transformation%
  \index{linear transformation!defined on a basis}
  $T:V\to W$ such that
  \begin{equation*}
    T(\vect{v}_1) = \vect{w}_1,
    \quad
    T(\vect{v}_2) = \vect{w}_2,
    \quad\ldots\quad
    T(\vect{v}_n) = \vect{w}_n.
  \end{equation*}
\end{theorem}

\begin{proof}
  To show that such a linear transformation $T$ exists, we first
  define a function $T:V\to W$ as follows. Given any $\vect{v}\in V$,
  there exists a unique set of coordinates $a_1,\ldots,a_n\in K$ such
  that
  \begin{equation*}
    \vect{v} = a_1\vect{v}_1 + \ldots + a_n\vect{v}_n.
  \end{equation*}
  Then define
  \begin{equation*}
    T(\vect{v}) = a_1\vect{w}_1 + \ldots + a_n\vect{w}_n.
  \end{equation*}
  This defines a function $T:V\to W$.  Next, we must check that $T$ is
  linear. To show that $T$ preserves addition, consider
  $\vect{v},\vect{v}'\in V$, with
  $\vect{v} = a_1\vect{v}_1 + \ldots + a_n\vect{v}_n$ and
  $\vect{v}' = b_1\vect{v}_1 + \ldots + b_n\vect{v}_n$.  Then
  \begin{eqnarray*}
    T(\vect{v}+\vect{v}')
    &=& T((a_1+b_1)\vect{v}_1 + \ldots + (a_n+b_n)\vect{v}_n) \\
    &=& (a_1+b_1)\vect{w}_1 + \ldots + (a_n+b_n)\vect{w}_n \\
    &=& (a_1\vect{w}_1 + \ldots + a_n\vect{w}_n)
        + (b_1\vect{w}_1 + \ldots + b_n\vect{w}_n) \\
    &=& T(\vect{v}) + T(\vect{v}').
  \end{eqnarray*}
  Therefore, $T$ preserves addition. To show that $T$ preserves scalar
  multiplication, consider $\vect{v} = a_1\vect{v}_1 + \ldots +
  a_n\vect{v}_n$ and $k\in K$. Then
  \begin{eqnarray*}
    T(k\vect{v})
    &=& T(ka_1\vect{v}_1 + \ldots + ka_n\vect{v}_n) \\
    &=& ka_1\vect{w}_1 + \ldots + ka_n\vect{w}_n \\
    &=& k(a_1\vect{w}_1 + \ldots + a_n\vect{w}_n) \\
    &=& kT(\vect{v}).
  \end{eqnarray*}
  Therefore, $T$ preserves scalar multiplication. It follows that $T$
  is linear. Next, we must show that $T$ satisfies the condition of
  the theorem, i.e., that $T(\vect{v}_i) = \vect{w}_i$ for each $i$.
  But this is clearly the case, because in this case, $a_i=1$ and
  $a_j=0$ for all $j\neq i$. We have shown that there exists a linear
  function $T$ satisfying all of the conditions required by the
  theorem.

  Finally, the only thing left to show is uniqueness. But this follows
  from Proposition~\ref{prop:transformation-spanning-set}. Namely, if
  $T'$ is another linear transformation such that
  $T'(\vect{v}_i) = \vect{w}_i$ for all $i$, then $T$ and $T'$ agree
  on $\vect{v}_1,\ldots,\vect{v}_n$, which is a basis and hence a
  spanning set. By Proposition~\ref{prop:transformation-spanning-set},
  $T=T'$.
\end{proof}

\begin{example}{Linear transformation defined on a basis}{transformation-basis}
  Recall that $\set{x^2, (x+1)^2, (x+2)^2}$ is a basis of $\Poly_2$.
  Consider the linear function $T:\Poly_2\to\Mat_{22}$ defined by
  \begin{equation*}
    T(x^2) = \begin{mymatrix}{rr} 1 & 1 \\ 0 & 0 \end{mymatrix},\quad
    T((x+1)^2) = \begin{mymatrix}{rr} 0 & 1 \\ 0 & 1 \end{mymatrix},\quad
    T((x+2)^2) = \begin{mymatrix}{rr} 0 & 0 \\ 1 & 1 \end{mymatrix}.
  \end{equation*}
  Find $T(4x)$.
\end{example}

\begin{solution}
  Let $\vect{v}_1=x^2$, $\vect{v}_2=(x+1)^2$, $\vect{v}_3=(x+2)^2$,
  $\vect{w}_1 = \begin{mysmallmatrix}{rr} 1 & 1 \\ 0 & 0 \end{mysmallmatrix}$,
  $\vect{w}_2 = \begin{mysmallmatrix}{rr} 0 & 1 \\ 0 & 1 \end{mysmallmatrix}$,
  and
  $\vect{w}_3 = \begin{mysmallmatrix}{rr} 0 & 0 \\ 1 & 1 \end{mysmallmatrix}$.
  We must first find $a,b,c$ such that
  $4x=a\vect{v}_1+b\vect{v}_2+c\vect{v}_3$. We do this by solving a
  system of equations, using the same method as in
  Example~\ref{exa:linear-combination-polynomials}. We find that
  $a=-3$, $b=4$, and $c=-1$.  Therefore
  \begin{equation*}
    T(x)
    ~=~ T(-3\vect{v}_1 + 4\vect{v}_2 - \vect{v}_3)
    ~=~ -3\vect{w}_1 + 4\vect{w}_2 - \vect{w}_3
    ~=~ \begin{mymatrix}{rr}
      -3 & 1 \\
      -1 & 3 \\
    \end{mymatrix}.
  \end{equation*}
\end{solution}

Let $V$ be a vector space with basis
$B=\set{\vect{v}_1,\ldots,\vect{v}_n}$. Recall from
Section~\ref{ssec:bases-and-coordinates} that the \textbf{coordinates}
of a vector $\vect{v}$ with respect to the basis $B$%
\index{coordinate!with respect to basis}%
\index{coordinate system!and basis} are the unique scalars
$a_1,\ldots,a_n$ such that
\begin{equation*}
  \vect{v} = a_1\,\vect{v}_1 + \ldots + a_n\,\vect{v}_n.
\end{equation*}
As before, we write
\begin{equation*}
  \coord{\vect{v}}_B = \begin{mymatrix}{c} a_1 \\ a_2 \\ a_3 \end{mymatrix}
\end{equation*}
to denote the coordinates of $\vect{v}$ with respect to the basis $B$.
We will now see how to use bases and coordinates to encode any linear
map between finite-dimensional vector spaces as a matrix. 

\begin{proposition}{The matrix of a linear transformation}{matrix-of-linear-transformation}
  Let $V$ and $W$ be vector spaces over a field $K$. Assume
  $B=\set{\vect{v}_1,\ldots,\vect{v}_n}$ is a basis of $V$, and
  $C=\set{\vect{w}_1,\ldots,\vect{w}_m}$ is a basis of $W$.
  Let $T:V\to W$ be a linear transformation. Then there exists a
  unique $m\times n$-matrix $A$ such that for all $\vect{v}\in V$,
  \begin{equation*}
    A\coord{\vect{v}}_B = \coord{T\vect{v}}_C.
  \end{equation*}
\end{proposition}

\begin{proof}
  By Theorem~\ref{thm:transformation-basis}, the linear transformation
  $T$ is completely determined by the images of the basis vectors,
  $T(\vect{v}_1),\ldots,T(\vect{v}_n)\in W$. Since
  $\set{\vect{w}_1,\ldots,\vect{w}_m}$ is a basis of $W$, we can write
  each $T(\vect{v}_i)$ as a linear combination of
  $\vect{w}_1,\ldots,\vect{w}_m$:
  \begin{eqnarray*}
    T(\vect{v}_1)
    &=& a_{11}\vect{w}_1 + a_{21}\vect{w}_2 + \ldots + a_{m1}\vect{w}_m, \\
    T(\vect{v}_2)
    &=& a_{12}\vect{w}_1 + a_{22}\vect{w}_2 + \ldots + a_{m2}\vect{w}_m, \\
    &\cdots& \\
    T(\vect{v}_n)
    &=& a_{1n}\vect{w}_1 + a_{2n}\vect{w}_2 + \ldots + a_{mn}\vect{w}_m.
  \end{eqnarray*}
  Let
  \begin{equation*}
    A = \begin{mymatrix}{cccc}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2n} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{m1} & a_{m2} & \cdots & a_{mn} \\
    \end{mymatrix}.
  \end{equation*}
  Then $A$ is an $m\times n$-matrix. We must prove that it has the
  desired property, i.e., that
  $A\coord{\vect{v}}_B = \coord{T\vect{v}}_C$, for all
  $\vect{v}\in V$. Since both the left-hand side and the right-hand
  side are linear functions of $\vect{v}$, it suffices to check that
  this property holds for basis vectors. Consider, therefore, one of
  the basis vectors $\vect{v}_i$.  Note that
  $\vect{v}_i = 0\vect{v}_1 + 0\vect{v}_2 + \ldots + 1\vect{v}_i +
  \ldots + 0\vect{v}_n$. Therefore, the coordinates of $\vect{v}_i$
  with respect to the basis $B$ are
  \begin{equation*}
    \coord{\vect{v}_i}_B =
    \begin{mysmallmatrix}{c}
      0 \\ \vdots \\ 1 \\ \vdots \\ 0
    \end{mysmallmatrix}
    = \vect{e}_i,
  \end{equation*}
  where $\vect{e}_i$ is the usual $i\th$ standard basis vector.  On
  the other hand, since
  $T(\vect{v}_i) = a_{1i}\vect{w}_1 + \ldots + a_{mi}\vect{w}_m$,
  we have
  \begin{equation*}
    \coord{T(\vect{v}_i)}_C =
    \begin{mysmallmatrix}{c}
      a_{1i} \\ \vdots \\ a_{mi}
    \end{mysmallmatrix}
    = A\vect{e}_i.
  \end{equation*}
  Here, in the last equation, we have used the fact that $A\vect{e}_i$
  is the same thing as the $i\th$ column of $A$. We therefore have
  $\coord{T(\vect{v}_i)}_C = A\vect{e}_i = A \coord{\vect{v}_i}_B$, as
  desired.
\end{proof}

\begin{definition}{The matrix of a linear transformation}{matrix-of-linear-transformation}
  The matrix $A$ is called the \textbf{matrix of the linear
    transformation\/ $T$ with respect to the bases $B$ and $C$}%
  \index{linear transformation!matrix of!w.r.t. basis}%
  \index{matrix!of a linear transformation!w.r.t. basis}.
  We also write
  \begin{equation*}
    A = \coord{T}_{C,B}.
  \end{equation*}
  Therefore,
  \begin{equation*}
    \coord{T}_{C,B}\coord{\vect{v}}_B = \coord{T\vect{v}}_C
  \end{equation*}
  for all $\vect{v}\in V$.
\end{definition}

\begin{example}{The matrix of a linear transformation}{matrix-of-linear-transformation}
  Find the matrix of the derivative operator $D:\Poly_3\to\Poly_2$
  with respect to the basis $B=\set{1,x,x^2,x^3}$ of $\Poly_3$ and the
  basis $C=\set{1,x,x^2}$ of $\Poly_2$.
\end{example}

\begin{solution}
  We first find the images of each basis vector of the basis $B$, and
  we write each of them as a linear combination of basis vectors from
  the basis $C$. Let us denote the basis vectors of $B$ as
  $\vect{v}_1 = 1$, $\vect{v}_2 = x$, $\vect{v}_3 = x^2$, and
  $\vect{v}_4=x^3$, and the basis vectors of $C$ as $\vect{w}_1 = 1$,
  $\vect{w}_2 = x$, and $\vect{w}_3 = x^2$. We have
  \begin{equation*}
    \begin{array}{rclclclcl}
      D(\vect{v}_1) &=& D(1) &=& 0 &=& 0 + 0x + 0x^2 &=& 0\vect{w}_1 + 0\vect{w}_2 + 0\vect{w}_3, \\
      D(\vect{v}_2) &=& D(x) &=& 1 &=& 1 + 0x + 0x^2 &=& 1\vect{w}_1 + 0\vect{w}_2 + 0\vect{w}_3, \\
      D(\vect{v}_3) &=& D(x^2) &=& 2x &=& 0 + 2x + 0x^2 &=& 0\vect{w}_1 + 2\vect{w}_2 + 0\vect{w}_3, \\
      D(\vect{v}_4) &=& D(x^3) &=& 3x^2 &=& 0 + 0x + 3x^2 &=& 0\vect{w}_1 + 0\vect{w}_2 + 3\vect{w}_3. \\
    \end{array}
  \end{equation*}
  Therefore, we have
  \begin{equation*}
    A=\coord{D}_{C,B} =
    \begin{mymatrix}{rrrr}
      0 & 1 & 0 & 0 \\
      0 & 0 & 2 & 0 \\
      0 & 0 & 0 & 3 \\
    \end{mymatrix}.
  \end{equation*}
\end{solution}

\begin{example}{The matrix of a linear transformation}{matrix-of-linear-transformation2}
  Find the matrix of the derivative operator $D:\Poly_3\to\Poly_2$
  with respect to the basis $B'=\set{1,x+1,x^2+x+1,x^3+x^2+x+1}$ of
  $\Poly_3$ and the basis $C'=\set{1,x-1,x^2-1}$ of $\Poly_2$.
\end{example}

\begin{solution}
  This is the same linear transformation as in the previous example,
  but we are given different bases. Let us denote the basis vectors of
  $B'$ as $\vect{v}_1 = 1$, $\vect{v}_2 = x+1$, $\vect{v}_3 = x^2+x+1$, and
  $\vect{v}_4=x^3+x^2+x+1$, and the basis vectors of $C'$ as $\vect{w}_1 = 1$,
  $\vect{w}_2 = x-1$, and $\vect{w}_3 = x^2-1$. We must write each
  $D(\vect{v}_i)$ as a linear combination of $\vect{w}_1$,
  $\vect{w}_2$, and $\vect{w}_3$, which requires solving a system of
  linear equations for each of them. We have:
  \begin{equation*}
    \begin{array}{rclclcl}
      D(\vect{v}_1) &=& D(1) &=& 0 &=& 0\vect{w}_1 + 0\vect{w}_2 + 0\vect{w}_3, \\
      D(\vect{v}_2) &=& D(x+1) &=& 1 &=& 1\vect{w}_1 + 0\vect{w}_2 + 0\vect{w}_3, \\
      D(\vect{v}_3) &=& D(x^2+x+1) &=& 2x+1 &=& 3\vect{w}_1 + 2\vect{w}_2 + 0\vect{w}_3, \\
      D(\vect{v}_4) &=& D(x^3+x^2+x+1) &=& 3x^2+2x+1 &=& 6\vect{w}_1 + 2\vect{w}_2 + 3\vect{w}_3. \\
    \end{array}
  \end{equation*}
  Therefore, the matrix is
  \begin{equation*}
    \coord{D}_{C',B'} =
    \begin{mymatrix}{rrrr}
      0 & 1 & 3 & 6 \\
      0 & 0 & 2 & 2 \\
      0 & 0 & 0 & 3 \\
    \end{mymatrix}.
  \end{equation*}
\end{solution}

The last two examples illustrate that a linear transformation can have
many different matrices, because the matrix depends not only on the
linear transformation, but also on the given bases. The art of linear
algebra often lies in choosing ``convenient'' bases for a given
application. Often, a ``convenient'' basis is one that gives rise to
simple matrices, for example, matrices containing many zeros, or
matrices that are diagonal.


% ----------------------------------------------------------------------
\subsection{CONTINUE HERE...}

Properties: Matrix of composition, identity, inverse. Matrix of
addition, scalar multiplication.


% ----------------------------------------------------------------------

We can also find the matrix of the composite of multiple transformations.

\begin{theorem}{Matrix of composition}{matrix-composition}
  Let $V,W$ and $U$ be finite-dimensional vector spaces, and suppose
  $T : V \to W$, $S: W \to U$ are linear transformations.  Suppose
  $V, W$ and $U$ have ordered bases of $B_1$, $B_2$ and $B_3$
  respectively.  Then the matrix of the composite transformation
  $S \circ T$ (or $ST$) is given by
  \begin{equation*}
    M_{B_3B_1}(ST)=M_{B_3B_2}(S) M_{B_2B_1}(T).
  \end{equation*}
\end{theorem}

The next important theorem gives a condition on when $T$ is an isomorphism.

\begin{theorem}{Isomorphism}{isomorphism}
  Let $V$ and $W$ be vector spaces such that both have dimension $n$
  and let $T: V \to W$ be a linear transformation. Suppose $B_1$ is an
  ordered basis of $V$ and $B_2$ is an ordered basis of $W$.

  Then the conditions that $M_{B_2B_1}(T)$ is invertible for
  \textbf{all} $B_1$ and $B_2$, and that $M_{B_2B_1}(T)$ is invertible
  for \textbf{some} $B_1$ and $B_2$ are equivalent. In fact, these
  occur if and only if $T$ is an isomorphism.

  If $T$ is an isomorphism, the matrix $M_{B_2B_1}(T)$ is invertible
  and its inverse is given by
  $\mat{M_{B_2B_1}(T) } ^{-1} = M_{B_1B_2}(T^{-1})$.
\end{theorem}

Consider the following example.

\begin{example}{}{}
  Suppose $T:\Poly_3\to\Mat_{2,2}$ is a linear transformation
  defined by
  \begin{equation*}
    T(ax^3+bx^2+cx+d)=
    \begin{mymatrix}{cc} a+d & b-c \\ b+c & a-d \end{mymatrix}
  \end{equation*}
  for all $ax^3+bx^2+cx+d\in\Poly_3$. Let
  $B_1=\set{x^3, x^2, x, 1}$ and
  \begin{equation*}
    B_2=\set{
      \begin{mymatrix}{cc} 1 & 0 \\ 0 & 0 \end{mymatrix},
      \begin{mymatrix}{cc} 0 & 1 \\ 0 & 0 \end{mymatrix},
      \begin{mymatrix}{cc} 0 & 0 \\ 1 & 0 \end{mymatrix},
      \begin{mymatrix}{cc} 0 & 0 \\ 0 & 1 \end{mymatrix}}
  \end{equation*}
  be ordered bases of $\Poly_3$ and $\Mat_{2,2}$, respectively.
  \begin{enumerate}
  \item Find $M_{B_2B_1}(T)$.
  \item Verify that $T$ is an isomorphism by proving that $M_{B_2B_1}(T)$
    is invertible.
  \item Find $M_{B_1B_2}(T^{-1})$, and verify that
    $M_{B_1B_2}(T^{-1}) = \mat{M_{B_2B_1}(T)}^{-1}$.
  \item Use $M_{B_1B_2}(T^{-1})$ to find $T^{-1}$.
  \end{enumerate}
\end{example}

\begin{solution}
  \begin{enumerate}
  \item
    \begin{eqnarray*}
      M_{B_2B_1}(T)
      & = &
            \begin{mymatrix}{cccc} C_{B_2}[T(1)] & C_{B_2}[T(x)] & C_{B_2}[T(x^2)]
              & C_{B_2}[T(x^3)] \end{mymatrix} \\
      & = &
            \begin{mymatrix}{cccc}
              C_{B_2}\begin{mymatrix}{cc} 1 & 0 \\ 0 & 1 \end{mymatrix}
              & C_{B_2}\begin{mymatrix}{cc} 0 & 1 \\ 1 & 0 \end{mymatrix}
              & C_{B_2}\begin{mymatrix}{cc} 0 & -1 \\ 1 & 0 \end{mymatrix}
              & C_{B_2}\begin{mymatrix}{cc} 1 & 0 \\ 0 & -1 \end{mymatrix}
            \end{mymatrix} \\
      & = & \begin{mymatrix}{rrrr}
        1 & 0 & 0 & 1 \\
        0 & 1 & -1 & 0 \\
        0 & 1 & 1 & 0 \\
        1 & 0 & 0 & -1 \end{mymatrix}
    \end{eqnarray*}

  \item $\det(M_{B_2B_1}(T))=4$, so the matrix is invertible, and hence $T$
    is an isomorphism.

  \item
    \begin{equation*}
      T^{-1}\begin{mymatrix}{cc} 1 & 0 \\ 0 & 1 \end{mymatrix} = 1,
      T^{-1}\begin{mymatrix}{cc} 0 & 1 \\ 1 & 0 \end{mymatrix}= x,
      T^{-1}\begin{mymatrix}{cc} 0 & -1 \\ 1 & 0 \end{mymatrix}= x^2,
      T^{-1}\begin{mymatrix}{cc} 1 & 0 \\ 0 & -1 \end{mymatrix}=x^3,
    \end{equation*}
    so
    \begin{equation*}
      T^{-1}\begin{mymatrix}{cc} 1 & 0 \\ 0 & 0 \end{mymatrix} = \frac{1+x^3}{2},
      T^{-1}\begin{mymatrix}{cc} 0 & 1 \\ 0 & 0 \end{mymatrix}= \frac{x-x^2}{2},
    \end{equation*}
    \begin{equation*}
      T^{-1}\begin{mymatrix}{cc} 0 & 0 \\ 1 & 0 \end{mymatrix} = \frac{x+x^2}{2},
      T^{-1}\begin{mymatrix}{cc} 0 & 1 \\ 0 & 0 \end{mymatrix}= \frac{1-x^3}{2}.
    \end{equation*}
    Therefore,
    \begin{equation*}
      M_{B_1B_2}(T^{-1})=\frac{1}{2}\begin{mymatrix}{rrrr}
        1 & 0 & 0 & 1 \\
        0 & 1 & 1 & 0 \\
        0 & -1 & 1 & 0 \\
        1 & 0 & 0 & -1 \end{mymatrix}
    \end{equation*}

    You should verify that $M_{B_2B_1}(T) M_{B_1B_2}(T^{-1}) =
    I_4$. From this it follows that
    $[M_{B_2B_1}(T)]^{-1}= M_{B_1B_2}(T^{-1})$.

  \item
    \begin{eqnarray*}
      C_{B_1}\paren{T^{-1}\begin{mymatrix}{cc} p & q \\ r & s \end{mymatrix}}
      & = &
            M_{B_1B_2}(T^{-1})
            C_{B_2}\paren{\begin{mymatrix}{cc}
                p & q \\ r & s \end{mymatrix}}\\
      T^{-1}\begin{mymatrix}{cc}
        p & q \\ r & s \end{mymatrix} & = &
                                            C_{B_1}^{-1}\paren{M_{B_1B_2}(T^{-1})
                                            C_{B_2}\paren{\begin{mymatrix}{cc}
                                                p & q \\ r & s \end{mymatrix}}}\\
            & = &
                  C_{B_1}^{-1}\paren{
                  \frac{1}{2}\begin{mymatrix}{rrrr}
                    1 & 0 & 0 & 1 \\
                    0 & 1 & 1 & 0 \\
                    0 & -1 & 1 & 0 \\
                    1 & 0 & 0 & -1 \end{mymatrix}
                                \begin{mymatrix}{c} p \\ q\\ r\\ s\end{mymatrix}} \\
            & =&
                 C_{B_1}^{-1}\paren{\frac{1}{2}\begin{mymatrix}{c}
                     p+s \\ q+r \\ r-q \\ p-s \end{mymatrix}} \\
            & = & \frac{1}{2}(p+s)x^3 +\frac{1}{2}(q+r)x^2 +\frac{1}{2}(r-q)x
                  + \frac{1}{2}(p-s).
    \end{eqnarray*}
  \end{enumerate}
\end{solution}
