\section{Basis and dimension}

\begin{outcome}
  \begin{enumerate}
  \item Extend a linearly independent set and shrink a spanning set to
    a basis of a given vector space.
  \end{enumerate}
\end{outcome}

\begin{definition}{Basis}{basis-vector-space}
  Let $V$ be a vector space. A set $B$ of vectors is called a
  \textbf{basis}%
  \index{basis}%
  \index{basis!of a vector space}%
  \index{vector space!basis} of $V$ if
  \begin{enumerate}
  \item $B$ is a spanning set for $V$, and
  \item $B$ is linearly independent.
  \end{enumerate}
\end{definition}

\begin{example}{Bases of $\Poly_2$}{basis-p2}
  Consider the vector space $\Poly_2$ of polynomials of degree at most
  2 with coefficients in a field $K$.%
  \index{P2@$\Poly_2$!basis of}
  \begin{itemize}
  \item $\set{1, x, x^2}$ is a basis of $\Poly_2$.
  \item $\set{x^2, (x+1)^2, (x+2)^2}$ is a basis of $\Poly_2$.
  \item $\set{1, x-1, (x-1)^2}$ is a basis of $\Poly_2$.
  \end{itemize}
  Unlike $\R^n$, a vector space like $\Poly_2$ does not
  necessarily have a ``standard'' basis. One basis might be useful for
  one application, and another basis for a different application.
\end{example}

\begin{proof}
  It is easy to verify that each set of vectors is linearly
  independent and spanning. See Examples~\ref{exa:spanning-set},
  {\ref{exa:linear-independence-polynomial}}, and
  {\ref{exa:polynomials-increasing-degree}} for similar calculations.
\end{proof}

\begin{example}{An infinite basis}{basis-p}
  Consider the vector space $\Poly$ of all polynomials with
  coefficients in a field $K$. The following is a basis for $\Poly$%
  \index{P@$\Poly$!basis of}:
  \begin{equation*}
    \set{1, x, x^2, x^3, x^4, \ldots}.
  \end{equation*}
  Note that this basis is infinite.
\end{example}

\begin{proof}
  The polynomials $1, x, x^2, x^3, x^4, \ldots$ are linearly
  independent by Proposition~\ref{prop:linear-dependence-redundant}.
  Namely, if they were linearly dependent, then one of the polynomials
  could be written as a linear combination of earlier ones. However,
  this is not possible because a polynomial of degree $n$ cannot be a
  linear combination of polynomials of degree less than $n$.

  To show that the polynomials $1, x, x^2, x^3, x^4, \ldots$ are a
  spanning set, consider an arbitrary element $p(x)$ of $\Poly$.
  Then by definition, $p(x)$ is of the form
  \begin{equation*}
    p(x) = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_1x + a_0,
  \end{equation*}
  for some $n\geq 0$ and $a_0,\ldots,a_n\in K$. But then $p(x)$ is a
  linear combination of $1,\ldots,x^n$, i.e., it is in the span of
  $\set{1, x, x^2, x^3, x^4, \ldots}$.
\end{proof}

\begin{example}{Not a basis}{not-a-basis}
  Consider the vector space $\Seq_K$ of infinite sequences. As before,
  let $e^k$ be the sequence whose $k\th$ component is $1$ and which is
  $0$ everywhere else, i.e.,
    \begin{equation*}
    \begin{array}{l}
      e^0 = 1,0,0,0,0,\ldots, \\
      e^1 = 0,1,0,0,0,\ldots, \\
      e^2 = 0,0,1,0,0,\ldots, \\
    \end{array}
  \end{equation*}
  and so on. Then the set
  \begin{equation*}
    \set{e^0, e^1, e^2, \ldots}
  \end{equation*}
  is \textit{not} a basis of $\Seq_K$. Indeed, although we saw in
  Example~\ref{exa:linear-independence-sequences} that the sequences
  $e^0, e^1, e^2, \ldots$ are linearly independent,
  Example~\ref{exa:spans-sequences} shows that they are not spanning.
  Indeed,
  \begin{equation*}
    W = \sspan\set{e^0, e^1, e^2, \ldots}
  \end{equation*}
  is a subspace of $\Seq_K$, consisting exactly of the
  \textbf{finitely supported}%
  \index{finitely supported sequence}%
  \index{sequence!finitely supported} sequences, i.e., those sequences
  that have only finitely many non-zero entries. Thus, $\set{e^0, e^1,
    e^2, \ldots}$ is a basis of $W$.
\end{example}

The following theorem ensures that every vector space has a basis.  We
will not prove this theorem, because when the spaces are
infinite-dimensional, the proof uses mathematics that is beyond the
scope of this book. The proof uses a reasoning principle called the
\textbf{axiom of choice}%
\index{axiom of choice}, which allows us to prove the existence of a
basis even in cases where we cannot find an actual concrete example of
a basis.  For example, it is not possible to give a specific example
of a basis for the space $\Seq_K$, even though the following theorem
guarantees that such a basis exists.

\begin{theorem}{Existence of bases}{basis-existence}
  Every vector space has a basis.
\end{theorem}

The Exchange Lemma, which we proved in the context of $\R^n$ in
Section~\ref{sec:basis-and-dimension}, is true in general vector
spaces.

\begin{lemma}{Exchange Lemma}{exchange-lemma-vector-space}
  \index{exchange lemma!in a vector space}%
  Let $V$ be a vector space over a field $K$. Suppose
  $\vect{u}_{1},\ldots,\vect{u}_{r}$ are linearly independent elements
  of $\sspan\set{\vect{v}_{1},\ldots,\vect{v}_{s}}$. Then $r\leq s$.
\end{lemma}

The proof is exactly the same as that of
Lemma~\ref{lem:exchange-lemma}, so we do not repeat it here.  As in
Section~\ref{sec:basis-and-dimension}, an important consequence of the
Exchange Lemma is that any two bases of a vector space have the same
size.

\begin{theorem}{Bases are of the same size}{basis-same-size-vector-space}
  \index{basis!size of}%
  Let $V$ be a vector space over some field $K$, and let $B_1$ and
  $B_2$ be bases of $V$. Then either $B_1$ and $B_2$ are both finite
  and have the same number of elements, or else $B_1$ and $B_2$ are
  both infinite.
\end{theorem}

\begin{proof}
  We first show that $B_1$ and $B_2$ are either both finite or both
  infinite. Assume one of them, say $B_1$, is finite and contains $s$
  vectors. Since $B_1$ is spanning and $B_2$ is linearly independent,
  it follows from the Exchange Lemma that $B_2$ cannot contain more
  than $s$ vectors, and in particular, $B_2$ must be finite.  So the
  sets are either both finite or both infinite. If they are both
  finite, say of size $s$ and $r$, then by the Exchange Lemma, we have
  $s\leq r$ and $r\leq s$, hence $r=s$.
\end{proof}

This allows us to define the dimension of a vector space.

\begin{definition}{Dimension}{dimension-vector-space}
  Let $V$ be a vector space over a field $K$. If $V$ has a basis
  consisting of $n$ vectors, we say that $V$ has \textbf{dimension}%
  \index{dimension!of vector space}%
  \index{vector space!dimension} $n$, and we write $\dim(V)=n$. In
  this case we also say that $V$ is \textbf{finite-dimensional}%
  \index{finite-dimensional}%
  \index{vector space!finite-dimensional}. If $V$ has an infinite
  basis, we say that $V$ is \textbf{infinite-dimensional}%
  \index{infinite-dimensional}%
  \index{space!infinite-dimensional}, and we write $\dim(V) = \infty$.
\end{definition}

Note that the dimension is well-defined by Theorems
{\ref{thm:basis-existence}} and
{\ref{thm:basis-same-size-vector-space}}, since these theorems ensure
that every vector space has a basis (and therefore a dimension), and
that any two bases are of the same size (and therefore a vector space
cannot have more than one dimension).

We now calculate the dimensions of some vector spaces we encountered in
Sections~\ref{sec:definition-vector-spaces} and
{\ref{sec:vector-space-subspaces}}.

\begin{itemize}
\item The space $\R^n$ has dimension $n$.
\item The space $\Poly_2$ has dimension $3$. We found several bases
  for this space in Example~\ref{exa:basis-p2}.
\item The space $\Mat_{m,n}$ has dimension $mn$. A possible basis
  consists of all the matrices that contain a single $1$ and zeros
  everywhere else.
\item The space $\Func_{X,K}$ is infinite-dimensional if $X$ is an
  infinite set. If $X$ is a finite set of $n$ elements, then this
  space is $n$-dimensional. In that case, a basis is given by the set
  of functions whose value is $1$ for one input and $0$ for all other
  inputs.
\item The space $\Seq_K$ is infinite-dimensional. We found an infinite
  linearly independent set in
  Example~\ref{exa:linear-independence-sequences}, showing that the
  space cannot be finite dimensional.
\item The space $\Poly$ is infinite-dimensional. We found a basis for
  this space in Example~\ref{exa:basis-p}.
\item The subspace of $\Func_{\R,\R}$ consisting of the continuous
  functions is infinite-dimensional. For example, the functions
  $\set{1,x,x^2,x^3,\ldots}$ form an infinite, linearly independent
  set of continuous functions.
\item The subspace of $\Func_{\R,\R}$ consisting of the differentiable
  functions is infinite-dimensional. Again, the set
  $\set{1,x,x^2,x^3,\ldots}$ is an infinite linearly independent set
  in this space. 
\end{itemize}

\begin{example}{Space of sequences satisfying a linear recurrence}{subspace-recurrence-dimension}
  In Example~\ref{exa:subspace-recurrence}, we considered the space
  $W$ of sequences of real numbers that satisfy the recurrence
  $a_{n+2}=a_n+a_{n+1}$. What is the dimension of this space?
\end{example}

\begin{solution}
  The space is 2-dimensional. The easiest way to see this is to
  observe that a sequence $a\in W$ is determined by its first two
  elements. We can say that the first two elements of the sequence are
  parameters, and all the other elements are then computed by the
  recurrence relation. Specifically, suppose $a_0=x$ and
  $a_1=y$. Using the recurrence relation to compute the remaining
  elements, we have
  \begin{eqnarray*}
    a &=& (x,y,x+y,x+2y,2x+3y,3x+5y,\ldots) \\
      &=& x(1,0,1,1,2,3,\ldots) + y(0,1,1,2,3,5,\ldots).
  \end{eqnarray*}
  Since this is the general form of the elements of $W$, and since the
  two sequences starting with 1,0 and 0,1 are clearly linearly
  independent, it follows that
  \begin{equation*}
    \set{(1,0,1,1,2,3,\ldots),~ (0,1,1,2,3,5,\ldots)}
  \end{equation*}
  is a basis of $W$.
\end{solution}

\begin{example}{Solution space of a linear differential equation}{dimension-differential-equation}
  In Example~\ref{exa:subspace-differential-equation}, we considered
  the space of solutions of the differential equation $f''=-f$. What
  is the dimension of this space?
\end{example}

\begin{solution}
  From calculus, we know that the general solution of the differential
  equation $f''=-f$ is
  \begin{equation*}
    f(x) = A\sin x + B\cos x,
  \end{equation*}
  where $A,B$ are constants. We also know, from
  Example~\ref{exa:linearly-independent-functions}, that $\sin x$ and
  $\cos x$ are linearly independent. It follows that
  $\set{\sin x, \cos x}$ is a basis for the solution space. The
  solution space is therefore 2-dimensional.
\end{solution}

% ======================================================================
\subsection{CONTINUE HERE}

* nested space, same (finite) dimension => equal
* shrink spanning set to basis; grow linearly independent set to basis.
* grow basis of subspace to basis of space



If in fact $W$ has $n$ vectors, then it follows that $W=V$.

\begin{theorem}{Subspace of same dimension}{subspace-vector-space}
  Let $V$ be a vector space of dimension $n$ and let $W$ be a
  subspace. Then $W=V$ if and only if the dimension of $W$ is also
  $n$.
\end{theorem}

\begin{proof}
  First suppose $W=V$. Then obviously the dimension of $W=n$.

  Now suppose that the dimension of $W$ is $n$. Let a basis for $W$ be
  $ \set{\vect{w}_{1},\ldots,\vect{w}_{n}}$. If $W$ is not equal to
  $V$ , then let $\vect{v}$ be a vector of $V$ which is not contained
  in $W$. Thus $ \vect{v}$ is not in
  $\sspan\set{\vect{w}_{1},\ldots,\vect{w} _{n}} $ and by
  Lemma~\ref{lem:bases-isomorphism},
  $\set{\vect{w}_{1},\ldots,\vect{w}_{n},\vect{v}} $ is linearly
  independent which contradicts Theorem~\ref{thm:exchange-theorem}
  because it would be an independent set of $n+1$ vectors even though
  each of these vectors is in a spanning set of $n$ vectors, a basis
  of $V$.
\end{proof}

Consider the following example.

\begin{example}{Basis of a subspace}{basis-subspace}
  Let $U=\set{A\in\Mat_{22} \left\vert
      A\begin{mymatrix}{rr}
        1 & 0 \\ 1 & -1 \end{mymatrix}
      = \begin{mymatrix}{rr}
        1 & 1 \\ 0 & -1 \end{mymatrix} A \right.}$.
  Then $U$ is a subspace of $\Mat_{22}$
  Find a basis of $U$, and hence $\dim(U)$.
\end{example}

\begin{solution}
  Let $A=\begin{mymatrix}{rr} a & b \\ c & d \end{mymatrix}
  \in\Mat_{22}$.
  Then
  \begin{equation*}
    A\begin{mymatrix}{rr} 1 & 0 \\ 1 & -1 \end{mymatrix}
    = \begin{mymatrix}{rr} a & b \\ c & d \end{mymatrix}
    \begin{mymatrix}{rr} 1 & 0 \\ 1 & -1 \end{mymatrix}
    =\begin{mymatrix}{rr} a+b & -b \\ c+d & -d \end{mymatrix}
  \end{equation*}
  and
  \begin{equation*}
    \begin{mymatrix}{rr} 1 & 1 \\ 0 & -1 \end{mymatrix} A
    = \begin{mymatrix}{rr} 1 & 1 \\ 0 & -1 \end{mymatrix}
    \begin{mymatrix}{rr} a & b \\ c & d \end{mymatrix}
    =\begin{mymatrix}{cc} a+c & b+d \\ -c & -d \end{mymatrix}.
  \end{equation*}
  If $A\in U$, then
  $\begin{mymatrix}{cc} a+b & -b \\ c+d & -d \end{mymatrix}=
  \begin{mymatrix}{cc} a+c & b+d \\ -c & -d \end{mymatrix}$.

  Equating entries leads to a system of four equations in the four
  variables $a,b,c$ and $d$.
  \begin{equation*}
    \begin{array}{ccc}
      a+b & = & a + c \\
      -b & = & b + d \\
      c + d & = & -c \\
      -d & = & -d \end{array}
    \quad\mbox{or}\quad
    \begin{array}{rcc}
      b - c & = & 0 \\
      -2b - d & = & 0 \\
      2c + d & = & 0
    \end{array}.
  \end{equation*}

  The solution to this system is
  $a=s$, $b=-\frac{1}{2}t$, $c=-\frac{1}{2}t$,  $d=t$ for any $s,t\in\R$,
  and thus
  \begin{equation*}
    A=\begin{mymatrix}{cc} s & \frac{t}{2} \\
      -\frac{t}{2} & t \end{mymatrix}
    = s\begin{mymatrix}{cc} 1 & 0 \\ 0 & 0 \end{mymatrix}
    + t\begin{mymatrix}{rr} 0  & -\frac{1}{2} \\
      -\frac{1}{2} & 1 \end{mymatrix} .
  \end{equation*}
  Let
  \begin{equation*}
    B=\set{
      \begin{mymatrix}{cc} 1 & 0 \\ 0 & 0 \end{mymatrix},
      \begin{mymatrix}{rr} 0  & -\frac{1}{2} \\
        -\frac{1}{2} & 1 \end{mymatrix}
    }.
  \end{equation*}
  Then $\sspan(B)=U$, and it is routine to verify that $B$ is an
  independent subset of $\Mat_{22}$.  Therefore $B$ is a basis of $U$,
  and $\dim(U)=2$.
\end{solution}

The following theorem claims that a spanning set of a vector space $V$
can be shrunk down to a basis of $V$. Similarly, a linearly
independent set within $V$ can be enlarged to create a basis of $V$.

\begin{theorem}{Basis of $V$}{basis-from-spanning-linear-independent}
  If $V=\sspan\set{\vect{u}_{1},\ldots,\vect{u} _{n}} $ is a vector
  space, then some subset of $\set{\vect{u}_{1},\ldots,\vect{u}_{n}}$
  is a basis for $V$. Also, if
  $\set{\vect{u}_{1},\ldots,\vect{u} _{k}}\subseteq V$ is linearly
  independent and the vector space is finite dimensional, then the set
  $\set{ \vect{u}_{1},\ldots,\vect{u}_{k}}$, can be enlarged to obtain
  a basis of $V$\index{linear independence!enlarging to form a basis}.
\end{theorem}

\begin{proof}
  Let
  \begin{equation*}
    S=\set{E\subseteq \set{\vect{u}_{1},\ldots,\vect{u}_{n}}\text{ such that }%
      \sspan\set{E} =V}.
  \end{equation*}
  For $E\in S$, let $\abs{E}$ denote the number of elements of
  $E$. Let
  \begin{equation*}
    m= \min \set{\abs{E}\text{ such that }E\in S}.
  \end{equation*}
  Thus there exist vectors
  \begin{equation*}
    \set{\vect{v}_{1},\ldots,\vect{v}_{m}}\subseteq \set{\vect{u}_{1},\ldots,%
      \vect{u}_{n}}
  \end{equation*}
  such that
  \begin{equation*}
    \sspan\set{\vect{v}_{1},\ldots,\vect{v}_{m}} =V
  \end{equation*}
  and $m$ is as small as possible for this to happen. If this set is
  linearly independent, it follows it is a basis for $V$ and the
  theorem is proved. On the other hand, if the set is not linearly
  independent, then there exist scalars, $c_{1},\ldots,c_{m}$ such
  that
  \begin{equation*}
    \vect{0}=\sum_{i=1}^{m}c_{i}\vect{v}_{i}
  \end{equation*}
  and not all the $c_{i}$ are equal to zero. Suppose $c_{k}\neq
  0$. Then solve for the vector $\vect{v}_{k}$ in terms of the other
  vectors.  Consequently,
  \begin{equation*}
    V=\sspan\set{\vect{v}_{1},\ldots,\vect{v}_{k-1},\vect{v}
      _{k+1},\ldots,\vect{v}_{m}}
  \end{equation*}
  contradicting the definition of $m$. This proves the first part of the
  theorem.

  To obtain the second part, begin with $\set{\vect{u}_{1},\ldots,\vect{u}
    _{k}}$ and suppose a basis for $V$ is
  \begin{equation*}
    \set{\vect{v}_{1},\ldots,\vect{v}_{n}}
  \end{equation*}
  If
  \begin{equation*}
    \sspan\set{\vect{u}_{1},\ldots,\vect{u}_{k}} =V,
  \end{equation*}
  then $k=n$. If not, there exists a vector
  \begin{equation*}
    \vect{u}_{k+1}\notin \sspan\set{\vect{u}_{1},\ldots,\vect{u}
      _{k}}
  \end{equation*}
  Then from Lemma~\ref{lem:adding-linearly-independent},
  $\set{\vect{u}_{1},\ldots,\vect{u}_{k}, \vect{u}_{k+1}}$ is also
  linearly independent. Continue adding vectors in this way until $n$
  linearly independent vectors have been obtained. Then
  \begin{equation*}
    \sspan\set{\vect{u}_{1},\ldots,\vect{u}_{n}} =V
  \end{equation*}
  because if it did not do so, there would exist $\vect{u}_{n+1}$ as
  just described and $\set{\vect{u}_{1},\ldots,\vect{u}_{n+1}} $ would
  be a linearly independent set of vectors having $n+1$ elements. This
  contradicts the fact that $\set{\vect{v}_{1},\ldots,\vect{v}_{n}} $
  is a basis.  In turn this would contradict
  Theorem~\ref{thm:exchange-theorem}. Therefore, this list is a basis.
\end{proof}

Recall Example~\ref{exa:adding-linear-independent} in which we added a
matrix to a linearly independent set to create a larger linearly
independent set. By
Theorem~\ref{thm:basis-from-spanning-linear-independent} we can extend
a linearly independent set to a basis.

\begin{example}{Adding to a linearly independent set}{adding-linear-independent-basis}
  Let $S \subseteq M_{22}$ be a linearly independent set given by
  \begin{equation*}
    S  = \set{\begin{mymatrix}{rr}
        1 & 0 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 1 \\
        0 & 0
      \end{mymatrix} }
  \end{equation*}
  Enlarge $S$ to a basis of $M_{22}$.
\end{example}

\begin{solution}
  Recall from the solution of
  Example~\ref{exa:adding-linear-independent} that the set
  $R \subseteq M_{22}$ given by
  \begin{equation*}
    R = \set{\begin{mymatrix}{rr}
        1 & 0 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 1 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 0 \\
        1 & 0
      \end{mymatrix} }
  \end{equation*}
  is also linearly independent.  However this set is still not a basis
  for $M_{22}$ as it is not a spanning set. In particular,
  $\begin{mymatrix}{rr}
    0 & 0 \\
    0 & 1
  \end{mymatrix}$ is not in $\sspan R$. Therefore, this matrix can be
  added to the set by Lemma~\ref{lem:adding-linearly-independent} to
  obtain a new linearly independent set given by
  \begin{equation*}
    T = \set{\begin{mymatrix}{rr}
        1 & 0 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 1 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 0 \\
        1 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 0 \\
        0 & 1
      \end{mymatrix} }
  \end{equation*}

  This set is linearly independent and now spans $M_{22}$. Hence $T$
  is a basis.
\end{solution}

Next we consider the case where you have a spanning set and you want a
subset which is a basis. The above discussion involved adding vectors
to a set. The next theorem involves removing vectors.

\begin{theorem}{Basis from a spanning set}{}
  Let $V$ be a vector space and let $W$ be a subspace. Also suppose
  that $W=\sspan\set{\vect{w}_{1},\ldots,\vect{w} _{m}}$. Then there
  exists a subset of $\set{\vect{w}_{1},\ldots, \vect{w}_{m}} $ which
  is a basis for $W$.
\end{theorem}

\begin{proof}
  Let $S$ denote the set of positive integers such that for $ k\in S$,
  there exists a subset of $\set{\vect{w}_{1},\ldots,\vect{w} _{m}} $
  consisting of exactly $k$ vectors which is a spanning set for
  $W$. Thus $m\in S$. Pick the smallest positive integer in $S$. Call
  it $k$.  Then there exists
  $\set{\vect{u}_{1},\ldots,\vect{u}_{k}} \subseteq
  \set{\vect{w}_{1},\ldots,\vect{w}_{m}} $ such that
  $\sspan \set{\vect{u}_{1},\ldots,\vect{u}_{k}} =W$. If
  \begin{equation*}
    \sum_{i=1}^{k}c_{i}\vect{w}_{i}=\vect{0}
  \end{equation*}
  and not all of the $c_{i}=0$, then you could pick $c_{j}\neq 0$,
  divide by it and solve for $\vect{u}_{j}$ in terms of the others.
  \begin{equation*}
    \vect{w}_{j}=\sum_{i\neq j}\paren{-\frac{c_{i}}{c_{j}}} \vect{w}_{i}
  \end{equation*}
  Then you could delete $\vect{w}_{j}$ from the list and have the same
  span.  In any linear combination involving $\vect{w}_{j}$, the
  linear combination would equal one in which $\vect{w}_{j}$ is
  replaced with the above sum, showing that it could have been
  obtained as a linear combination of $\vect{w}_{i}$ for $i\neq
  j$. Thus $k-1\in S$ contrary to the choice of $k$ . Hence each
  $c_{i}=0$ and so $\set{\vect{u}_{1},\ldots,\vect{u} _{k}} $ is a
  basis for $W$ consisting of vectors of
  $\set{\vect{w} _{1},\ldots,\vect{w}_{m}}$.
\end{proof}

Consider the following example of this concept.

\begin{example}{Basis from a spanning set}{}
  Let $V$ be the vector space of polynomials of degree no more than 3,
  denoted earlier as $\Poly_{3}$. Consider the following vectors in
  $V$.
  \begin{eqnarray*}
    &&2x^{2}+x+1,x^{3}+4x^{2}+2x+2,2x^{3}+2x^{2}+2x+1, \\
    &&x^{3}+4x^{2}-3x+2,x^{3}+3x^{2}+2x+1
  \end{eqnarray*}
  Then, as mentioned above, $V$ has dimension 4 and so clearly these
  vectors are not linearly independent. A basis for $V$ is
  $\set{ 1,x,x^{2},x^{3}}$. Determine a linearly independent subset of
  these which has the same span. Determine whether this subset is a
  basis for $V$.
\end{example}

\begin{solution}
  Consider an isomorphism which maps $\R%
  ^{4}$ to $V$ in the obvious way. Thus
  \begin{equation*}
    \begin{mymatrix}{c}
      1 \\
      1 \\
      2 \\
      0
    \end{mymatrix}
  \end{equation*}
  corresponds to $2x^{2}+x+1$ through the use of this
  isomorphism. Then corresponding to the above vectors in $V$ we would
  have the following vectors in $\R^{4}$.
  \begin{equation*}
    \begin{mymatrix}{c}
      1 \\
      1 \\
      2 \\
      0
    \end{mymatrix},
    \begin{mymatrix}{c}
      2 \\
      2 \\
      4 \\
      1
    \end{mymatrix},
    \begin{mymatrix}{c}
      1 \\
      2 \\
      2 \\
      2
    \end{mymatrix},
    \begin{mymatrix}{r}
      2 \\
      -3 \\
      4 \\
      1
    \end{mymatrix},
    \begin{mymatrix}{c}
      1 \\
      2 \\
      3 \\
      1
    \end{mymatrix}
  \end{equation*}
  Now if we obtain a subset of these which has the same span but which
  is linearly independent, then the corresponding vectors from $V$
  will also be linearly independent. If there are four in the list,
  then the resulting vectors from $V$ must be a basis for $V$.  The
  {\rref} for the matrix which has the above vectors as columns is
  \begin{equation*}
    \begin{mymatrix}{rrrrr}
      1 & 0 & 0 & -15 & 0 \\
      0 & 1 & 0 & 11 & 0 \\
      0 & 0 & 1 & -5 & 0 \\
      0 & 0 & 0 & 0 & 1
    \end{mymatrix}
  \end{equation*}
  Therefore, a basis for $V$ consists of the vectors
  \begin{eqnarray*}
    &&2x^{2}+x+1,x^{3}+4x^{2}+2x+2,2x^{3}+2x^{2}+2x+1, \\
    &&x^{3}+3x^{2}+2x+1.
  \end{eqnarray*}
  Note how this is a subset of the original set of vectors. If there
  had been only three pivot columns in this matrix, then we would not
  have had a basis for $V$ but we would at least have obtained a
  linearly independent subset of the original set of vectors in this
  way.

  Note also that, since all linear relations are preserved by an
  isomorphism,
  \begin{eqnarray*}
    &&-15(2x^{2}+x+1) +11(x^{3}+4x^{2}+2x+2) +(
       -5) (2x^{3}+2x^{2}+2x+1) \\
    &=&x^{3}+4x^{2}-3x+2
  \end{eqnarray*}

\end{solution}

Consider the following example.

\begin{example}{Shrinking a spanning set}{shrink-spanning}
  Consider the set $S \subseteq \Poly_2$ given by
  \begin{equation*}
    S = \set{1, x, x^2, x^2 + 1 }
  \end{equation*}
  Show that $S$ spans $\Poly_2$, then remove vectors from $S$ until it
  creates a basis.
\end{example}

\begin{solution}
  First we need to show that $S$ spans $\Poly_2$. Let $ax^2 + bx + c$
  be an arbitrary polynomial in $\Poly_2$. Write
  \begin{equation*}
    ax^2 + bx + c = r(1) + s(x) + t(x^2) + u (x^2 + 1)
  \end{equation*}
  Then,
  \begin{eqnarray*}
    ax^2 +bx + c &=& r(1) + s(x) + t(x^2) + u (x^2 + 1) \\
                 &=& (t+u) x^2 + s(x) + (r+u)
  \end{eqnarray*}

  It follows that
  \begin{eqnarray*}
    a &=& t + u \\
    b &=& s \\
    c &=& r + u
  \end{eqnarray*}
  Clearly a solution exists for all $a,b,c$ and so $S$ is a spanning
  set for $\Poly_2$. By
  Theorem~\ref{thm:basis-from-spanning-linear-independent}, some
  subset of $S$ is a basis for $\Poly_2$.

  Recall that a basis must be both a spanning set and a linearly
  independent set.  Therefore we must remove a vector from $S$ keeping
  this in mind. Suppose we remove $x$ from $S$. The resulting set
  would be $\set{1, x^2, x^2 + 1 }$. This set is clearly linearly
  dependent (and also does not span $\Poly_2$) and so is not a basis.

  Suppose we remove $x^2 + 1$ from $S$. The resulting set is
  $\set{1, x, x^2 }$ which is both linearly independent and spans
  $\Poly_2$. Hence this is a basis for $\Poly_2$. Note that removing
  any one of $1, x^2$, or $x^2 + 1$ will result in a basis.
\end{solution}

Now the following is a fundamental result about subspaces.

\begin{theorem}{Basis of a vector space}{basis-vector-space}
  Let $V$ be a finite dimensional vector space and let $W$ be a
  non-zero subspace. Then $W$ has a basis. That is, there exists a
  linearly independent set of vectors
  $\set{\vect{w}_{1},\ldots,\vect{w}_{r}} $ such that
  \begin{equation*}
    \sspan\set{\vect{w}_{1},\ldots,\vect{w}_{r}} =W
  \end{equation*}
  Also if $\set{\vect{w}_{1},\ldots,\vect{w}_{s}} $ is a linearly
  independent set of vectors, then $W$ has a basis of the form
  $\set{\vect{w} _{1},\ldots,\vect{w}_{s},\ldots,\vect{w}_{r}} $ for
  $r\geq s$.
\end{theorem}

\begin{proof}
  Let the dimension of $V$ be $n$. Pick $\vect{w}_{1}\in W$ where
  $\vect{w}_{1}\neq \vect{0}$. If $\vect{w}_{1},\ldots,\vect{w}_{s}$
  have been chosen such that $\set{\vect{w}_{1},\ldots,\vect{w}_{s}} $
  is linearly independent, if
  $\sspan\set{\vect{w}_{1},\ldots,\vect{w} _{r}} =W$, stop. You have
  the desired basis. Otherwise, there exists
  $ \vect{w}_{s+1}\notin \sspan\set{\vect{w}_{1},\ldots,\vect{w} _{s}}
  $ and $\set{\vect{w}_{1},\ldots, \vect{w}_{s},\vect{w}_{s+1}} $ is
  linearly independent. Continue this way until the process stops. It
  must stop since otherwise, you could obtain a linearly independent
  set of vectors having more than $n$ vectors which is impossible.

  The last claim is proved by following the above procedure starting
  with $ \set{\vect{w}_{1},\ldots,\vect{w}_{s}} $ as above.
\end{proof}

This also proves the following corollary. Let $V$ play the role of
$ W$ in the above theorem and begin with a basis for $W$, enlarging it
to form a basis for $V$ as discussed above.

\begin{corollary}{Basis extension}{}
  Let $W$ be any non-zero subspace of a vector space $V$.  Then every
  basis of $W$ can be extended to a basis for $V$.
\end{corollary}

Consider the following example.

\begin{example}{Basis extension}{}
  Let $V=\R^{4}$ and let
  \begin{equation*}
    W=\sspan\set{\begin{mymatrix}{c}
        1 \\
        0 \\
        1 \\
        1
      \end{mymatrix} ,\begin{mymatrix}{c}
        0 \\
        1 \\
        0 \\
        1
      \end{mymatrix} }
  \end{equation*}
  Extend this basis of $W$ to a basis of $V$.
\end{example}

\begin{solution}
  An easy way to do this is to take the {\rref} of the matrix
  \begin{equation}
    \begin{mymatrix}{cccccc}
      1 & 0 & 1 & 0 & 0 & 0 \\
      0 & 1 & 0 & 1 & 0 & 0 \\
      1 & 0 & 0 & 0 & 1 & 0 \\
      1 & 1 & 0 & 0 & 0 & 1
    \end{mymatrix}  \label{vector-space-eq1}
  \end{equation}
  Note how the given vectors were placed as the first two and then the
  matrix was extended in such a way that it is clear that the span of
  the columns of this matrix yield all of $\R^{4}$. Now determine the
  pivot columns.  The {\rref} is
  \begin{equation}
    \begin{mymatrix}{rrrrrr}
      1 & 0 & 0 & 0 & 1 & 0 \\
      0 & 1 & 0 & 0 & -1 & 1 \\
      0 & 0 & 1 & 0 & -1 & 0 \\
      0 & 0 & 0 & 1 & 1 & -1
    \end{mymatrix}  \label{vector-space-eq2}
  \end{equation}
  These are
  \begin{equation*}
    \begin{mymatrix}{c}
      1 \\
      0 \\
      1 \\
      1
    \end{mymatrix} ,\begin{mymatrix}{c}
      0 \\
      1 \\
      0 \\
      1
    \end{mymatrix} ,\begin{mymatrix}{c}
      1 \\
      0 \\
      0 \\
      0
    \end{mymatrix} ,\begin{mymatrix}{c}
      0 \\
      1 \\
      0 \\
      0
    \end{mymatrix}
  \end{equation*}
  and now this is an extension of the given basis for $W$ to a basis
  for $ \R^{4}$.

  Why does this work? The columns of {\eqref{vector-space-eq1}}
  obviously span $\R ^{4}$ the span of the first four is the same as
  the span of all six.
\end{solution}
