\section{Basis and dimension}

\begin{outcome}
  \begin{enumerate}
  \item Extend a linearly independent set and shrink a spanning set to
    a basis of a given vector space.
  \end{enumerate}
\end{outcome}

\begin{definition}{Basis}{basis-vector-space}
  Let $V$ be a vector space. A set $B$ of vectors is called a
  \textbf{basis}%
  \index{basis}%
  \index{basis!of a vector space}%
  \index{vector space!basis} of $V$ if
  \begin{enumerate}
  \item $B$ is a spanning set for $V$, and
  \item $B$ is linearly independent.
  \end{enumerate}
\end{definition}

\begin{example}{Bases of $\Poly_2$}{basis-p2}
  Consider the vector space $\Poly_2$ of polynomials of degree at most
  2 with coefficients in a field $K$.%
  \index{P2@$\Poly_2$!basis of}
  \begin{itemize}
  \item $\set{1, x, x^2}$ is a basis of $\Poly_2$.
  \item $\set{x^2, (x+1)^2, (x+2)^2}$ is a basis of $\Poly_2$.
  \item $\set{1, x-1, (x-1)^2}$ is a basis of $\Poly_2$.
  \end{itemize}
  Unlike $\R^n$, a general vector space like $\Poly_2$ does not
  necessarily have a ``standard'' basis. One basis might be useful for
  one application, and another basis for a different application.
\end{example}

\begin{proof}
  It is easy to verify that each set of vectors is linearly
  independent and spanning. See Examples~\ref{exa:spanning-set},
  {\ref{exa:linear-independence-polynomial}}, and
  {\ref{exa:polynomials-increasing-degree}} for similar calculations.
\end{proof}

\begin{example}{An infinite basis}{basis-p}
  Consider the vector space $\Poly$ of all polynomials with
  coefficients in a field $K$. The following is a basis for $\Poly$%
  \index{P@$\Poly$!basis of}:
  \begin{equation*}
    \set{1, x, x^2, x^3, x^4, \ldots}.
  \end{equation*}
  Note that this basis is infinite.
\end{example}

\begin{proof}
  The polynomials $1, x, x^2, x^3, x^4, \ldots$ are linearly
  independent by Proposition~\ref{prop:linear-dependence-redundant}.
  Namely, if they were linearly dependent, then one of the polynomials
  could be written as a linear combination of earlier ones. However,
  this is not possible because a polynomial of degree $n$ cannot be a
  linear combination of polynomials of degree less than $n$.

  To show that the polynomials $1, x, x^2, x^3, x^4, \ldots$ are a
  spanning set, consider an arbitrary element $p(x)$ of $\Poly$.
  Then by definition, $p(x)$ is of the form
  \begin{equation*}
    p(x) = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_1x + a_0,
  \end{equation*}
  for some $n\geq 0$ and $a_0,\ldots,a_n\in K$. But then $p(x)$ is a
  linear combination of $1,\ldots,x^n$, i.e., it is in the span of
  $\set{1, x, x^2, x^3, x^4, \ldots}$.
\end{proof}

\begin{example}{Not a basis}{not-a-basis}
  Consider the vector space $\Seq_K$ of infinite sequences. As before,
  let $e^k$ be the sequence whose $k\th$ component is $1$ and which is
  $0$ everywhere else, i.e.,
    \begin{equation*}
    \begin{array}{l}
      e^0 = 1,0,0,0,0,\ldots, \\
      e^1 = 0,1,0,0,0,\ldots, \\
      e^2 = 0,0,1,0,0,\ldots, \\
    \end{array}
  \end{equation*}
  and so on. Then the set
  \begin{equation*}
    \set{e^0, e^1, e^2, \ldots}
  \end{equation*}
  is \textit{not} a basis of $\Seq_K$. Indeed, although we saw in
  Example~\ref{exa:linear-independence-sequences} that the sequences
  $e^0, e^1, e^2, \ldots$ are linearly independent,
  Example~\ref{exa:spans-sequences} shows that they are not spanning.
  Indeed,
  \begin{equation*}
    W = \sspan\set{e^0, e^1, e^2, \ldots}
  \end{equation*}
  is a subspace of $\Seq_K$, consisting exactly of the
  \textbf{finitely supported}%
  \index{finitely supported sequence}%
  \index{sequence!finitely supported} sequences, i.e., those sequences
  that have only finitely many non-zero entries. Thus, $\set{e^0, e^1,
    e^2, \ldots}$ is a basis of $W$.
\end{example}

% ======================================================================
\subsection{CONTINUE HERE}

\begin{example}{}{}
  As a matter of fact, the space $W$ of the last example is a
  2-dimensional space. From calculus, we know that the general solution
  of the differential equation $f''=-f$ is
  \begin{equation*}
    f(x) = A\sin x + B\cos x,
  \end{equation*}
  where $A,B$ are constants. This means that $W=\sspan\set{\sin x, \cos x}$.
\end{example}


The next theorem is an essential result in linear algebra and is
called the exchange theorem\index{exchange theorem}.

\begin{theorem}{Exchange theorem}{exchange-theorem}
  Let $\set{\vect{x}_{1},\ldots,\vect{x}_{r}} $ be a linearly
  independent set of vectors such that each $\vect{x}_{i}$ is
  contained in span$\set{\vect{y}_{1},\ldots,\vect{y}_{s}}$. Then
  $ r\leq s$.
\end{theorem}

\begin{proof}
  The proof will proceed as follows. First, we set up the necessary
  steps for the proof. Next, we will assume that $r > s$ and show that
  this leads to a contradiction, thus requiring that $r \leq s$.

  Define span$\set{\vect{y}_{1},\ldots,\vect{y}_{s}} = V$. Since each
  $\vect{x}_i$ is in span$\set{\vect{y}_{1},\ldots,\vect{y}_{s}}$, it
  follows there exist scalars $c_{1},\ldots,c_{s}$ such that
  \begin{equation}
    \vect{x}_{1}=\sum_{i=1}^{s}c_{i}\vect{y}_{i}  \label{linear-comb}
  \end{equation}
  Note that not all of these scalars $c_i$ can equal zero. Suppose
  that all the $c_i=0$. Then it would follow that
  $\vect{x}_{1}=\vect{0}$ and so
  $\set{\vect{x} _{1},\ldots,\vect{x}_{r}} $ would not be linearly
  independent.  Indeed, if $\vect{x}_{1}=\vect{0}$,
  $1\vect{x}_{1}+\sum_{i=2}^{r}0 \vect{x}_{i}=\vect{x}_{1}=\vect{0}$
  and so there would exist a non-trivial linear combination of the
  vectors $\set{\vect{x}_{1},\ldots, \vect{x}_{r}} $ which equals
  zero. Therefore at least one $c_i$ is non-zero.

  Say $c_{k}\neq 0$. Then solve {\eqref{linear-comb}} for
  $\vect{y}_{k}$ and obtain
  \begin{equation*}
    \vect{y}_{k}\in \sspan\set{\vect{x}_{1},\overset{\text{s-1
          vectors here}}{\overbrace{\vect{y}_{1},\ldots,\vect{y}_{k-1},\vect{y}
          _{k+1},\ldots,\vect{y}_{s}}}} .
  \end{equation*}
  Define $\set{\vect{z}_{1},\ldots,\vect{z}_{s-1}} $ to be
  \begin{equation*}
    \set{\vect{z}_{1},\ldots,\vect{z}_{s-1}} = \set{
      \vect{y}_{1},\ldots,\vect{y}_{k-1},\vect{y}_{k+1},\ldots,\vect{y}
      _{s}}
  \end{equation*}
  Now we can write
  \begin{equation*}
    \vect{y}_{k}\in \sspan\set{\vect{x}_{1}, \vect{z}_{1},\ldots, \vect{z}_{s-1}}
  \end{equation*}
  Therefore,
  $\sspan\set{\vect{x}_{1},\vect{z}_{1},\ldots,\vect{z }_{s-1}}=V$. To
  see this, suppose $\vect{v}\in V$. Then there exist constants
  $ c_{1},\ldots,c_{s}$ such that
  \begin{equation*}
    \vect{v}=\sum_{i=1}^{s-1}c_{i}\vect{z}_{i}+c_{s}\vect{y}_{k}.
  \end{equation*}
  Replace this $\vect{y}_{k}$ with a linear combination of the vectors
  $\set{\vect{x}_{1},\vect{z}_{1},\ldots,\vect{z}_{s-1}}$ to obtain
  $\vect{v}\in \sspan\set{\vect{x}_{1},\vect{z}
    _{1},\ldots,\vect{z}_{s-1}}$. The vector $\vect{y}_{k}$, in the
  list $\set{\vect{y}_{1},\ldots,\vect{y}_{s}}$, has now been replaced
  with the vector $\vect{x}_{1}$ and the resulting modified list of
  vectors has the same span as the original list of vectors,
  $\set{\vect{y }_{1},\ldots,\vect{y}_{s}}$.

  We are now ready to move on to the proof. Suppose that $r>s$ and
  that
  \begin{equation*}
    \sspan\set{\vect{x}_{1},\ldots,
      \vect{x}_{l},\vect{z}_{1},\ldots,\vect{z}_{p}} =V
  \end{equation*}
  where the process established above has continued. In other words,
  the vectors $\vect{z}_{1},\ldots,\vect{z}_{p}$ are each taken from
  the set $\set{\vect{y}_{1},\ldots,\vect{y}_{s}} $ and $l+p=s$.  This
  was done for $l=1$ above. Then since $r>s$, it follows that
  $ l\leq s<r$ and so $l+1\leq r$. Therefore, $\vect{x}_{l+1}$ is a
  vector not in the list, $\set{\vect{x}_{1},\ldots,\vect{x}_{l}} $
  and since
  \begin{equation*}
    \sspan\set{\vect{x}_{1},\ldots,\vect{x}_{l},\vect{z}
      _{1},\ldots,\vect{z}_{p}} =V
  \end{equation*}
  there exist scalars, $c_{i}$ and $
  d_{j}$ such that
  \begin{equation}
    \vect{x}_{l+1}=\sum_{i=1}^{l}c_{i}\vect{x}_{i}+\sum_{j=1}^{p}d_{j}
    \vect{z}_{j}.  \label{linear-comb2}
  \end{equation}
  Not all the $d_{j}$ can equal zero because if this were so, it would
  follow that $\set{\vect{x}_{1},\ldots,\vect{x}_{r}} $ would be a
  linearly dependent set because one of the vectors would equal a
  linear combination of the others. Therefore, {\eqref{linear-comb2}}
  can be solved for one of the $\vect{z}_{i}$, say $\vect{z}_{k}$, in
  terms of $\vect{x}_{l+1}$ and the other $\vect{z}_{i}$ and just as
  in the above argument, replace that $ \vect{z}_{i}$ with
  $\vect{x}_{l+1}$ to obtain
  \begin{equation*}
    \sspan\set{\vect{x}_{1},\ldots,\vect{x}_{l},\vect{x}_{l+1},
      \overset{\text{p-1 vectors here}}{\overbrace{\vect{z}_{1},\ldots,\vect{z}
          _{k-1},\vect{z}_{k+1},\ldots,\vect{z}_{p}}}} =V
  \end{equation*}
  Continue this way, eventually obtaining
  \begin{equation*}
    \sspan\set{\vect{x}_{1},\ldots,\vect{x}_{s}} =V.
  \end{equation*}
  But then $\vect{x}_{r}\in $
  $\sspan\set{\vect{x}_{1},\ldots, \vect{x}_{s}} $ contrary to the
  assumption that $\set{\vect{x} _{1},\ldots,\vect{x}_{r}} $ is
  linearly independent. Therefore, $ r\leq s$ as claimed.
\end{proof}

The following corollary follows from the exchange theorem.

\begin{corollary}{Two bases of the same length}{bases-length}
  Let $B_1$, $B_2$ be two bases of a vector space $V$. Suppose $B_1$
  contains $m$ vectors and $B_2$ contains $n$ vectors. Then
  $m = n$.\index{basis!any two same size}
\end{corollary}

\begin{proof}
  By Theorem~\ref{thm:exchange-theorem}, $m\leq n$ and $n\leq
  m$. Therefore $m=n$.
\end{proof}

This corollary is very important so we provide another proof
independent of the exchange theorem above.

\begin{proof}
  Suppose $n > m$. Then since the vectors
  $\set{\vect{u} _{1},\ldots,\vect{u}_{m}} $ span $V$, there exist
  scalars $c_{ij}$ such that
  \begin{equation*}
    \sum_{i=1}^{m}c_{ij}\vect{u}_{i}=\vect{v}_{j}.
  \end{equation*}
  Therefore,
  \begin{equation*}
    \sum_{j=1}^{n}d_{j}\vect{v}_{j}=\vect{0}
    \text{ if and only if }\sum_{j=1}^{n}\sum_{i=1}^{m}c_{ij}d_{j}\vect{u}_{i}=
    \vect{0}
  \end{equation*}
  if and only if
  \begin{equation*}
    \sum_{i=1}^{m}\paren{\sum_{j=1}^{n}c_{ij}d_{j}} \vect{u}_{i}=\vect{
      0}
  \end{equation*}
  Now since $\set{\vect{u}_{1},\ldots,\vect{u}_{n}}$ is independent, this
  happens if and only if
  \begin{equation*}
    \sum_{j=1}^{n}c_{ij}d_{j}=0,\;i=1,2,\ldots,m.
  \end{equation*}
  However, this is a system of $m$ equations in $n$ variables,
  $d_{1},\ldots,d_{n}$ and $m<n$. Therefore, there exists a solution
  to this system of equations in which not all the $d_{j}$ are equal
  to zero. Recall why this is so. The augmented matrix for the system
  is of the form $\begin{mymatrix}{c|c} C & \vect{0}
  \end{mymatrix} $ where $C$ is a matrix which has more columns than
  rows. Therefore, there are free variables and hence non-zero
  solutions to the system of equations. However, this contradicts the
  linear independence of $\set{
    \vect{u}_{1},\ldots,\vect{u}_{m}}$. Similarly it cannot happen
  that $m > n$.
\end{proof}

Given the result of the previous corollary, the following definition follows.

\begin{definition}{Dimension}{dimension-vector-space}
  A vector space $V$ is of dimension $n$ if it has a basis consisting
  of $n$ vectors\index{dimension of vector space}\index{vector
    space!dimension}.
\end{definition}

Notice that the dimension is well defined by
Corollary~\ref{cor:bases-length}. It is assumed here that $n<\infty $
and therefore such a vector space is said to be \textbf{finite
  dimensional}\index{finite dimensional}.

\begin{example}{Dimension of a vector space}{dimension}
  Let $\Poly_2$ be the set of all polynomials of degree at most
  $2$. Find the dimension of $\Poly_2$.
\end{example}

\begin{solution}
  If we can find a basis of $\Poly_2$ then the number of vectors in
  the basis will give the dimension. Recall from
  Example~\ref{exa:poly-degree-two} that a basis of $\Poly_2$ is given
  by
  \begin{equation*}
    S  = \set{x^2, x, 1 }
  \end{equation*}
  There are three polynomials in $S$ and hence the dimension of
  $\Poly_2$ is three.
\end{solution}

It is important to note that a basis for a vector space is not
unique. A vector space can have many bases. Consider the following
example.

\begin{example}{A different basis for polynomials of degree two}{polynomial-degree-two-different-basis}
  Let $\Poly_2$ be the polynomials of degree no more than 2. Is
  $\set{ x^{2}+x+1,2x+1,3x^{2}+1} $ a basis for $\Poly_2$?
\end{example}

\begin{solution}
  Suppose these vectors are linearly independent but do not form a
  spanning set for $\Poly_2$. Then by
  Lemma~\ref{lem:adding-linearly-independent}, we could find a fourth
  polynomial in $\Poly_2$ to create a new linearly independent set
  containing four polynomials. However this would imply that we could
  find a basis of $\Poly_2$ of more than three polynomials. This
  contradicts the result of Example~\ref{exa:dimension} in which we
  determined the dimension of $\Poly_2$ is three.  Therefore if these
  vectors are linearly independent they must also form a spanning set
  and thus a basis for $\Poly_2$.

  Suppose then that
  \begin{eqnarray*}
    a(x^{2}+x+1) +b(2x+1) +c(3x^{2}+1) &=& 0\\
    (a+3c) x^{2}+(a+2b) x+(a+b+c) &=& 0
  \end{eqnarray*}
  We know that $\set{x^2, x, 1 }$ is linearly independent, and so it
  follows that
  \begin{eqnarray*}
    a+3c &=& 0 \\
    a+2b &=& 0 \\
    a+b+c &=& 0
  \end{eqnarray*}
  and there is only one solution to this system of equations,
  $a=b=c=0$.  Therefore, these are linearly independent and form a
  basis for $\Poly_2$.
\end{solution}

Consider the following theorem.

\begin{theorem}{Every subspace has a basis}{every-subspace-basis}
  Let $W$ be a non-zero subspace of a finite dimensional vector
  space $V$. Suppose $V$ has dimension $n$.
  Then $W$ has a basis\index{subspace!has a basis}
  with no more than $n$ vectors.
\end{theorem}

\begin{proof}
  Let $\vect{v}_{1}\in V$ where $\vect{v}_{1}\neq 0$. If
  $ \sspan\set{\vect{v}_{1}} =V$, then it follows that
  $\set{\vect{v} _{1}} $ is a basis for $V$. Otherwise, there exists
  $\vect{v} _{2}\in V$ which is not in $\sspan\set{\vect{v}_{1}}$. By
  Lemma~\ref{lem:adding-linearly-independent}
  $\set{\vect{v}_{1},\vect{v}_{2}} $ is a linearly independent set of
  vectors. Then $\set{\vect{v}_{1},\vect{v} _{2}} $ is a basis for $V$
  and we are done. If $\sspan\set{\vect{v}_{1}, \vect{v}_{2}} \neq V$,
  then there exists
  $\vect{v}_{3}\notin \sspan\set{\vect{v}_{1},\vect{v}_{2}} $ and
  $\set{\vect{v} _{1},\vect{v}_{2},\vect{v}_{3}} $ is a larger
  linearly independent set of vectors. Continuing this way, the
  process must stop before $n+1$ steps because if not, it would be
  possible to obtain $n+1$ linearly independent vectors contrary to
  the exchange theorem, Theorem~\ref{thm:exchange-theorem}.
\end{proof}

If in fact $W$ has $n$ vectors, then it follows that $W=V$.

\begin{theorem}{Subspace of same dimension}{subspace-vector-space}
  Let $V$ be a vector space of dimension $n$ and let $W$ be a
  subspace. Then $W=V$ if and only if the dimension of $W$ is also
  $n$.
\end{theorem}

\begin{proof}
  First suppose $W=V$. Then obviously the dimension of $W=n$.

  Now suppose that the dimension of $W$ is $n$. Let a basis for $W$ be
  $ \set{\vect{w}_{1},\ldots,\vect{w}_{n}}$. If $W$ is not equal to
  $V$ , then let $\vect{v}$ be a vector of $V$ which is not contained
  in $W$. Thus $ \vect{v}$ is not in
  $\sspan\set{\vect{w}_{1},\ldots,\vect{w} _{n}} $ and by
  Lemma~\ref{lem:bases-isomorphism},
  $\set{\vect{w}_{1},\ldots,\vect{w}_{n},\vect{v}} $ is linearly
  independent which contradicts Theorem~\ref{thm:exchange-theorem}
  because it would be an independent set of $n+1$ vectors even though
  each of these vectors is in a spanning set of $n$ vectors, a basis
  of $V$.
\end{proof}

Consider the following example.

\begin{example}{Basis of a subspace}{basis-subspace}
  Let $U=\set{A\in\Mat_{22} \left\vert
      A\begin{mymatrix}{rr}
        1 & 0 \\ 1 & -1 \end{mymatrix}
      = \begin{mymatrix}{rr}
        1 & 1 \\ 0 & -1 \end{mymatrix} A \right.}$.
  Then $U$ is a subspace of $\Mat_{22}$
  Find a basis of $U$, and hence $\dim(U)$.
\end{example}

\begin{solution}
  Let $A=\begin{mymatrix}{rr} a & b \\ c & d \end{mymatrix}
  \in\Mat_{22}$.
  Then
  \begin{equation*}
    A\begin{mymatrix}{rr} 1 & 0 \\ 1 & -1 \end{mymatrix}
    = \begin{mymatrix}{rr} a & b \\ c & d \end{mymatrix}
    \begin{mymatrix}{rr} 1 & 0 \\ 1 & -1 \end{mymatrix}
    =\begin{mymatrix}{rr} a+b & -b \\ c+d & -d \end{mymatrix}
  \end{equation*}
  and
  \begin{equation*}
    \begin{mymatrix}{rr} 1 & 1 \\ 0 & -1 \end{mymatrix} A
    = \begin{mymatrix}{rr} 1 & 1 \\ 0 & -1 \end{mymatrix}
    \begin{mymatrix}{rr} a & b \\ c & d \end{mymatrix}
    =\begin{mymatrix}{cc} a+c & b+d \\ -c & -d \end{mymatrix}.
  \end{equation*}
  If $A\in U$, then
  $\begin{mymatrix}{cc} a+b & -b \\ c+d & -d \end{mymatrix}=
  \begin{mymatrix}{cc} a+c & b+d \\ -c & -d \end{mymatrix}$.

  Equating entries leads to a system of four equations in the four
  variables $a,b,c$ and $d$.
  \begin{equation*}
    \begin{array}{ccc}
      a+b & = & a + c \\
      -b & = & b + d \\
      c + d & = & -c \\
      -d & = & -d \end{array}
    \quad\mbox{or}\quad
    \begin{array}{rcc}
      b - c & = & 0 \\
      -2b - d & = & 0 \\
      2c + d & = & 0
    \end{array}.
  \end{equation*}

  The solution to this system is
  $a=s$, $b=-\frac{1}{2}t$, $c=-\frac{1}{2}t$,  $d=t$ for any $s,t\in\R$,
  and thus
  \begin{equation*}
    A=\begin{mymatrix}{cc} s & \frac{t}{2} \\
      -\frac{t}{2} & t \end{mymatrix}
    = s\begin{mymatrix}{cc} 1 & 0 \\ 0 & 0 \end{mymatrix}
    + t\begin{mymatrix}{rr} 0  & -\frac{1}{2} \\
      -\frac{1}{2} & 1 \end{mymatrix} .
  \end{equation*}
  Let
  \begin{equation*}
    B=\set{
      \begin{mymatrix}{cc} 1 & 0 \\ 0 & 0 \end{mymatrix},
      \begin{mymatrix}{rr} 0  & -\frac{1}{2} \\
        -\frac{1}{2} & 1 \end{mymatrix}
    }.
  \end{equation*}
  Then $\sspan(B)=U$, and it is routine to verify that $B$ is an
  independent subset of $\Mat_{22}$.  Therefore $B$ is a basis of $U$,
  and $\dim(U)=2$.
\end{solution}

The following theorem claims that a spanning set of a vector space $V$
can be shrunk down to a basis of $V$. Similarly, a linearly
independent set within $V$ can be enlarged to create a basis of $V$.

\begin{theorem}{Basis of $V$}{basis-from-spanning-linear-independent}
  If $V=\sspan\set{\vect{u}_{1},\ldots,\vect{u} _{n}} $ is a vector
  space, then some subset of $\set{\vect{u}_{1},\ldots,\vect{u}_{n}}$
  is a basis for $V$. Also, if
  $\set{\vect{u}_{1},\ldots,\vect{u} _{k}}\subseteq V$ is linearly
  independent and the vector space is finite dimensional, then the set
  $\set{ \vect{u}_{1},\ldots,\vect{u}_{k}}$, can be enlarged to obtain
  a basis of $V$\index{linear independence!enlarging to form a basis}.
\end{theorem}

\begin{proof}
  Let
  \begin{equation*}
    S=\set{E\subseteq \set{\vect{u}_{1},\ldots,\vect{u}_{n}}\text{ such that }%
      \sspan\set{E} =V}.
  \end{equation*}
  For $E\in S$, let $\abs{E}$ denote the number of elements of
  $E$. Let
  \begin{equation*}
    m= \min \set{\abs{E}\text{ such that }E\in S}.
  \end{equation*}
  Thus there exist vectors
  \begin{equation*}
    \set{\vect{v}_{1},\ldots,\vect{v}_{m}}\subseteq \set{\vect{u}_{1},\ldots,%
      \vect{u}_{n}}
  \end{equation*}
  such that
  \begin{equation*}
    \sspan\set{\vect{v}_{1},\ldots,\vect{v}_{m}} =V
  \end{equation*}
  and $m$ is as small as possible for this to happen. If this set is
  linearly independent, it follows it is a basis for $V$ and the
  theorem is proved. On the other hand, if the set is not linearly
  independent, then there exist scalars, $c_{1},\ldots,c_{m}$ such
  that
  \begin{equation*}
    \vect{0}=\sum_{i=1}^{m}c_{i}\vect{v}_{i}
  \end{equation*}
  and not all the $c_{i}$ are equal to zero. Suppose $c_{k}\neq
  0$. Then solve for the vector $\vect{v}_{k}$ in terms of the other
  vectors.  Consequently,
  \begin{equation*}
    V=\sspan\set{\vect{v}_{1},\ldots,\vect{v}_{k-1},\vect{v}
      _{k+1},\ldots,\vect{v}_{m}}
  \end{equation*}
  contradicting the definition of $m$. This proves the first part of the
  theorem.

  To obtain the second part, begin with $\set{\vect{u}_{1},\ldots,\vect{u}
    _{k}}$ and suppose a basis for $V$ is
  \begin{equation*}
    \set{\vect{v}_{1},\ldots,\vect{v}_{n}}
  \end{equation*}
  If
  \begin{equation*}
    \sspan\set{\vect{u}_{1},\ldots,\vect{u}_{k}} =V,
  \end{equation*}
  then $k=n$. If not, there exists a vector
  \begin{equation*}
    \vect{u}_{k+1}\notin \sspan\set{\vect{u}_{1},\ldots,\vect{u}
      _{k}}
  \end{equation*}
  Then from Lemma~\ref{lem:adding-linearly-independent},
  $\set{\vect{u}_{1},\ldots,\vect{u}_{k}, \vect{u}_{k+1}}$ is also
  linearly independent. Continue adding vectors in this way until $n$
  linearly independent vectors have been obtained. Then
  \begin{equation*}
    \sspan\set{\vect{u}_{1},\ldots,\vect{u}_{n}} =V
  \end{equation*}
  because if it did not do so, there would exist $\vect{u}_{n+1}$ as
  just described and $\set{\vect{u}_{1},\ldots,\vect{u}_{n+1}} $ would
  be a linearly independent set of vectors having $n+1$ elements. This
  contradicts the fact that $\set{\vect{v}_{1},\ldots,\vect{v}_{n}} $
  is a basis.  In turn this would contradict
  Theorem~\ref{thm:exchange-theorem}. Therefore, this list is a basis.
\end{proof}

Recall Example~\ref{exa:adding-linear-independent} in which we added a
matrix to a linearly independent set to create a larger linearly
independent set. By
Theorem~\ref{thm:basis-from-spanning-linear-independent} we can extend
a linearly independent set to a basis.

\begin{example}{Adding to a linearly independent set}{adding-linear-independent-basis}
  Let $S \subseteq M_{22}$ be a linearly independent set given by
  \begin{equation*}
    S  = \set{\begin{mymatrix}{rr}
        1 & 0 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 1 \\
        0 & 0
      \end{mymatrix} }
  \end{equation*}
  Enlarge $S$ to a basis of $M_{22}$.
\end{example}

\begin{solution}
  Recall from the solution of
  Example~\ref{exa:adding-linear-independent} that the set
  $R \subseteq M_{22}$ given by
  \begin{equation*}
    R = \set{\begin{mymatrix}{rr}
        1 & 0 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 1 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 0 \\
        1 & 0
      \end{mymatrix} }
  \end{equation*}
  is also linearly independent.  However this set is still not a basis
  for $M_{22}$ as it is not a spanning set. In particular,
  $\begin{mymatrix}{rr}
    0 & 0 \\
    0 & 1
  \end{mymatrix}$ is not in $\sspan R$. Therefore, this matrix can be
  added to the set by Lemma~\ref{lem:adding-linearly-independent} to
  obtain a new linearly independent set given by
  \begin{equation*}
    T = \set{\begin{mymatrix}{rr}
        1 & 0 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 1 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 0 \\
        1 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 0 \\
        0 & 1
      \end{mymatrix} }
  \end{equation*}

  This set is linearly independent and now spans $M_{22}$. Hence $T$
  is a basis.
\end{solution}

Next we consider the case where you have a spanning set and you want a
subset which is a basis. The above discussion involved adding vectors
to a set. The next theorem involves removing vectors.

\begin{theorem}{Basis from a spanning set}{}
  Let $V$ be a vector space and let $W$ be a subspace. Also suppose
  that $W=\sspan\set{\vect{w}_{1},\ldots,\vect{w} _{m}}$. Then there
  exists a subset of $\set{\vect{w}_{1},\ldots, \vect{w}_{m}} $ which
  is a basis for $W$.
\end{theorem}

\begin{proof}
  Let $S$ denote the set of positive integers such that for $ k\in S$,
  there exists a subset of $\set{\vect{w}_{1},\ldots,\vect{w} _{m}} $
  consisting of exactly $k$ vectors which is a spanning set for
  $W$. Thus $m\in S$. Pick the smallest positive integer in $S$. Call
  it $k$.  Then there exists
  $\set{\vect{u}_{1},\ldots,\vect{u}_{k}} \subseteq
  \set{\vect{w}_{1},\ldots,\vect{w}_{m}} $ such that
  $\sspan \set{\vect{u}_{1},\ldots,\vect{u}_{k}} =W$. If
  \begin{equation*}
    \sum_{i=1}^{k}c_{i}\vect{w}_{i}=\vect{0}
  \end{equation*}
  and not all of the $c_{i}=0$, then you could pick $c_{j}\neq 0$,
  divide by it and solve for $\vect{u}_{j}$ in terms of the others.
  \begin{equation*}
    \vect{w}_{j}=\sum_{i\neq j}\paren{-\frac{c_{i}}{c_{j}}} \vect{w}_{i}
  \end{equation*}
  Then you could delete $\vect{w}_{j}$ from the list and have the same
  span.  In any linear combination involving $\vect{w}_{j}$, the
  linear combination would equal one in which $\vect{w}_{j}$ is
  replaced with the above sum, showing that it could have been
  obtained as a linear combination of $\vect{w}_{i}$ for $i\neq
  j$. Thus $k-1\in S$ contrary to the choice of $k$ . Hence each
  $c_{i}=0$ and so $\set{\vect{u}_{1},\ldots,\vect{u} _{k}} $ is a
  basis for $W$ consisting of vectors of
  $\set{\vect{w} _{1},\ldots,\vect{w}_{m}}$.
\end{proof}

Consider the following example of this concept.

\begin{example}{Basis from a spanning set}{}
  Let $V$ be the vector space of polynomials of degree no more than 3,
  denoted earlier as $\Poly_{3}$. Consider the following vectors in
  $V$.
  \begin{eqnarray*}
    &&2x^{2}+x+1,x^{3}+4x^{2}+2x+2,2x^{3}+2x^{2}+2x+1, \\
    &&x^{3}+4x^{2}-3x+2,x^{3}+3x^{2}+2x+1
  \end{eqnarray*}
  Then, as mentioned above, $V$ has dimension 4 and so clearly these
  vectors are not linearly independent. A basis for $V$ is
  $\set{ 1,x,x^{2},x^{3}}$. Determine a linearly independent subset of
  these which has the same span. Determine whether this subset is a
  basis for $V$.
\end{example}

\begin{solution}
  Consider an isomorphism which maps $\R%
  ^{4}$ to $V$ in the obvious way. Thus
  \begin{equation*}
    \begin{mymatrix}{c}
      1 \\
      1 \\
      2 \\
      0
    \end{mymatrix}
  \end{equation*}
  corresponds to $2x^{2}+x+1$ through the use of this
  isomorphism. Then corresponding to the above vectors in $V$ we would
  have the following vectors in $\R^{4}$.
  \begin{equation*}
    \begin{mymatrix}{c}
      1 \\
      1 \\
      2 \\
      0
    \end{mymatrix},
    \begin{mymatrix}{c}
      2 \\
      2 \\
      4 \\
      1
    \end{mymatrix},
    \begin{mymatrix}{c}
      1 \\
      2 \\
      2 \\
      2
    \end{mymatrix},
    \begin{mymatrix}{r}
      2 \\
      -3 \\
      4 \\
      1
    \end{mymatrix},
    \begin{mymatrix}{c}
      1 \\
      2 \\
      3 \\
      1
    \end{mymatrix}
  \end{equation*}
  Now if we obtain a subset of these which has the same span but which
  is linearly independent, then the corresponding vectors from $V$
  will also be linearly independent. If there are four in the list,
  then the resulting vectors from $V$ must be a basis for $V$.  The
  {\rref} for the matrix which has the above vectors as columns is
  \begin{equation*}
    \begin{mymatrix}{rrrrr}
      1 & 0 & 0 & -15 & 0 \\
      0 & 1 & 0 & 11 & 0 \\
      0 & 0 & 1 & -5 & 0 \\
      0 & 0 & 0 & 0 & 1
    \end{mymatrix}
  \end{equation*}
  Therefore, a basis for $V$ consists of the vectors
  \begin{eqnarray*}
    &&2x^{2}+x+1,x^{3}+4x^{2}+2x+2,2x^{3}+2x^{2}+2x+1, \\
    &&x^{3}+3x^{2}+2x+1.
  \end{eqnarray*}
  Note how this is a subset of the original set of vectors. If there
  had been only three pivot columns in this matrix, then we would not
  have had a basis for $V$ but we would at least have obtained a
  linearly independent subset of the original set of vectors in this
  way.

  Note also that, since all linear relations are preserved by an
  isomorphism,
  \begin{eqnarray*}
    &&-15(2x^{2}+x+1) +11(x^{3}+4x^{2}+2x+2) +(
       -5) (2x^{3}+2x^{2}+2x+1) \\
    &=&x^{3}+4x^{2}-3x+2
  \end{eqnarray*}

\end{solution}

Consider the following example.

\begin{example}{Shrinking a spanning set}{shrink-spanning}
  Consider the set $S \subseteq \Poly_2$ given by
  \begin{equation*}
    S = \set{1, x, x^2, x^2 + 1 }
  \end{equation*}
  Show that $S$ spans $\Poly_2$, then remove vectors from $S$ until it
  creates a basis.
\end{example}

\begin{solution}
  First we need to show that $S$ spans $\Poly_2$. Let $ax^2 + bx + c$
  be an arbitrary polynomial in $\Poly_2$. Write
  \begin{equation*}
    ax^2 + bx + c = r(1) + s(x) + t(x^2) + u (x^2 + 1)
  \end{equation*}
  Then,
  \begin{eqnarray*}
    ax^2 +bx + c &=& r(1) + s(x) + t(x^2) + u (x^2 + 1) \\
                 &=& (t+u) x^2 + s(x) + (r+u)
  \end{eqnarray*}

  It follows that
  \begin{eqnarray*}
    a &=& t + u \\
    b &=& s \\
    c &=& r + u
  \end{eqnarray*}
  Clearly a solution exists for all $a,b,c$ and so $S$ is a spanning
  set for $\Poly_2$. By
  Theorem~\ref{thm:basis-from-spanning-linear-independent}, some
  subset of $S$ is a basis for $\Poly_2$.

  Recall that a basis must be both a spanning set and a linearly
  independent set.  Therefore we must remove a vector from $S$ keeping
  this in mind. Suppose we remove $x$ from $S$. The resulting set
  would be $\set{1, x^2, x^2 + 1 }$. This set is clearly linearly
  dependent (and also does not span $\Poly_2$) and so is not a basis.

  Suppose we remove $x^2 + 1$ from $S$. The resulting set is
  $\set{1, x, x^2 }$ which is both linearly independent and spans
  $\Poly_2$. Hence this is a basis for $\Poly_2$. Note that removing
  any one of $1, x^2$, or $x^2 + 1$ will result in a basis.
\end{solution}

Now the following is a fundamental result about subspaces.

\begin{theorem}{Basis of a vector space}{basis-vector-space}
  Let $V$ be a finite dimensional vector space and let $W$ be a
  non-zero subspace. Then $W$ has a basis. That is, there exists a
  linearly independent set of vectors
  $\set{\vect{w}_{1},\ldots,\vect{w}_{r}} $ such that
  \begin{equation*}
    \sspan\set{\vect{w}_{1},\ldots,\vect{w}_{r}} =W
  \end{equation*}
  Also if $\set{\vect{w}_{1},\ldots,\vect{w}_{s}} $ is a linearly
  independent set of vectors, then $W$ has a basis of the form
  $\set{\vect{w} _{1},\ldots,\vect{w}_{s},\ldots,\vect{w}_{r}} $ for
  $r\geq s$.
\end{theorem}

\begin{proof}
  Let the dimension of $V$ be $n$. Pick $\vect{w}_{1}\in W$ where
  $\vect{w}_{1}\neq \vect{0}$. If $\vect{w}_{1},\ldots,\vect{w}_{s}$
  have been chosen such that $\set{\vect{w}_{1},\ldots,\vect{w}_{s}} $
  is linearly independent, if
  $\sspan\set{\vect{w}_{1},\ldots,\vect{w} _{r}} =W$, stop. You have
  the desired basis. Otherwise, there exists
  $ \vect{w}_{s+1}\notin \sspan\set{\vect{w}_{1},\ldots,\vect{w} _{s}}
  $ and $\set{\vect{w}_{1},\ldots, \vect{w}_{s},\vect{w}_{s+1}} $ is
  linearly independent. Continue this way until the process stops. It
  must stop since otherwise, you could obtain a linearly independent
  set of vectors having more than $n$ vectors which is impossible.

  The last claim is proved by following the above procedure starting
  with $ \set{\vect{w}_{1},\ldots,\vect{w}_{s}} $ as above.
\end{proof}

This also proves the following corollary. Let $V$ play the role of
$ W$ in the above theorem and begin with a basis for $W$, enlarging it
to form a basis for $V$ as discussed above.

\begin{corollary}{Basis extension}{}
  Let $W$ be any non-zero subspace of a vector space $V$.  Then every
  basis of $W$ can be extended to a basis for $V$.
\end{corollary}

Consider the following example.

\begin{example}{Basis extension}{}
  Let $V=\R^{4}$ and let
  \begin{equation*}
    W=\sspan\set{\begin{mymatrix}{c}
        1 \\
        0 \\
        1 \\
        1
      \end{mymatrix} ,\begin{mymatrix}{c}
        0 \\
        1 \\
        0 \\
        1
      \end{mymatrix} }
  \end{equation*}
  Extend this basis of $W$ to a basis of $V$.
\end{example}

\begin{solution}
  An easy way to do this is to take the {\rref} of the matrix
  \begin{equation}
    \begin{mymatrix}{cccccc}
      1 & 0 & 1 & 0 & 0 & 0 \\
      0 & 1 & 0 & 1 & 0 & 0 \\
      1 & 0 & 0 & 0 & 1 & 0 \\
      1 & 1 & 0 & 0 & 0 & 1
    \end{mymatrix}  \label{vector-space-eq1}
  \end{equation}
  Note how the given vectors were placed as the first two and then the
  matrix was extended in such a way that it is clear that the span of
  the columns of this matrix yield all of $\R^{4}$. Now determine the
  pivot columns.  The {\rref} is
  \begin{equation}
    \begin{mymatrix}{rrrrrr}
      1 & 0 & 0 & 0 & 1 & 0 \\
      0 & 1 & 0 & 0 & -1 & 1 \\
      0 & 0 & 1 & 0 & -1 & 0 \\
      0 & 0 & 0 & 1 & 1 & -1
    \end{mymatrix}  \label{vector-space-eq2}
  \end{equation}
  These are
  \begin{equation*}
    \begin{mymatrix}{c}
      1 \\
      0 \\
      1 \\
      1
    \end{mymatrix} ,\begin{mymatrix}{c}
      0 \\
      1 \\
      0 \\
      1
    \end{mymatrix} ,\begin{mymatrix}{c}
      1 \\
      0 \\
      0 \\
      0
    \end{mymatrix} ,\begin{mymatrix}{c}
      0 \\
      1 \\
      0 \\
      0
    \end{mymatrix}
  \end{equation*}
  and now this is an extension of the given basis for $W$ to a basis
  for $ \R^{4}$.

  Why does this work? The columns of {\eqref{vector-space-eq1}}
  obviously span $\R ^{4}$ the span of the first four is the same as
  the span of all six.
\end{solution}
