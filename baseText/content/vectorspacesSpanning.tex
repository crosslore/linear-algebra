\section{Linear combinations, span, and linear independence}

\begin{outcome}
  \begin{enumerate}
  \item Determine if a vector is within a given span.
  \item Determine if a set is spanning.
  \item Determine if a set is linearly independent.
  \end{enumerate}
\end{outcome}

In this section, we will again explore concepts introduced earlier in terms of $\R^n$ and extend them to apply to abstract vector spaces.

We can now revisit many of the concepts first introduced in
Chapter~\ref{cha:vectors-rn} in the context of general vector spaces.
We will look at linear combinations, span, and linear independence in
this section, and at subspaces, bases, and dimension in the next
section.

\begin{definition}{Linear combination}{linear-combination}
  Let $V$ be a vector space over a field $K$. Let
  $\vect{u}_1,\ldots,\vect{u}_n\in V$. A vector
  $\vect{v}\in V$ is called a \textbf{linear combination}%
  \index{linear combination!in a vector space}%
  \index{linear combination!of vectors} of
  $\vect{u}_1,\ldots,\vect{u}_n$ if there exist scalars
  $a_{1},\ldots,a_{n}\in K$ such that
  \begin{equation*}
    \vect{v} = a_1 \vect{u}_1 + \ldots + a_n \vect{u}_n.
  \end{equation*}
\end{definition}

\begin{example}{Linear combination of matrices}{linear-combination-matrix}
  Write the matrix $A=\begin{mymatrix}{rr} 1 & 3 \\ -1 & 2 \end{mymatrix}$
  as a linear combination%
  \index{linear combination!of matrices} of
  \begin{equation*}
    \begin{mymatrix}{rr} 1 & 0 \\ 0 & 1 \end{mymatrix},\quad
    \begin{mymatrix}{rr} 1 & 0 \\ 0 & -1 \end{mymatrix},\quad
    \begin{mymatrix}{rr} 0 & 1 \\ 1 & 0 \end{mymatrix},\quad\mbox{and}\quad
    \begin{mymatrix}{rr} 0 & -1 \\ 1 & 0 \end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  We must find coefficients $a,b,c,d$ such that
  \begin{equation*}
    \begin{mymatrix}{rr} 1 & 3 \\ -1 & 2 \end{mymatrix}
    ~=~ a \begin{mymatrix}{rr} 1 & 0 \\ 0 & 1 \end{mymatrix}
    + b \begin{mymatrix}{rr} 1 & 0 \\ 0 & -1 \end{mymatrix}
    + c \begin{mymatrix}{rr} 0 & 1 \\ 1 & 0 \end{mymatrix}
    + d \begin{mymatrix}{rr} 0 & -1 \\ 1 & 0 \end{mymatrix},
  \end{equation*}
  or equivalently,
  \begin{equation*}
    \begin{mymatrix}{rr} 1 & 3 \\ -1 & 2 \end{mymatrix}
    ~=~ \begin{mymatrix}{cc} a+b & c-d \\ c+d & a-b \end{mymatrix}.
  \end{equation*}
  This yields a system of four equations in four variables:
  \begin{equation*}
    \begin{array}{r@{~~}c@{~}r}
      a+b &=& 1, \\
      c+d &=& -1, \\
      c-d &=& 3, \\
      a-b &=& 2.
    \end{array}
  \end{equation*}
  We can easily solve the system of equations to find the unique
  solution $a=\frac{3}{2}$, $b=-\frac{1}{2}$, $c=1$, $d=-2$.
  Therefore
  \begin{equation*}
    \begin{mymatrix}{rr} 1 & 3 \\ -1 & 2 \end{mymatrix}
    ~=~ \frac{3}{2} \begin{mymatrix}{rr} 1 & 0 \\ 0 & 1 \end{mymatrix}
    - \frac{1}{2} \begin{mymatrix}{rr} 1 & 0 \\ 0 & -1 \end{mymatrix}
    + 1 \begin{mymatrix}{rr} 0 & 1 \\ 1 & 0 \end{mymatrix}
    - 2 \begin{mymatrix}{rr} 0 & -1 \\ 1 & 0 \end{mymatrix}.
  \end{equation*}
\end{solution}

\begin{example}{Linear combination of polynomials}{linear-combination-polynomials}
  Write the polynomial $p(x) = 7x^2 + 4x - 3$ as a linear combination%
  \index{linear combination!of polynomials} of
  \begin{equation*}
    q_1(x) = x^2,\quad
    q_2(x) = (x+1)^2,\quad\mbox{and}\quad
    q_3(x) = (x+2)^2.
  \end{equation*}
\end{example}

\begin{solution}
  Note that $q_2(x) = (x+1)^2 = x^2 + 2x + 1$ and
  $q_3(x) = (x+2)^2 = x^2 + 4x + 4$. We must find coefficients $a,b,c$
  such that $p(x) = aq_1(x) + bq_2(x) + cq_3(x)$, or equivalently,
  \begin{equation*}
    7x^2 + 4x - 3 ~=~ ax^2 ~+~ b(x^2 + 2x + 1) ~+~ c(x^2 + 4x + 4).
  \end{equation*}
  Collecting equal powers of $x$, we can rewrite this as
  \begin{equation*}
    7x^2 + 4x - 3 ~=~ (a+b+c)x^2 ~+~ (2b+4c)x ~+~ (b+4c).
  \end{equation*}
  Since two polynomials are equal if and only if each corresponding
  coefficient is equal, this yields a system of three equations in
  three variables
  \begin{equation*}
    \begin{array}{r@{~~}c@{~}r}
      a+b+c &=& 7, \\
      2b+4c &=& 4, \\
      b+4c &=& -3.
    \end{array}
  \end{equation*}
  We can easily solve this system of equations and find that the
  unique solution is $a=\frac{5}{2}$, $b=7$,
  $c=-\frac{5}{2}$. Therefore
  \begin{equation*}
    p(x) ~=~ \frac{5}{2}\,q_1(x) ~+~ 7\,q_2(x) ~-~ \frac{5}{2}\,q_3(x).
  \end{equation*}
\end{solution}

As in Chapter~\ref{cha:vectors-rn}, the span of a set of vectors is
defined as the set of all of its linear combinations. We generalize
the concept of span to consider spans of arbitrary (possibly finite,
possibly infinite) sets of vectors.

\begin{definition}{Span of a set of vectors}{vector-space-span}
  Let $V$ be a vector space over some field $K$, and let $S$ be a set
  of vectors (i.e., a subset of $V$). The \textbf{span}%
  \index{span}%
  \index{vector!span}%
  \index{vector space!span in} of $S$ is the set of all linear
  combinations of elements of $S$. In symbols, we have
  \begin{equation*}
    \sspan S
    ~=~ \set{a_1\vect{u}_1+\ldots+a_k\vect{u}_k \mid
      \mbox{
        $\vect{u}_1,\ldots,\vect{u}_k\in S$
        and
        $a_1,\ldots,a_k\in K$
      }}.
  \end{equation*}
\end{definition}

It is important not to misunderstand this definition.  Even when the
set $S$ is infinite, each {\em individual} element
$\vect{v}\in\sspan S$ is a linear combination of only {\em finitely
  many} elements $\vect{u}_1,\ldots,\vect{u}_k$ of $S$.
The definition does not talk about infinite linear combinations
\begin{equation*}
  a_1\vect{u}_1 + a_2\vect{u}_2 + a_3\vect{u}_3 + \ldots
\end{equation*}
Indeed, such infinite sums do not typically exist.  However, different
elements $\vect{v},\vect{w}\in\sspan S$ can be linear combinations of
a different (finite) number of vectors of $S$. For example, it is
possible that $\vect{v}$ is a linear combination of 10 elements of
$S$, and $\vect{w}$ is a linear combination of 100 elements of $S$.

\begin{example}{Spans of sequences}{spans-sequences}
  Consider the vector space $\Seq_K$ of infinite sequences. For every
  $k\in\N$, let $e^k$ be the sequence that has a $1$ in the $k\th$
  component, and is $0$ everywhere else, i.e.,
  \begin{equation*}
    \begin{array}{l}
      e^0 = 1,0,0,0,0,\ldots, \\
      e^1 = 0,1,0,0,0,\ldots, \\
      e^2 = 0,0,1,0,0,\ldots, \\
    \end{array}
  \end{equation*}
  and so on.
  Let $S=\set{e^k \mid k\in\N}$. Which of the following sequences are in
  $\sspan S$?
  \begin{enumialphparenastyle}
    \begin{enumerate}
    \item $f = 1,1,1,0,0,0,0,0,\ldots$ (followed by infinitely many zeros),
    \item $g = 1,2,0,5,0,0,0,0,\ldots$ (followed by infinitely many zeros),
    \item $h = 1,1,1,1,1,1,1,1,\ldots$ (followed by infinitely many ones),
    \item $k = 1,0,1,0,1,0,1,0,\ldots$ (forever alternating between $1$ and $0$).
    \end{enumerate}
  \end{enumialphparenastyle}
\end{example}

\begin{solution}
  \begin{enumialphparenastyle}
    \begin{enumerate}
    \item We have $f\in\sspan S$, because $f = e^0 + e^1 + e^2$.
    \item We have $g\in\sspan S$, because $g = 1e^0 + 2e^1 + 5e^3$.
    \item The sequence $h$ is not in $\sspan S$, because each element
      of $\sspan S$ is, by definition, a linear combinations of {\em
        finitely many} elements of $S$. No linear combinations of
      finitely many $e^k$ can end in infinitely many ones. Note that
      we are not permitted to write an infinite sum such as
      $e^0+e^1+e^2+\ldots$. Such infinite sums are not defined in
      vector spaces.
    \item The sequence $k$ is not in $\sspan S$, for the same reason.
      We would need to add infinitely many sequences of the form $e^k$
      to get a sequence that contains infinitely many non-zero
      elements. However, this is not permitted by the definition of
      span.
    \end{enumerate}
  \end{enumialphparenastyle}
  \vspace{-4ex}
\end{solution}

\begin{example}{Span of polynomials}{span-of-polynomials}
  Let $p(x)=7x^2+4x-3$. Is $p(x)\in\sspan\set{x^2,~ (x+1)^2,~ (x+2)^2}$?
\end{example}

\begin{solution}
  The answer is yes, because we found in
  Example~\ref{exa:linear-combination-polynomials} that
  $p(x) = \frac{5}{2}\,x^2 ~+~ 7\,(x+1)^2 ~-~ \frac{5}{2}\,(x+2)^2$.
\end{solution}

We say that a set of vectors $S$ is a \textbf{spanning set}%
\index{spanning set}%
\index{vector space!spanning set} for $V$ if $V = \sspan S$.

\begin{example}{Spanning set}{spanning-set}
  Let $S = \set{x^2,~ (x+1)^2,~ (x+2)^2 }$. Show that $S$ is a
  spanning set for $\Poly_2$, the vector space of all polynomials of
  degree at most $2$.
\end{example}

\begin{solution}
  This is analogous to Example~\ref{exa:linear-combination-polynomials}.
  Consider an arbitrary element $p(x) = p_2x^2 + p^1x + p_0$ of
  $\Poly_2$. We must show that $p(x)\in\sspan S$, i.e., that there
  exists $a,b,c\in K$ such that
  \begin{equation*}
    p(x) = ax^2 + b(x+1)^2 + c(x+2)^2.
  \end{equation*}
  We can equivalently rewrite this equation as
  \begin{equation*}
    p_2x^2 + p^1x + p_0 ~=~ (a+b+c)x^2 ~+~ (2b+4c)x ~+~ (b+4c),
  \end{equation*}
  which yields the system of equations
  \begin{equation*}
    \begin{array}{r@{~~}c@{~}r}
      a+b+c &=& p_2 \\
      2b+4c &=& p_1 \\
      b+4c &=& p_0
    \end{array}
    \quad\roweq\quad
    \begin{mymatrix}{ccc|c}
      1 & 1 & 1 & p_2 \\
      0 & 2 & 4 & p_1 \\
      0 & 1 & 4 & p_0 \\
    \end{mymatrix}
    \quad\roweq\quad
    \begin{mymatrix}{ccc|c}
      1 & 0 & 0 & p_2-\frac{3}{4}p_1+\frac{1}{2}p_0 \\
      0 & 1 & 0 & p_1-p_0 \\
      0 & 0 & 1 & \frac{1}{2}p_0-\frac{1}{4}p_1 \\
    \end{mymatrix}.
  \end{equation*}
  Since the system has rank 3, it has a solution. Therefore,
  $p(x)\in\sspan S$. Since $p(x)$ was an arbitrary element of
  $\Poly_2$, it follows that $S$ is a spanning set for $\Poly_2$.
\end{solution}

To define the concept of linear independence in a general vector
space, it will be convenient to base our definition on the
``alternative'' characterization of
Theorem~\ref{characterization-linear-independence}. Here too, we
generalize the definition to an arbitrary (finite or infinite) set of
vectors.

\begin{definition}{Linear independence}{linear-independence-vector-space}
  Let $V$ be a vector space over some field $K$. A finite set of
  vectors $\set{\vect{u}_1,\ldots,\vect{u}_k}$ is called
  \textbf{linearly independent}%
  \index{linear independence!in a vector space}%
  \index{vector!linearly independent!in a vector space}%
  \index{vector space!linear independence in}
  if the equation
  \begin{equation*}
    a_1\,\vect{u}_1 + \ldots + a_k\,\vect{u}_k = \vect{0}
  \end{equation*}
  has only the trivial solution $a_1,\ldots,a_k=0$. An infinite set
  $S$ of vectors is called linearly independent if every finite subset
  of $S$ is linearly independent. A set of vectors is called
  \textbf{linearly dependent}%
  \index{linear dependence}%
  \index{vector!linearly dependent}if it is not linearly independent.
\end{definition}

\begin{example}{Linearly independent polynomials}{linear-independence-polynomial}
  Determine whether the polynomials $x^2$, $x^2 + 2x - 1$, and
  $2x^2 - x + 3$ are linearly independent.
\end{example}

\begin{solution}
  According to the definition of linear independence, we must solve
  the equation
  \begin{equation*}
    a x^2 + b ( x^2 + 2x -1 ) + c(2x^2 - x + 3) ~=~ 0.
  \end{equation*}
  If there is a non-trivial solution, the polynomials are linearly
  dependent. If there is only the trivial solution, they are linearly
  independent. We first rearrange the left-hand side to collect equal
  powers of $x$:
  \begin{equation*}
    (a+b+2c)x^2 + (2b-c)x  +(3c-b) ~=~ 0.
  \end{equation*}
  This turns into a system of 3 equations in 3 variables:
  \begin{equation*}
    \begin{array}{rcl}
      a+b+2c &=& 0 \\
      2b-c &=& 0 \\
      3c-b &=& 0
    \end{array}
    \quad\roweq\quad
    \begin{mymatrix}{rrr|r}
      1 &  1 &  2 & 0 \\
      0 &  2 & -1 & 0 \\
      0 & -1 &  3 & 0
    \end{mymatrix}
    \quad\roweq\quad
    \begin{mymatrix}{rrr|r}
      1 &  0 &  0 & 0 \\
      0 &  1 &  0 & 0 \\
      0 &  0 &  1 & 0
    \end{mymatrix}.
  \end{equation*}
  Since the system has rank 3, there are no free variables. The only
  solution is $a=b=c=0$, and the polynomials are linearly
  independent.
\end{solution}

\begin{example}{Linearly independent sequences}{linear-independence-sequences}
  Let $K$ be a field, and consider again the sequences from
  Example~\ref{exa:spans-sequences},
    \begin{equation*}
    \begin{array}{l}
      e^0 = 1,0,0,0,0,\ldots, \\
      e^1 = 0,1,0,0,0,\ldots, \\
      e^2 = 0,0,1,0,0,\ldots, \\
    \end{array}
  \end{equation*}
  and so on. Let $S=\set{e^0, e^1, e^2,\ldots}$. This is an infinite
  subset of $\Seq_K$. Show that $S$ is linearly independent.
\end{example}

\begin{solution}
  Since $S$ is an infinite set, we have to show that every finite
  subset of $S$ is linearly independent. So consider a finite subset
  \begin{equation*}
    \set{e^{k_1}, e^{k_2}, \ldots, e^{k_n}} \subseteq S
  \end{equation*}
  and assume that
  \begin{equation}\label{eqn:linear-independence-sequences}
    a_1e^{k_1} + a_2e^{k_2} + \ldots + a_ne^{k_n} = 0.
  \end{equation}
  We have to show that $a_1,\ldots,a_n=0$. Consider some index
  $i\in\set{1,\ldots,n}$. Then the $k_i\th$ element of
  $a_1e^{k_1} + \ldots + a_ne^{k_n}$ is equal to $a_i$ by the
  left-hand side of {\eqref{eqn:linear-independence-sequences}}, but
  it is also equal to $0$ by the right-hand side of
  {\eqref{eqn:linear-independence-sequences}}. It follows that $a_i=0$
  for all $i\in\set{1,\ldots,n}$, and therefore
  $\set{e^{k_1}, e^{k_2}, \ldots, e^{k_n}}$ is linearly
  independent. Since $\set{e^{k_1}, e^{k_2}, \ldots, e^{k_n}}$ was an
  arbitrary finite subset of $S$, it follows, by definition, that $S$
  is linearly independent.  
\end{solution}

\begin{example}{Linearly dependent matrices}{linearly-dependent-matrices}
  Determine whether the following elements of $\Mat_{m,n}$ are
  linearly independent:
  \begin{equation*}
    M_1 = \begin{mymatrix}{rr} -1 & 0 \\ 1 & -1 \end{mymatrix},\quad
    M_2 = \begin{mymatrix}{rr}  1 & 1 \\ 1 &  2 \end{mymatrix},\quad
    M_3 = \begin{mymatrix}{rr}  1 & 3 \\ 5 &  4 \end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  To determine whether $\set{M_1,M_2,M_3}$ is linearly independent, we
  look for solutions to
  \begin{equation*}
    aM_1 + bM_2 + cM_3 = 0.
  \end{equation*}
  Notice that this equation has non-trivial solutions, for example
  $a=2$, $b=3$ and $c=-1$. Therefore the matrices are linearly
  dependent.
\end{solution}

% ======================================================================
\subsection*{CONTINUE HERE...}

The following is an important result regarding dependent sets.

\begin{lemma}{Dependent sets}{dependent}
  Let $V$ be a vector space and suppose
  $W = \set{\vect{v}_1, \vect{v}_2,\ldots, \vect{v}_k }$ is a subset
  of $V$. Then $W$ is dependent if and only if $\vect{v}_i$ can be
  written as a linear combination of
  $\set{\vect{v}_1, \vect{v}_2,\ldots, \vect{v}_{i-1},
    \vect{v}_{i+1},\ldots, \vect{v}_k }$ for some $i \leq k$.
\end{lemma}

Revisit Example~\ref{exa:dependent} with this in mind. Notice that we
can write one of the three vectors as a combination of the others.
\begin{equation*}
  \begin{mymatrix}{c} 1 \\ 3 \\ 5 \end{mymatrix}
  =
  2\begin{mymatrix}{c} -1 \\ 0 \\ 1 \end{mymatrix}
  +3\begin{mymatrix}{c} 1 \\ 1 \\ 1 \end{mymatrix}
\end{equation*}
By Lemma~\ref{lem:dependent} this set is dependent.

If we know that one particular set is linearly independent, we can use
this information to determine if a related set is linearly
independent. Consider the following example.

\begin{example}{Related independent sets}{related-independent}
  Let $V$ be a vector space and suppose $S \subseteq V$ is a set of
  linearly independent vectors given by
  $S = \set{\vect{u}, \vect{v}, \vect{w} }$. Let $R \subseteq V$ be
  given by
  $R = \set{2\vect{u} - \vect{w}, \vect{w} + \vect{v}, 3\vect{v} +
    \frac{1}{2} \vect{u} }$. Show that $R$ is also linearly
  independent.
\end{example}

\begin{solution}
  To determine if $R$ is linearly independent, we write
  \begin{equation*}
    a(2\vect{u} - \vect{w}) + b(\vect{w} + \vect{v}) + c( 3\vect{v} +
    \vspace{0.05in}\frac{1}{2}\vect{u}) = \vect{0}
  \end{equation*}
  If the set is linearly independent, the only solution will be
  $a=b=c=0$. We proceed as follows.
  \begin{eqnarray*}
    a(2\vect{u} - \vect{w}) + b(\vect{w} + \vect{v}) + c( 3\vect{v} + \vspace{0.05in}\frac{1}{2} \vect{u}) &=& \vect{0} \\
    2a\vect{u} - a\vect{w} + b\vect{w} + b\vect{v}  + 3c\vect{v} + \vspace{0.05in}\frac{1}{2}c\vect{u} &=& \vect{0}\\
    (2a + \vspace{0.05in}\frac{1}{2}c) \vect{u} + (b+3c)\vect{v} + (-a + b) \vect{w} &=& \vect{0}
  \end{eqnarray*}

  We know that the set $S = \set{\vect{u}, \vect{v}, \vect{w} }$ is
  linearly independent, which implies that the coefficients in the
  last line of this equation must all equal $0$.  In other words:
  \begin{eqnarray*}
    2a + \vspace{0.05in}\frac{1}{2} c &=& 0 \\
    b + 3c &=& 0 \\
    -a + b &=& 0
  \end{eqnarray*}

  The augmented matrix and resulting {\rref} are given by:
  \begin{equation*}
    \begin{mymatrix}{rrr|r}
      2 & 0 & \vspace{0.05in}\frac{1}{2} & 0 \\
      0 & 1 & 3 & 0 \\
      -1 & 1 & 0 & 0
    \end{mymatrix}
    \roweq\ldots\roweq
    \begin{mymatrix}{rrr|r}
      1 & 0 & 0 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 1 & 0
    \end{mymatrix}
  \end{equation*}
  Hence the solution is $a=b=c=0$ and the set is linearly independent.
\end{solution}

The following theorem was discussed in terms in $\R^n$. We consider it
here in the general case.

\begin{theorem}{Unique representation}{unique-representation}
  Let $V$ be a vector space and let
  $U = \set{\vect{v}_1,\ldots, \vect{v}_k } \subseteq V$ be an
  independent set. If $\vect{v} \in \sspan \;U$, then $\vect{v}$ can
  be written uniquely as a linear combination of the vectors in $U$.
\end{theorem}

Consider the span of a linearly independent set of vectors. Suppose we
take a vector which is not in this span and add it to the set. The
following lemma claims that the resulting set is still linearly
independent.

\begin{lemma}{Adding to a linearly independent set}{adding-linearly-independent}
  Suppose
  $\vect{v}\notin \sspan\set{\vect{u}_{1},\ldots,\vect{u}_{k}} $ and
  $\set{\vect{u}_{1},\ldots, \vect{u}_{k}} $ is linearly
  independent. Then the set
  \begin{equation*}
    \set{\vect{u}_{1},\ldots,\vect{u}_{k},\vect{v} }
  \end{equation*}
  is also linearly independent.
\end{lemma}

\begin{proof}
  Suppose $\sum_{i=1}^{k}c_{i}\vect{u}_{i}+d\vect{v}= \vect{0}$. It is
  required to verify that each $c_{i}=0$ and that $d=0$.  But if
  $d\neq 0$, then you can solve for $\vect{v}$ as a linear combination
  of the vectors, $\set{\vect{u}_{1},\ldots,\vect{u} _{k}}$,
  \begin{equation*}
    \vect{v}=-\sum_{i=1}^{k}\paren{\frac{c_{i}}{d}} \vect{u}_{i}
  \end{equation*}
  contrary to the assumption that $\vect{v}$ is not in the span of the
  $\vect{u}_{i}$. Therefore, $d=0$. But then
  $\sum_{i=1}^{k}c_{i} \vect{u}_{i}=\vect{0}$ and the linear
  independence of $\set{\vect{u} _{1},\ldots,\vect{u}_{k}} $ implies
  each $c_{i}=0$ also.
\end{proof}

Consider the following example.

\begin{example}{Adding to a linearly independent set}{adding-linear-independent}
  Let $S \subseteq M_{22}$ be a linearly independent set given by
  \begin{equation*}
    S  = \set{\begin{mymatrix}{rr}
        1 & 0 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 1 \\
        0 & 0
      \end{mymatrix} }
  \end{equation*}
  Show that the set $R \subseteq M_{22}$ given by
  \begin{equation*}
    R = \set{\begin{mymatrix}{rr}
        1 & 0 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 1 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 0 \\
        1 & 0
      \end{mymatrix} }
  \end{equation*}
  is also linearly independent.
\end{example}

\begin{solution}
  Instead of writing a linear combination of the matrices which equals
  $0$ and showing that the coefficients must equal $0$, we can instead
  use Lemma~\ref{lem:adding-linearly-independent}.

  To do so, we show that
  \begin{equation*}
    \begin{mymatrix}{rr}
      0 & 0 \\
      1 & 0
    \end{mymatrix}
    \notin
    \sspan\set{\begin{mymatrix}{rr}
        1 & 0 \\
        0 & 0
      \end{mymatrix}, \begin{mymatrix}{rr}
        0 & 1 \\
        0 & 0
      \end{mymatrix} }
  \end{equation*}

  Write
  \begin{eqnarray*}
    \begin{mymatrix}{rr}
      0 & 0 \\
      1 & 0
    \end{mymatrix}
    &=&  a\begin{mymatrix}{rr}
      1 & 0 \\
      0 & 0
    \end{mymatrix} +  b\begin{mymatrix}{rr}
      0 & 1 \\
      0 & 0
    \end{mymatrix} \\
    &=&
        \begin{mymatrix}{rr}
          a & 0 \\
          0 & 0
        \end{mymatrix} +  \begin{mymatrix}{rr}
          0 & b \\
          0 & 0
        \end{mymatrix} \\
    &=& \begin{mymatrix}{rr}
      a & b \\
      0 & 0
    \end{mymatrix}
  \end{eqnarray*}

  Clearly there are no possible $a,b$ to make this equation
  true. Hence the new matrix does not lie in the span of the matrices
  in $S$. By Lemma~\ref{lem:adding-linearly-independent}, $R$ is also
  linearly independent.
\end{solution}
