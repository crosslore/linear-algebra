\section{Linear independence}

In Example~\ref{exa:redundant-span}, we encountered three vectors
$\vect{u}$, $\vect{v}$, and $\vect{w}$ such that
$\sspan\set{\vect{u},\vect{v},\vect{w}} =
\sspan\set{\vect{u},\vect{v}}$.  If this happens, then the vector
$\vect{w}$ does not contribute anything to the span of
$\set{\vect{u},\vect{v},\vect{w}}$, and we say that $\vect{w}$ is
\textbf{redundant}%
\index{redundant vector}\index{vector!redundant}. The following
definition generalizes this notion.

\begin{definition}{Redundant vectors, linear dependence, and linear independence}{redundant-vectors}
  Consider a list of $k$ vectors $\vect{u}_1,\ldots,\vect{u}_k$.  We
  say that the vector $\vect{u}_j$ is \textbf{redundant}%
  \index{redundant vector}\index{vector!redundant} if it can be
  written as a linear combination of earlier vectors in the list,
  i.e., if
  \begin{equation*}
    \vect{u}_j = a_1\,\vect{u}_1 + a_2\,\vect{u}_2 + \ldots + a_{j-1}\,\vect{u}_{j-1}
  \end{equation*}
  for some scalars $a_1,\ldots,a_{j-1}$. We say that the list of
  vectors $\vect{u}_1,\ldots,\vect{u}_k$ is \textbf{linearly
    dependent}%
  \index{linear dependence}\index{vector!linearly dependent} if it
  contains one or more redundant vectors. Otherwise, we say that the
  vectors are \textbf{linearly independent}%
  \index{linear independence}\index{vector!linearly independent}.  
\end{definition}

\begin{example}{Redundant vectors}{redundant-vectors}
  Find the redundant vectors in the following list of vectors. Are
  the vectors linearly independent?
  \begin{equation*}
    \vect{u}_1 = \begin{mymatrix}{c} 0 \\ 0 \\ 0 \\ 0\end{mymatrix},
    \quad
    \vect{u}_2 = \begin{mymatrix}{c} 1 \\ 2 \\ 2 \\ 3\end{mymatrix},
    \quad
    \vect{u}_3 = \begin{mymatrix}{c} 1 \\ 1 \\ 1 \\ 1\end{mymatrix},
    \quad
    \vect{u}_4 = \begin{mymatrix}{c} 2 \\ 3 \\ 3 \\ 4\end{mymatrix},
    \quad
    \vect{u}_5 = \begin{mymatrix}{c} 0 \\ 1 \\ 2 \\ 3\end{mymatrix},
    \quad
    \vect{u}_6 = \begin{mymatrix}{c} 3 \\ 3 \\ 2 \\ 2\end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  \begin{itemize}
  \item The vector $\vect{u}_1$ is redundant, because it is a linear
    combination of earlier vectors. (Although there are no earlier
    vectors, recall from Example~\ref{span-empty-set} that the empty
    sum of vectors is equal to the zero vector $\vect{0}$. Therefore,
    $\vect{u}_1$ is indeed an (empty) linear combination of earlier
    vectors.)
  \item The vector $\vect{u}_2$ is not redundant, because it cannot be
    written as a linear combination of $\vect{u}_1$. This is because
    the system of equations
    \begin{equation*}
      \begin{mymatrix}{l|l}
        0 & 1 \\
        0 & 2 \\
        0 & 2 \\
        0 & 3 \\
      \end{mymatrix}
    \end{equation*}
    has no solution.
  \item The vector $\vect{u}_3$ is not redundant, because it cannot be
    written as a linear combination of $\vect{u}_1$ an $\vect{u}_2$.
    This is because the system of equations
    \begin{equation*}
      \begin{mymatrix}{ll|l}
        0 & 1 & 1 \\
        0 & 2 & 1\\
        0 & 2 & 1\\
        0 & 3 & 1\\
      \end{mymatrix}
    \end{equation*}
    has no solution.
  \item The vector $\vect{u}_4$ is redundant, because $\vect{u}_4 =
    \vect{u}_2 + \vect{u}_3$.
  \item The vector $\vect{u}_5$ is not redundant, because 
    This is because the system of equations
    \begin{equation*}
      \begin{mymatrix}{llll|l}
        0 & 1 & 1 & 2 & 0 \\
        0 & 2 & 1 & 3 & 1 \\
        0 & 2 & 1 & 3 & 2 \\
        0 & 3 & 1 & 4 & 3 \\
      \end{mymatrix}
    \end{equation*}
    has no solution.
  \item The vector $\vect{u}_6$ is redundant, because $\vect{u}_6 =
    \vect{u}_2 + 2\vect{u}_3-\vect{u}_5$.
  \end{itemize}
  In summary, the vectors $\vect{u}_1$, $\vect{u}_4$, and $\vect{u}_6$
  are redundant, and the vectors $\vect{u}_2$, $\vect{u}_3$, and
  $\vect{u}_5$ are not. It follows that the list of all six vectors
  $\vect{u}_1,\ldots,\vect{u}_6$ is linearly dependent.
\end{solution}

The example shows that it can be a lot of work to find the redundant
vectors in a list of $k$ vectors. Doing so in the naive way
require us to solve up to $k$ systems of linear equations!
Fortunately, there is a much faster and easier method, the so-called
{\em casting-out algorithm}.

\begin{algorithm}{Casting-out algorithm}{casting-out}
  \textbf{Input:} a list of $k$ vectors
  $\vect{u}_1,\ldots,\vect{u}_k\in\R^n$.
  \smallskip

  \textbf{Output:} the set of indices $j$ such that $\vect{u}_j$ is
  redundant.
  \smallskip
  
  \textbf{Algorithm:} Write the vectors $\vect{u}_1,\ldots,\vect{u}_k$
  as the columns of an $n\times k$-matrix, and reduce to {\ef}. Every
  {\em non-pivot} column, if any, corresponds to a redundant vector.
\end{algorithm}

\begin{example}{Casting-out algorithm}{casting-out}
  Use the casting-out algorithm to find the redundant vectors among
  the vectors from Example~\ref{exa:redundant-vectors}.
\end{example}

\begin{solution}
  Following the casting-out algorithm, we write the vectors
  $\vect{u}_1,\ldots,\vect{u}_6$ as the columns of a matrix and reduce
  to {\ef}.
  \begin{equation*}
    \begin{mymatrix}{rrrrrr}
      0 & 1 & 1 & 2 & 0 & 3 \\
      0 & 2 & 1 & 3 & 1 & 3 \\
      0 & 2 & 1 & 3 & 2 & 2 \\
      0 & 3 & 1 & 4 & 3 & 2 \\
    \end{mymatrix}
    \sim \ldots \sim
    \begin{mymatrix}{rrrrrr}
      0 & \circled{1} & 1 & 2 & 0 & 3 \\
      0 & 0 & \circled{1} & 1 & -1 & 3 \\
      0 & 0 & 0 & 0 & \circled{1} & -1 \\
      0 & 0 & 0 & 0 & 0 & 0 \\
    \end{mymatrix}.
  \end{equation*}
  The pivot columns are columns $2$, $3$, and $5$. The non-pivot
  columns are columns $1$, $4$, and $6$. Therefore, the vectors
  $\vect{u}_1$, $\vect{u}_4$, and $\vect{u}_6$ are redundant. Note
  that this is the same answer we got in
  Example~\ref{exa:redundant-vectors}.
\end{solution}

The above version of the casting-out algorithm only tells us which of
the vectors (if any) are redundant, but it does not give us a specific
way to write the redundant vectors as linear combinations of previous
vectors. However, we can easily get this additional information if we
reduce the matrix all the way to {\rref}. We call this version of the
algorithm the {\em extended casting-out algorithm}.

\begin{algorithm}{Extended casting-out algorithm}{extended-casting-out}
  \textbf{Input:} a list of $k$ vectors
  $\vect{u}_1,\ldots,\vect{u}_k\in\R^n$.
  \smallskip

  \textbf{Output:} the set of indices $j$ such that $\vect{u}_j$ is
  redundant, and a set of coefficients for writing each redundant
  vector as a linear combination of previous vectors.
  \smallskip
  
  \textbf{Algorithm:} Write the vectors $\vect{u}_1,\ldots,\vect{u}_k$
  as the columns of an $n\times k$-matrix, and reduce to
  {\rref}. Every {\em non-pivot} column, if any, corresponds to a
  redundant vector. If $\vect{u}_j$ is a redundant vector, then the
  entries in the $j\th$ column of the {\rref} are coefficients for
  writing $\vect{u}_j$ as a linear combination of previous
  non-redundant vectors.
\end{algorithm}

\begin{example}{Extended casting-out algorithm}{extended-casting-out}
  Use the casting-out algorithm to find the redundant vectors among
  the vectors from Example~\ref{exa:redundant-vectors}, and write each
  redundant vector as a linear combination of previous non-redundant
  vectors.
\end{example}

\begin{solution}
  Once again, we write the vectors $\vect{u}_1,\ldots,\vect{u}_6$ as
  the columns of a matrix. This time we use the extended casting-out
  algorithm, which means we reduce the matrix to {\rref} instead of
  {\ef}.
  \begin{equation*}
    \begin{mymatrix}{rrrrrr}
      0 & 1 & 1 & 2 & 0 & 3 \\
      0 & 2 & 1 & 3 & 1 & 3 \\
      0 & 2 & 1 & 3 & 2 & 2 \\
      0 & 3 & 1 & 4 & 3 & 2 \\
    \end{mymatrix}
    \sim \ldots \sim
    \begin{mymatrix}{rrrrrr}
      0 & \circled{1} & 0 & 1 & 0 & 1 \\
      0 & 0 & \circled{1} & 1 & 0 & 2 \\
      0 & 0 & 0 & 0 & \circled{1} & -1 \\
      0 & 0 & 0 & 0 & 0 & 0 \\
    \end{mymatrix}.
  \end{equation*}
  As before, the non-pivot columns are columns $1$, $4$, and $6$, and
  therefore, the vectors $\vect{u}_1$, $\vect{u}_4$, and $\vect{u}_6$
  are redundant. The non-redundant vectors are $\vect{u}_2$,
  $\vect{u}_3$, and $\vect{u}_5$. Moreover, the entries in the sixth
  column are $1$, $2$, and $-1$.  Note that this means that the sixth
  column can be written as $1$ times the second column plus $2$ times
  the third column plus $(-1)$ times the fourth column. The same
  coefficients can be used to write $\vect{u}_6$ as a linear
  combination of previous {\em non-redundant} columns, namely:
  \begin{equation*}
    \vect{u}_6 = 1\,\vect{u}_2 + 2\,\vect{u}_3 - 1\,\vect{u}_5.
  \end{equation*}
  Also, the entries in the fourth column are $1$ and $1$, which are
  the coefficients for writing $\vect{u}_4$ as a linear combination of
  previous non-redundant columns, namely:
  \begin{equation*}
    \vect{u}_4 = 1\,\vect{u}_2 + 1\,\vect{u}_3.
  \end{equation*}
  Finally, there are no non-zero entries in the first column. This
  means that $\vect{u}_1$ is the empty linear combination
  \begin{equation*}
    \vect{u}_1 = \vect{0}.
  \end{equation*}
\end{solution}

When we remove all the redundant vectors from a list of vectors, the
remaining vectors are linearly independent. Moreover, the span remains
unchanged when the redundant vectors are removed. This is the subject
of the following theorem.

\begin{theorem}{Removing redundant vectors}{linearly-independent-subset}
  Let $\vect{u}_1,\ldots,\vect{u}_k$ be a list of vectors, and suppose
  that after removing all the redundant vectors, the remaining vectors
  are $\vect{u}_{j_1},\ldots,\vect{u}_{j_\ell}$. Then
  $\vect{u}_{j_1},\ldots,\vect{u}_{j_\ell}$ are linearly independent
  and
  \begin{equation*}
    \sspan\set{\vect{u}_{j_1},\ldots,\vect{u}_{j_\ell}}
    =
    \sspan\set{\vect{u}_1,\ldots,\vect{u}_k}.    
  \end{equation*}
\end{theorem}

\begin{proof}
  Remove the redundant vectors one by one, from right to left. Each
  time a redundant vector is removed, the span does not change; the
  proof of this is similar to Example~\ref{exa:redundant-span}.
  Moreover, the resulting list of vectors
  $\sspan\set{\vect{u}_{j_1},\ldots,\vect{u}_{j_\ell}}$ is linearly
  independent, because if any of these vectors were a linear
  combination of earlier ones, then it would have been redundant in
  the original list of vectors, and would have therefore been removed.
\end{proof}

\begin{example}{Finding a linearly independent set of spanning vectors}{linearly-independent-subset}
  Find a linearly independent subset of
  $\set{\vect{u}_1,\ldots,\vect{u}_4}$ having the same span as
  $\set{\vect{u}_1,\ldots,\vect{u}_4}$.
  \begin{equation*}
    \vect{u}_1 = \begin{mymatrix}{r} 1 \\ 0 \\ -2 \\ 3 \end{mymatrix},
    \quad
    \vect{u}_2 = \begin{mymatrix}{r} -2 \\ 0 \\ 4 \\ -6 \end{mymatrix},
    \quad
    \vect{u}_3 = \begin{mymatrix}{r} 1 \\ 2 \\ 2 \\ 1 \end{mymatrix},
    \quad
    \vect{u}_4 = \begin{mymatrix}{r} 3 \\ 4 \\ 2 \\ 5 \end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  We use the casting-out algorithm to find the redundant vectors:
  \begin{equation*}
    \begin{mymatrix}{rrrr}
      1  & -2 & 1 & 3 \\
      0  &  0 & 2 & 4 \\
      -2 &  4 & 2 & 2 \\
      3  & -6 & 1 & 5 \\
    \end{mymatrix}
    \sim\ldots\sim
    \begin{mymatrix}{rrrr}
      \circled{1}  & -2 & 1 & 3 \\
      0  &  0 & \circled{2} & 4 \\
      0  &  0 & 0 & 0 \\
      0  &  0 & 0 & 0 \\
    \end{mymatrix}.
  \end{equation*}
  Therefore, the redundant vectors are $\vect{u}_2$ and
  $\vect{u}_4$. We remove them (``cast them out'') and are left with
  $\vect{u}_1$ and $\vect{u}_3$. Therefore, by
  Theorem~\ref{thm:linearly-independent-subset},
  $\set{\vect{u}_1,\vect{u}_3}$ is linearly independent and
  $\sspan\set{\vect{u}_1,\vect{u}_3} =
  \sspan\set{\vect{u}_1,\ldots,\vect{u}_4}$.
\end{solution}

Our definition of redundant vectors depends on the order in which the
vectors are written. This is because each redundant vector must be a
linear combination of {\em earlier} vectors in the list. For example,
in the list of vectors
\begin{equation*}
  \vect{u}=\begin{mymatrix}{r} 1 \\ 1 \\ 1 \end{mymatrix},\quad
  \vect{v}=\begin{mymatrix}{r} 3 \\ 2 \\ 1 \end{mymatrix},\quad
  \vect{w}=\begin{mymatrix}{r} 11 \\ 8 \\ 5 \end{mymatrix},
\end{equation*}
the vector $\vect{w}$ is redundant, because it is a linear combination
of earlier vectors $\vect{w} = 2\,\vect{u}+3\,\vect{v}$. Neither
$\vect{u}$ nor $\vect{v}$ are redundant. On the other hand, in the list
of vectors
\begin{equation*}
  \vect{u}=\begin{mymatrix}{r} 1 \\ 1 \\ 1 \end{mymatrix},\quad
  \vect{w}=\begin{mymatrix}{r} 11 \\ 8 \\ 5 \end{mymatrix},\quad
  \vect{v}=\begin{mymatrix}{r} 3 \\ 2 \\ 1 \end{mymatrix},
\end{equation*}
$\vect{v}$ is redundant because
$\vect{v} = \frac{1}{3}\vect{w} - \frac{2}{3}\vect{u}$, but neither
$\vect{u}$ nor $\vect{w}$ are redundant. Note that none of the vectors
have changed; only the order in which they are written is
different. Yet $\vect{w}$ is the redundant vector in the first list,
and $\vect{v}$ is the redundant vector in the second list.

Because linear independence was defined as the absence of redundant
vectors, you may suspect that the concept of linear independence also
depends on the order in which the vectors are written. However, this
is not the case. The following theorem gives an alternative
characterization of linear independence that is more symmetric (it
does not depend on the order of the vectors).

\begin{theorem}{Characterization of linear independence}{characterization-linear-independence}
  Let $\vect{u}_1,\ldots,\vect{u}_k$ be vectors. Then
  $\vect{u}_1,\ldots,\vect{u}_k$ are linearly independent%
  \index{linear independence}\index{vector!linearly independent} if
  and only if the homogeneous equation
  \begin{equation*}
    a_1\,\vect{u}_1 + \ldots + a_k\,\vect{u}_k = \vect{0}
  \end{equation*}
  has only the trivial solution%
  \index{trivial solution}\index{solution!trivial}%
  \index{system of linear equations!trivial solution}.
\end{theorem}

\begin{proof}
  Recall that the trivial solution is $a_1=0$, \ldots, $a_k=0$. First
  suppose that the equation
  $a_1\,\vect{u}_1 + \ldots + a_k\,\vect{u}_k = \vect{0}$ has a
  non-trivial solution. We will show that
  $\vect{u}_1,\ldots,\vect{u}_k$ are linearly dependent. Let
  $(a_1,\ldots,a_k)$ be a non-trivial solution. This means that not
  all $a_j$ are zero. Let $j$ be the largest index with $a_j\neq 0$.
  ...............  
\end{proof}



% ======================================================================
\subsection{CONTINUE HERE} %###

* standard characterization of linear independence

* properties of linear independence: size; subset; reordering

% ----------------------------------------------------------------------

We now turn our attention to the following question: what linear
combinations of a given set of vectors $\set{\vect{u}_1, \cdots
,\vect{u}_k}$ in $\R^{n}$ yields the zero vector? Clearly
$0\vect{u}_1 + 0\vect{u}_2+ \cdots + 0 \vect{u}_k = \vect{0}$, but is
it possible to have $\sum_{i=1}^{k}a_{i}\vect{u}_{i}=\vect{0}$ without
all coefficients being zero?

You can create examples where this easily happens. For example if $\vec{u}_1=\vect{u}_2$, then 
$1\vect{u}_1 - \vect{u}_2+ 0 \vect{u}_3 + \cdots  + 0 \vect{u}_k = \vect{0}$, no matter the vectors 
 $\set{\vect{u}_3, \cdots ,\vect{u}_k}$. 0But sometimes it can be more subtle. 

\begin{example}{Linearly dependent set of vectors}{linearly-dependent-vectors}
Consider the vectors 
\begin{equation*}
\vect{u}_1=\begin{mymatrix}{rrr}
0  & 1 & -2
\end{mymatrix}^T, 
\vect{u}_2=\begin{mymatrix}{rrr}
1  & 1 & 0
\end{mymatrix}^T, 
\vect{u}_3=\begin{mymatrix}{rrr}
-2  & 3 & 2
\end{mymatrix}^T, \mbox{ and } 
\vect{u}_4=\begin{mymatrix}{rrr}
1  & -2 & 0
\end{mymatrix}^T
\end{equation*}
in $\R^{3}$.

Then verify that 
\begin{equation*}
1\vect{u}_1 +0 \vect{u}_2+ - \vect{u}_3 -2 \vect{u}_4 = \vect{0}
\end{equation*}
\end{example}

You can see that the linear combination does yield the zero vector but
has some non-zero coefficients. Thus we define a set of vectors to be
{\em linearly dependent} if this happens.

\begin{definition}{Linearly dependent set of vectors}{linear-dependent}
A set of non-zero vectors $\set{\vect{u}_1, \cdots ,\vect{u}_k}$ in $\R^{n}$ is said to be 
\textbf{linearly dependent} if a linear combination of these vectors without all  coefficients being zero does yield the zero vector.
\index{linear dependence}\index{vector!linear dependent}
\end{definition}

Note that if $\sum_{i=1}^{k}a_{i}\vect{u}_{i}=\vect{0}$ and some
coefficient is non-zero, say $a_1 \neq 0$, then 
\begin{equation*}
\vec{u}_1 = \frac{-1}{a_1} \sum_{i=2}^{k}a_{i}\vect{u}_{i} 
\end{equation*}
and thus $\vec{u}_1$ is in the span of the other vectors. And the converse clearly works as well,
so we get that a set of vectors is linearly dependent precisely when
one of its vector is in the span of the other vectors of that set.

In particular, you can show that the vector $\vec{u}_1$ in the above
example is in the span of the vectors  $\set{\vect{u}_2, \vect{u}_3, \vect{u}_4}$. 

If a set of vectors is NOT linearly dependent, then it must be that
any linear combination of these vectors which yields the zero vector
must use all zero coefficients. This is a very important notion, and we give it its own name of {\em
linear independence}. 

\begin{definition}{Linearly independent set of vectors}{linear-independent}
A set of non-zero vectors $\set{\vect{u}_1, \cdots ,\vect{u}_k}$ in $\R^{n}$ is said to be 
\textbf{linearly independent} if whenever 
\begin{equation*}
\sum_{i=1}^{k}a_{i}\vect{u}_{i}=\vect{0}
\end{equation*}
it follows that each $a_{i}=0$.
\end{definition}

\index{linear independence}\index{vector!linear independent}

Note also that we require all vectors to be non-zero to form a
linearly independent set.

To view this in a more familiar setting, form the $n \times k$-matrix
$A$ having these vectors as columns. Then all we are saying is that
the set $\set{\vect{u}_1, \cdots ,\vect{u}_k}$ is linearly independent
precisely when $AX=0$ has only the trivial solution.

Here is an example.  

\begin{example}{Linearly independent vectors}{linearly-independent-vectors}
Consider the vectors $\vect{u}=\begin{mymatrix}{rrr}
1  & 1 & 0
\end{mymatrix}^T$, 
$\vect{v}=\begin{mymatrix}{rrr}
1  & 0 & 1
\end{mymatrix}^T$, and
$\vect{w}=\begin{mymatrix}{rrr}
0  & 1 & 1
\end{mymatrix}^T$ in $\R^{3}$.
Verify whether the set $\set{\vect{u}, \vect{v}, \vect{w}}$ is linearly independent. 
\end{example}

\begin{solution}
So suppose that we have a linear combinations $a\vect{u} + b \vect{v}
+ c\vect{w} = \vect{0}$. Then you can see that this can only happen
with $a=b=c=0$.

As mentioned above, you can equivalently form the $3 \times 3$-matrix $A = 
\begin{mymatrix}{ccc}
1  & 1 & 0 \\
1  & 0 & 1 \\
0  & 1 & 1 \\
\end{mymatrix}$, and show that $AX=0$ has only the trivial solution.

Thus this  means the set $\set{\vect{u}, \vect{v}, \vect{w} }$ is linearly independent. 
\end{solution}

In terms of spanning, a set of vectors is linearly independent if it
does not contain unnecessary vectors, that is not vector is in the span of the others.

Thus we put all this together in the following important theorem.

\begin{theorem}{Linear independence as a linear combination}{linear-independence-combination}
Let $\set{\vect{u}_{1},\cdots ,\vect{u}_{k}}$ be a collection of vectors  in
$\R^{n}$. Then the following are equivalent:

\begin{enumerate}
\item It is linearly independent, that is whenever
\begin{equation*}
\sum_{i=1}^{k}a_{i}\vect{u}_{i}=\vect{0}
\end{equation*}
it follows that each coefficient $a_{i}=0$.
\item No vector is in the span of the others.
\item The system of
linear equations $AX=0$ has only the trivial solution, where $A$ is
the $n \times k$-matrix having these vectors as columns. 
\end{enumerate}
\end{theorem}

The last sentence of this theorem is useful as it allows us to use the
{\rref} of a matrix to determine if a set of vectors is linearly
independent. Let the vectors be columns of a matrix $A$. Find the
{\rref} of $A$. If each column has a leading one, then it follows
that the vectors are linearly independent.

Sometimes we refer to the condition regarding sums as follows: The set
of vectors, $\set{\vect{u}_{1},\cdots ,\vect{u}_{k}} $ is
linearly independent if and only if there is no non-trivial linear
combination which equals the zero vector. A non-trivial linear
combination is one in which not all the scalars equal zero. Similarly,
a trivial linear combination is one in which all scalars equal zero.

Here is a detailed  example in  $\R^{4}$. 

\begin{example}{Linear independence}{linear-independence2}
Determine whether the set of vectors given by  
\[ \set{\begin{mymatrix}{r}
1 \\
2 \\
3 \\
0
\end{mymatrix}, \; \begin{mymatrix}{r}
2 \\
1 \\
0 \\
1
\end{mymatrix} , \; \begin{mymatrix}{r}
0 \\
1 \\
1 \\
2
\end{mymatrix}  , \; \begin{mymatrix}{r}
3 \\
2 \\
2 \\
0
\end{mymatrix} } \]
is linearly independent. If it is linearly dependent,
express one of the vectors as a linear combination of the others.
\end{example}

\begin{solution}
In this case the matrix of the corresponding homogeneous system of linear equations  is 
\begin{equation*}
\begin{mymatrix}{rrrr|r}
1 & 2 & 0 & 3 & 0\\ 
2 & 1 & 1 & 2 & 0 \\ 
3 & 0 & 1 & 2 & 0 \\ 
0 & 1 & 2 & 0 & 0 
\end{mymatrix}
\end{equation*}
The {\rref} is 
\begin{equation*}
\begin{mymatrix}{rrrr|r}
1 & 0 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 0 & 0 \\ 
0 & 0 & 1 & 0 & 0 \\ 
0 & 0 & 0 & 1 & 0 
\end{mymatrix}
\end{equation*}
and so every column is a pivot column and the corresponding system
$AX=0$ only has the trivial solution.  Therefore, these vectors are
linearly independent and there is no way to obtain one of the vectors
as a linear combination of the others.
\end{solution}

Consider another example.

\begin{example}{Linear independence}{linear-independence}
Determine whether the set of vectors given by  
\[\set{
\begin{mymatrix}{r}
1 \\
2 \\
3 \\
0
\end{mymatrix}, \; \begin{mymatrix}{r}
2 \\
1 \\
0 \\
1
\end{mymatrix}, \; \begin{mymatrix}{r}
0 \\
1 \\
1 \\
2
\end{mymatrix}, \; \begin{mymatrix}{r}
3 \\
2 \\
2 \\
-1
\end{mymatrix} } \]
is linearly independent. If it is linearly dependent,
express one of the vectors as a linear combination of the others.
\end{example}

\begin{solution}
Form the $4 \times 4$-matrix $A$ having these vectors as columns:
\begin{equation*}
A= \begin{mymatrix}{rrrr}
1 & 2 & 0 & 3 \\ 
2 & 1 & 1 & 2 \\ 
3 & 0 & 1 & 2 \\ 
0 & 1 & 2 & -1
\end{mymatrix}
\end{equation*}
Then by Theorem~\ref{thm:linear-independence-combination}, the given set of vectors is linearly independent
exactly if the system $AX=0$ has only the trivial solution.

The augmented matrix for this system and corresponding {\rref} are given by  
\begin{equation*}
 \begin{mymatrix}{rrrr|r}
1 & 2 & 0 & 3 & 0 \\ 
2 & 1 & 1 & 2 & 0 \\ 
3 & 0 & 1 & 2 & 0 \\ 
0 & 1 & 2 & -1 & 0 
\end{mymatrix}
\rightarrow \cdots \rightarrow
\begin{mymatrix}{rrrr|r}
1 & 0 & 0 & 1 & 0 \\ 
0 & 1 & 0 & 1 & 0 \\ 
0 & 0 & 1 & -1 & 0 \\ 
0 & 0 & 0 & 0 & 0 
\end{mymatrix} 
\end{equation*}
Not all the columns of the coefficient matrix are pivot columns and so the vectors are not linearly independent. In this case, we say the vectors are linearly dependent. 

It follows that there are infinitely many solutions to $AX=0$, one of which is
\begin{equation*}
\begin{mymatrix}{r}
1 \\ 
1 \\ 
-1 \\ 
-1
\end{mymatrix}
\end{equation*}
Therefore we can write 
\begin{equation*}
1\begin{mymatrix}{r}
1 \\ 
2 \\ 
3 \\ 
0
\end{mymatrix} +1\begin{mymatrix}{r}
2 \\ 
1 \\ 
0 \\ 
1
\end{mymatrix}  -1 \begin{mymatrix}{r}
0 \\ 
1 \\ 
1 \\ 
2
\end{mymatrix} -1 \begin{mymatrix}{r}
3 \\ 
2 \\ 
2 \\ 
-1
\end{mymatrix} =
\begin{mymatrix}{r}
0 \\ 
0 \\ 
0 \\ 
0
\end{mymatrix}
\end{equation*}

This can be rearranged as follows
\begin{equation*}
1\begin{mymatrix}{r}
1 \\ 
2 \\ 
3 \\ 
0
\end{mymatrix} +1\begin{mymatrix}{r}
2 \\ 
1 \\ 
0 \\ 
1
\end{mymatrix}  -1 \begin{mymatrix}{r}
0 \\ 
1 \\ 
1 \\ 
2
\end{mymatrix} =\begin{mymatrix}{r}
3 \\ 
2 \\ 
2 \\ 
-1
\end{mymatrix} 
\end{equation*}
This gives the last vector as a linear combination of the first three vectors.

Notice that we could rearrange this equation to write any of the four vectors as a linear combination of the other three. 
\end{solution}

When given a linearly independent set of vectors, we can determine if related sets are linearly independent. 

\begin{example}{Related sets of vectors}{related-linear-independence}
Let $\set{\vect{u},\vect{v},\vect{w}}$ be an independent set of $\R^n$.
Is $\set{\vect{u}+\vect{v}, 2\vect{u}+\vect{w}, \vect{v}-5\vect{w}}$ linearly
independent?
\end{example}

\begin{solution}
Suppose $a(\vect{u}+\vect{v}) + b(2\vect{u}+\vect{w}) + c(\vect{v}-5\vect{w})=\vect{0}_n$
for some $a,b,c\in\R$.
Then 
\[ (a+2b)\vect{u} + (a+c)\vect{v} + (b-5c)\vect{w}=\vect{0}_n.\]

Since $\set{\vect{u},\vect{v},\vect{w}}$ is independent, 
\begin{eqnarray*}
a + 2b & = & 0 \\
a + c & = & 0 \\
b - 5c & = & 0 
\end{eqnarray*}

This system of three equations in three variables has 
the unique solution $a=b=c=0$.
Therefore, $\set{\vect{u}+\vect{v}, 2\vect{u}+\vect{w}, \vect{v}-5\vect{w}}$ is independent.
\end{solution}

The following corollary follows from the fact that if the augmented matrix of a homogeneous
system of linear equations has more columns than rows, the system has infinitely many
solutions.

\begin{corollary}{Linear dependence in $\R^{n}$}{linear-dependence-Rn}
Let $\set{\vect{u}_{1},\cdots ,\vect{u}_{k}} $
be a set of vectors in $\R^{n}$. 
If $k>n$, then the set is linearly dependent (i.e. NOT linearly independent).
\end{corollary}

\begin{proof}
Form the $n \times k$-matrix $A$ having the vectors $\set{
\vect{u}_{1},\cdots ,\vect{u}_{k}} $ as its columns and suppose $k > n$. Then $A$ has rank $r \leq n <k$, so 
the system $AX=0$ has a non-trivial solution % by Theorem~\ref{thm:rank-homogeneous-solutions}
 and thus not linearly independent by Theorem~\ref{thm:linear-independence-combination}.
\end{proof}

\begin{example}{Linear dependence}{linear-dependence}
Consider the vectors 
\[
\set{\begin{mymatrix}{r}
1 \\
4 
\end{mymatrix}, 
\begin{mymatrix}{r}
2 \\
3
\end{mymatrix}, 
\begin{mymatrix}{r}
3 \\
2
\end{mymatrix} }
\]
Are these vectors linearly independent?
\end{example}

\begin{solution}
This set contains three vectors in $\R^2$. By Corollary~\ref{cor:linear-dependence-Rn} these vectors are linearly dependent.
In fact, we can write
\[
(-1) \begin{mymatrix}{r}
1 \\
4 
\end{mymatrix} + (2) 
\begin{mymatrix}{r}
2 \\
3
\end{mymatrix} = 
\begin{mymatrix}{r}
3 \\
2
\end{mymatrix}
\]
showing that this set is linearly dependent. 
\end{solution}

The third vector in the previous example is in the span of the first two vectors. We could find a way to write this vector as a linear combination of the other two vectors. It turns out that the linear combination which we found is the \textbf{only} one, provided that the set is linearly independent. 

\begin{theorem}{Unique linear combination}{unique-linear-combination}
Let $U \subseteq\R^n$ be an independent set.
Then any vector $\vect{x}\in\sspan(U)$ can be written uniquely as a linear combination of vectors of $U$.
\end{theorem}

\begin{proof}
To prove this theorem, we will show that two linear combinations of vectors in $U$ that equal $\vect{x}$ must be the same. Let $U =\set{\vect{u}_1, \vect{u}_2, \ldots, \vect{u}_k}$.
Suppose that there is a vector $\vect{x}\in \sspan(U)$ such that
\begin{eqnarray*}
\vect{x} & = & s_1\vect{u}_1 + s_2\vect{u}_2 + \cdots + s_k\vect{u}_k,
\mbox{ for some } s_1, s_2, \ldots, s_k\in\R, \mbox{ and} \\
\vect{x} & = & t_1\vect{u}_1 + t_2\vect{u}_2 + \cdots + t_k\vect{u}_k,
\mbox{ for some } t_1, t_2, \ldots, t_k\in\R.
\end{eqnarray*}
Then 
$\vect{0}_n=\vect{x}-\vect{x} = (s_1-t_1)\vect{u}_1 + (s_2-t_2)\vect{u}_2 + \cdots +
(s_k-t_k)\vect{u}_k$.

Since $U$ is independent, the only linear combination that vanishes
is the trivial one, so $s_i-t_i=0$ for all $i$, $1\leq i\leq k$.

Therefore, $s_i=t_i$ for all $i$, $1\leq i\leq k$, and the
representation is unique.Let $U \subseteq\R^n$ be an independent set.
Then any vector $\vect{x}\in\sspan(U)$ can be written uniquely as a linear combination of vectors of $U$.
\end{proof}

Suppose that $\vect{u},\vect{v}$ and $\vect{w}$ are non-zero vectors in $\R^3$,
and that $\set{\vect{v},\vect{w}}$ is independent. Consider the set $\set{\vect{u},\vect{v},\vect{w}}$. When can we know that this set is independent? It turns out that this follows exactly when $\vect{u}\not\in\sspan\set{\vect{v},\vect{w}}$.

\begin{example}{}{}
Suppose that $\vect{u},\vect{v}$ and $\vect{w}$ are non-zero vectors in $\R^3$,
and that $\set{\vect{v},\vect{w}}$ is independent.
Prove that $\set{\vect{u},\vect{v},\vect{w}}$ is independent if and only if 
$\vect{u}\not\in\sspan\set{\vect{v},\vect{w}}$.
\end{example}

\begin{solution}
If $\vect{u}\in\sspan\set{\vect{v},\vect{w}}$, then there exist $a,b\in\R$ so
that $\vect{u}=a\vect{v} + b\vect{w}$.
This implies that $\vect{u}-a\vect{v} - b\vect{w}=\vect{0}_3$,
so  $\vect{u}-a\vect{v} - b\vect{w}$
is a non-trivial linear combination of $\set{\vect{u},\vect{v},\vect{w}}$ that
vanishes, 
and thus $\set{\vect{u},\vect{v},\vect{w}}$ is dependent.

Now suppose that $\vect{u}\not\in\sspan\set{\vect{v},\vect{w}}$, and suppose
that there exist $a,b,c\in\R$ such that
$a\vect{u}+b\vect{v}+c\vect{w}=\vect{0}_3$.
If $a\neq 0$, then $\vect{u}=-\frac{b}{a}\vect{v}-\frac{c}{a}\vect{w}$,
and $\vect{u}\in\sspan\set{\vect{v},\vect{w}}$, a contradiction.
Therefore, $a=0$, implying that $b\vect{v}+c\vect{w}=\vect{0}_3$.
Since $\set{\vect{v},\vect{w}}$ is independent, $b=c=0$, and thus
$a=b=c=0$, i.e., the only linear combination of 
$\vect{u},\vect{v}$ and $\vect{w}$ that vanishes is the trivial one.

Therefore, $\set{\vect{u},\vect{v},\vect{w}}$ is independent.
\end{solution}

Consider the following useful theorem.

\begin{theorem}{Invertible matrices}{invertible-matrices}
Let $A$ be an invertible $n \times n$-matrix. Then the columns of $A$ are independent and span $\R^n$. Similarly, the rows of $A$ are independent and span the set of all $1 \times n$ vectors. 
\end{theorem}

This theorem also allows us to determine if a matrix is invertible. If an $n \times n$-matrix $A$ has columns which are independent, or span $\R^n$, then it follows that $A$ is invertible. If it has rows that are independent, or span the set of all $1 \times n$ vectors, then $A$ is invertible.
