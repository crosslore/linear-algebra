\section{Linear independence}

% ----------------------------------------------------------------------
\subsection{Redundant vectors and linear independence}

In Example~\ref{exa:redundant-span}, we encountered three vectors
$\vect{u}$, $\vect{v}$, and $\vect{w}$ such that
$\sspan\set{\vect{u},\vect{v},\vect{w}} =
\sspan\set{\vect{u},\vect{v}}$.  If this happens, then the vector
$\vect{w}$ does not contribute anything to the span of
$\set{\vect{u},\vect{v},\vect{w}}$, and we say that $\vect{w}$ is
\textbf{redundant}%
\index{redundant vector}\index{vector!redundant}. The following
definition generalizes this notion.

\begin{definition}{Redundant vectors, linear dependence, and linear independence}{redundant-vectors}
  Consider a sequence of $k$ vectors $\vect{u}_1,\ldots,\vect{u}_k$.  We
  say that the vector $\vect{u}_j$ is \textbf{redundant}%
  \index{redundant vector}\index{vector!redundant} if it can be
  written as a linear combination of earlier vectors in the sequence,
  i.e., if
  \begin{equation*}
    \vect{u}_j = a_1\,\vect{u}_1 + a_2\,\vect{u}_2 + \ldots + a_{j-1}\,\vect{u}_{j-1}
  \end{equation*}
  for some scalars $a_1,\ldots,a_{j-1}$. We say that the sequence of
  vectors $\vect{u}_1,\ldots,\vect{u}_k$ is \textbf{linearly
    dependent}%
  \index{linear dependence}\index{vector!linearly dependent} if it
  contains one or more redundant vectors. Otherwise, we say that the
  vectors are \textbf{linearly independent}%
  \index{linear independence}\index{vector!linearly independent}.  
\end{definition}

\begin{example}{Redundant vectors}{redundant-vectors}
  Find the redundant vectors in the following sequence of vectors. Are
  the vectors linearly independent?
  \begin{equation*}
    \vect{u}_1 = \begin{mymatrix}{c} 0 \\ 0 \\ 0 \\ 0\end{mymatrix},
    \quad
    \vect{u}_2 = \begin{mymatrix}{c} 1 \\ 2 \\ 2 \\ 3\end{mymatrix},
    \quad
    \vect{u}_3 = \begin{mymatrix}{c} 1 \\ 1 \\ 1 \\ 1\end{mymatrix},
    \quad
    \vect{u}_4 = \begin{mymatrix}{c} 2 \\ 3 \\ 3 \\ 4\end{mymatrix},
    \quad
    \vect{u}_5 = \begin{mymatrix}{c} 0 \\ 1 \\ 2 \\ 3\end{mymatrix},
    \quad
    \vect{u}_6 = \begin{mymatrix}{c} 3 \\ 3 \\ 2 \\ 2\end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  \begin{itemize}
  \item The vector $\vect{u}_1$ is redundant, because it is a linear
    combination of earlier vectors. (Although there are no earlier
    vectors, recall from Example~\ref{exa:span-empty-set} that the empty
    sum of vectors is equal to the zero vector $\vect{0}$. Therefore,
    $\vect{u}_1$ is indeed an (empty) linear combination of earlier
    vectors.)
  \item The vector $\vect{u}_2$ is not redundant, because it cannot be
    written as a linear combination of $\vect{u}_1$. This is because
    the system of equations
    \begin{equation*}
      \begin{mymatrix}{l|l}
        0 & 1 \\
        0 & 2 \\
        0 & 2 \\
        0 & 3 \\
      \end{mymatrix}
    \end{equation*}
    has no solution.
  \item The vector $\vect{u}_3$ is not redundant, because it cannot be
    written as a linear combination of $\vect{u}_1$ an $\vect{u}_2$.
    This is because the system of equations
    \begin{equation*}
      \begin{mymatrix}{ll|l}
        0 & 1 & 1 \\
        0 & 2 & 1\\
        0 & 2 & 1\\
        0 & 3 & 1\\
      \end{mymatrix}
    \end{equation*}
    has no solution.
  \item The vector $\vect{u}_4$ is redundant, because $\vect{u}_4 =
    \vect{u}_2 + \vect{u}_3$.
  \item The vector $\vect{u}_5$ is not redundant, because 
    This is because the system of equations
    \begin{equation*}
      \begin{mymatrix}{llll|l}
        0 & 1 & 1 & 2 & 0 \\
        0 & 2 & 1 & 3 & 1 \\
        0 & 2 & 1 & 3 & 2 \\
        0 & 3 & 1 & 4 & 3 \\
      \end{mymatrix}
    \end{equation*}
    has no solution.
  \item The vector $\vect{u}_6$ is redundant, because $\vect{u}_6 =
    \vect{u}_2 + 2\vect{u}_3-\vect{u}_5$.
  \end{itemize}
  In summary, the vectors $\vect{u}_1$, $\vect{u}_4$, and $\vect{u}_6$
  are redundant, and the vectors $\vect{u}_2$, $\vect{u}_3$, and
  $\vect{u}_5$ are not. It follows that the vectors
  $\vect{u}_1,\ldots,\vect{u}_6$ are linearly dependent.
\end{solution}

% ----------------------------------------------------------------------
\subsection{The casting-out algorithm}

The last example shows that it can be a lot of work to find the
redundant vectors in a sequence of $k$ vectors. Doing so in the naive way
require us to solve up to $k$ systems of linear equations!
Fortunately, there is a much faster and easier method, the so-called
{\em casting-out algorithm}.

\begin{algorithm}{Casting-out algorithm}{casting-out}
  \textbf{Input:} a list of $k$ vectors
  $\vect{u}_1,\ldots,\vect{u}_k\in\R^n$.
  \smallskip

  \textbf{Output:} the set of indices $j$ such that $\vect{u}_j$ is
  redundant.
  \smallskip
  
  \textbf{Algorithm:} Write the vectors $\vect{u}_1,\ldots,\vect{u}_k$
  as the columns of an $n\times k$-matrix, and reduce to {\ef}. Every
  {\em non-pivot} column, if any, corresponds to a redundant vector.
\end{algorithm}

\begin{example}{Casting-out algorithm}{casting-out}
  Use the casting-out algorithm to find the redundant vectors among
  the vectors from Example~\ref{exa:redundant-vectors}.
\end{example}

\begin{solution}
  Following the casting-out algorithm, we write the vectors
  $\vect{u}_1,\ldots,\vect{u}_6$ as the columns of a matrix and reduce
  to {\ef}.
  \begin{equation*}
    \begin{mymatrix}{rrrrrr}
      0 & 1 & 1 & 2 & 0 & 3 \\
      0 & 2 & 1 & 3 & 1 & 3 \\
      0 & 2 & 1 & 3 & 2 & 2 \\
      0 & 3 & 1 & 4 & 3 & 2 \\
    \end{mymatrix}
    \sim \ldots \sim
    \begin{mymatrix}{rrrrrr}
      0 & \circled{1} & 1 & 2 & 0 & 3 \\
      0 & 0 & \circled{1} & 1 & -1 & 3 \\
      0 & 0 & 0 & 0 & \circled{1} & -1 \\
      0 & 0 & 0 & 0 & 0 & 0 \\
    \end{mymatrix}.
  \end{equation*}
  The pivot columns are columns $2$, $3$, and $5$. The non-pivot
  columns are columns $1$, $4$, and $6$. Therefore, the vectors
  $\vect{u}_1$, $\vect{u}_4$, and $\vect{u}_6$ are redundant. Note
  that this is the same answer we got in
  Example~\ref{exa:redundant-vectors}.
\end{solution}

The above version of the casting-out algorithm only tells us which of
the vectors (if any) are redundant, but it does not give us a specific
way to write the redundant vectors as linear combinations of previous
vectors. However, we can easily get this additional information if we
reduce the matrix all the way to {\rref}. We call this version of the
algorithm the {\em extended casting-out algorithm}.

\begin{algorithm}{Extended casting-out algorithm}{extended-casting-out}
  \textbf{Input:} a list of $k$ vectors
  $\vect{u}_1,\ldots,\vect{u}_k\in\R^n$.
  \smallskip

  \textbf{Output:} the set of indices $j$ such that $\vect{u}_j$ is
  redundant, and a set of coefficients for writing each redundant
  vector as a linear combination of previous vectors.
  \smallskip
  
  \textbf{Algorithm:} Write the vectors $\vect{u}_1,\ldots,\vect{u}_k$
  as the columns of an $n\times k$-matrix, and reduce to
  {\rref}. Every {\em non-pivot} column, if any, corresponds to a
  redundant vector. If $\vect{u}_j$ is a redundant vector, then the
  entries in the $j\th$ column of the {\rref} are coefficients for
  writing $\vect{u}_j$ as a linear combination of previous
  non-redundant vectors.
\end{algorithm}

\begin{example}{Extended casting-out algorithm}{extended-casting-out}
  Use the casting-out algorithm to find the redundant vectors among
  the vectors from Example~\ref{exa:redundant-vectors}, and write each
  redundant vector as a linear combination of previous non-redundant
  vectors.
\end{example}

\begin{solution}
  Once again, we write the vectors $\vect{u}_1,\ldots,\vect{u}_6$ as
  the columns of a matrix. This time we use the extended casting-out
  algorithm, which means we reduce the matrix to {\rref} instead of
  {\ef}.
  \begin{equation*}
    \begin{mymatrix}{rrrrrr}
      0 & 1 & 1 & 2 & 0 & 3 \\
      0 & 2 & 1 & 3 & 1 & 3 \\
      0 & 2 & 1 & 3 & 2 & 2 \\
      0 & 3 & 1 & 4 & 3 & 2 \\
    \end{mymatrix}
    \sim \ldots \sim
    \begin{mymatrix}{rrrrrr}
      0 & \circled{1} & 0 & 1 & 0 & 1 \\
      0 & 0 & \circled{1} & 1 & 0 & 2 \\
      0 & 0 & 0 & 0 & \circled{1} & -1 \\
      0 & 0 & 0 & 0 & 0 & 0 \\
    \end{mymatrix}.
  \end{equation*}
  As before, the non-pivot columns are columns $1$, $4$, and $6$, and
  therefore, the vectors $\vect{u}_1$, $\vect{u}_4$, and $\vect{u}_6$
  are redundant. The non-redundant vectors are $\vect{u}_2$,
  $\vect{u}_3$, and $\vect{u}_5$. Moreover, the entries in the sixth
  column are $1$, $2$, and $-1$.  Note that this means that the sixth
  column can be written as $1$ times the second column plus $2$ times
  the third column plus $(-1)$ times the fourth column. The same
  coefficients can be used to write $\vect{u}_6$ as a linear
  combination of previous {\em non-redundant} columns, namely:
  \begin{equation*}
    \vect{u}_6 = 1\,\vect{u}_2 + 2\,\vect{u}_3 - 1\,\vect{u}_5.
  \end{equation*}
  Also, the entries in the fourth column are $1$ and $1$, which are
  the coefficients for writing $\vect{u}_4$ as a linear combination of
  previous non-redundant columns, namely:
  \begin{equation*}
    \vect{u}_4 = 1\,\vect{u}_2 + 1\,\vect{u}_3.
  \end{equation*}
  Finally, there are no non-zero entries in the first column. This
  means that $\vect{u}_1$ is the empty linear combination
  \begin{equation*}
    \vect{u}_1 = \vect{0}.
  \end{equation*}
\end{solution}

% ----------------------------------------------------------------------
\subsection{Removing redundant vectors}

When we remove all the redundant vectors from a sequence of vectors,
the remaining vectors are linearly independent. Moreover, the span
remains unchanged when the redundant vectors are removed. This is the
subject of the following theorem.

\begin{theorem}{Removing redundant vectors}{linearly-independent-subset}
  Let $\vect{u}_1,\ldots,\vect{u}_k$ be a sequence of vectors, and
  suppose that after removing all the redundant vectors, the remaining
  vectors are $\vect{u}_{j_1},\ldots,\vect{u}_{j_\ell}$. Then
  $\vect{u}_{j_1},\ldots,\vect{u}_{j_\ell}$ are linearly independent
  and
  \begin{equation*}
    \sspan\set{\vect{u}_{j_1},\ldots,\vect{u}_{j_\ell}}
    =
    \sspan\set{\vect{u}_1,\ldots,\vect{u}_k}.    
  \end{equation*}
\end{theorem}

\begin{proof}
  Remove the redundant vectors one by one, from right to left. Each
  time a redundant vector is removed, the span does not change; the
  proof of this is similar to Example~\ref{exa:redundant-span}.
  Moreover, the resulting sequence of vectors
  $\sspan\set{\vect{u}_{j_1},\ldots,\vect{u}_{j_\ell}}$ is linearly
  independent, because if any of these vectors were a linear
  combination of earlier ones, then it would have been redundant in
  the original sequence of vectors, and would have therefore been removed.
\end{proof}

\begin{example}{Finding a linearly independent set of spanning vectors}{linearly-independent-subset}
  Find a linearly independent subset of
  $\set{\vect{u}_1,\ldots,\vect{u}_4}$ having the same span as
  $\set{\vect{u}_1,\ldots,\vect{u}_4}$.
  \begin{equation*}
    \vect{u}_1 = \begin{mymatrix}{r} 1 \\ 0 \\ -2 \\ 3 \end{mymatrix},
    \quad
    \vect{u}_2 = \begin{mymatrix}{r} -2 \\ 0 \\ 4 \\ -6 \end{mymatrix},
    \quad
    \vect{u}_3 = \begin{mymatrix}{r} 1 \\ 2 \\ 2 \\ 1 \end{mymatrix},
    \quad
    \vect{u}_4 = \begin{mymatrix}{r} 3 \\ 4 \\ 2 \\ 5 \end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  We use the casting-out algorithm to find the redundant vectors:
  \begin{equation*}
    \begin{mymatrix}{rrrr}
      1  & -2 & 1 & 3 \\
      0  &  0 & 2 & 4 \\
      -2 &  4 & 2 & 2 \\
      3  & -6 & 1 & 5 \\
    \end{mymatrix}
    \sim\ldots\sim
    \begin{mymatrix}{rrrr}
      \circled{1}  & -2 & 1 & 3 \\
      0  &  0 & \circled{2} & 4 \\
      0  &  0 & 0 & 0 \\
      0  &  0 & 0 & 0 \\
    \end{mymatrix}.
  \end{equation*}
  Therefore, the redundant vectors are $\vect{u}_2$ and
  $\vect{u}_4$. We remove them (``cast them out'') and are left with
  $\vect{u}_1$ and $\vect{u}_3$. Therefore, by
  Theorem~\ref{thm:linearly-independent-subset},
  $\set{\vect{u}_1,\vect{u}_3}$ is linearly independent and
  $\sspan\set{\vect{u}_1,\vect{u}_3} =
  \sspan\set{\vect{u}_1,\ldots,\vect{u}_4}$.
\end{solution}

Our definition of redundant vectors depends on the order in which the
vectors are written. This is because each redundant vector must be a
linear combination of {\em earlier} vectors in the sequence. For example,
in the sequence of vectors
\begin{equation*}
  \vect{u}=\begin{mymatrix}{r} 1 \\ 1 \\ 1 \end{mymatrix},\quad
  \vect{v}=\begin{mymatrix}{r} 3 \\ 2 \\ 1 \end{mymatrix},\quad
  \vect{w}=\begin{mymatrix}{r} 11 \\ 8 \\ 5 \end{mymatrix},
\end{equation*}
the vector $\vect{w}$ is redundant, because it is a linear combination
of earlier vectors $\vect{w} = 2\,\vect{u}+3\,\vect{v}$. Neither
$\vect{u}$ nor $\vect{v}$ are redundant. On the other hand, in the sequence
of vectors
\begin{equation*}
  \vect{u}=\begin{mymatrix}{r} 1 \\ 1 \\ 1 \end{mymatrix},\quad
  \vect{w}=\begin{mymatrix}{r} 11 \\ 8 \\ 5 \end{mymatrix},\quad
  \vect{v}=\begin{mymatrix}{r} 3 \\ 2 \\ 1 \end{mymatrix},
\end{equation*}
$\vect{v}$ is redundant because
$\vect{v} = \frac{1}{3}\vect{w} - \frac{2}{3}\vect{u}$, but neither
$\vect{u}$ nor $\vect{w}$ are redundant. Note that none of the vectors
have changed; only the order in which they are written is
different. Yet $\vect{w}$ is the redundant vector in the first sequence,
and $\vect{v}$ is the redundant vector in the second sequence.

% ----------------------------------------------------------------------
\subsection{Alternative characterization of linear independence}

Because linear independence was defined as the absence of redundant
vectors, you may suspect that the concept of linear independence also
depends on the order in which the vectors are written. However, this
is not the case. The following theorem gives an alternative
characterization of linear independence that is more symmetric (it
does not depend on the order of the vectors).

\begin{theorem}{Characterization of linear independence}{characterization-linear-independence}
  Let $\vect{u}_1,\ldots,\vect{u}_k$ be vectors. Then
  $\vect{u}_1,\ldots,\vect{u}_k$ are linearly independent%
  \index{linear independence}\index{vector!linearly independent} if
  and only if the homogeneous equation
  \begin{equation*}
    a_1\,\vect{u}_1 + \ldots + a_k\,\vect{u}_k = \vect{0}
  \end{equation*}
  has only the trivial solution%
  \index{trivial solution}\index{solution!trivial}%
  \index{system of linear equations!trivial solution}.
\end{theorem}

\begin{proof}
  Let $A$ be the $n\times k$-matrix whose columns are
  $\vect{u}_1,\ldots,\vect{u}_k$. We know from the theory of
  homogeneous systems that the system
  $a_1\,\vect{u}_1 + \ldots + a_k\,\vect{u}_k = \vect{0}$ has no
  non-trivial solution if and only if every column of the {\ef} of $A$
  is a pivot column. By the casting-out algorithm, this is the case if
  and only if none of the vectors $\vect{u}_1,\ldots,\vect{u}_k$ are
  redundant, i.e., if and only if the vectors are linearly independent.
\end{proof}

\begin{example}{Characterization of linear independence}{characterization-linear-independence}
  Use the method of
  Theorem~\ref{thm:characterization-linear-independence} to determine
  whether the following vectors are linearly independent in $\R^4$.
  \begin{equation*}
    \vect{u}_1 = \begin{mymatrix}{r} 1 \\ 1 \\ 2 \\ 0 \end{mymatrix},\quad
    \vect{u}_2 = \begin{mymatrix}{r} 0 \\ 1 \\ 1 \\ 1 \end{mymatrix},\quad
    \vect{u}_3 = \begin{mymatrix}{r} 1 \\ 2 \\ 3 \\ 2 \end{mymatrix},\quad
    \vect{u}_4 = \begin{mymatrix}{r} 2 \\ 3 \\ 3 \\ 1 \end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  We must check whether the equation
  \begin{equation*}
    a_1 \begin{mymatrix}{r} 1 \\ 1 \\ 2 \\ 0 \end{mymatrix}
    + a_2 \begin{mymatrix}{r} 0 \\ 1 \\ 1 \\ 1 \end{mymatrix}
    + a_3 \begin{mymatrix}{r} 1 \\ 2 \\ 3 \\ 2 \end{mymatrix}
    + a_4 \begin{mymatrix}{r} 2 \\ 3 \\ 3 \\ 1 \end{mymatrix}
    = \begin{mymatrix}{r} 0 \\ 0 \\ 0 \\ 0 \end{mymatrix}
  \end{equation*}
  has a non-trivial solution. If it does, the vectors are linearly
  dependent. On the other hand, if there is only the trivial solution,
  the vectors are linearly independent. We write the augmented matrix
  and solve:
  \begin{equation*}
    \begin{mymatrix}{rrrr|r}
      1 & 0 & 1 & 2 & 0 \\
      1 & 1 & 2 & 3 & 0 \\
      2 & 1 & 3 & 7 & 0 \\
      0 & 1 & 2 & 1 & 0 \\
    \end{mymatrix}
    \sim
    \ldots
    \sim
    \begin{mymatrix}{rrrr|r}
      \circled{1} & 0 & 1 & 2 & 0 \\
      0 & \circled{1} & 1 & 1 & 0 \\
      0 & 0 & \circled{1} & 0 & 0 \\
      0 & 0 & 0 & \circled{2} & 0 \\
    \end{mymatrix}.
  \end{equation*}
  Since every column is a pivot column, there are no free variables;
  the system of equations has a unique solution, which is
  $a_1=a_2=a_3=a_4=0$, i.e., the trivial solution. Therefore, the
  vectors $\vect{u}_1,\ldots,\vect{u}_4$ are linearly independent.
\end{solution}

\begin{example}{Characterization of linear independence}{characterization-linear-independence2}
  Use the method of
  Theorem~\ref{thm:characterization-linear-independence} to determine
  whether the following vectors are linearly independent in $\R^4$.
  \begin{equation*}
    \vect{u}_1 = \begin{mymatrix}{r} 1 \\ 3 \\ 1 \\ 1 \end{mymatrix},\quad
    \vect{u}_2 = \begin{mymatrix}{r} 1 \\ 1 \\ 0 \\ 1 \end{mymatrix},\quad
    \vect{u}_3 = \begin{mymatrix}{r} 2 \\ 2 \\ 1 \\ 2 \end{mymatrix},\quad
    \vect{u}_4 = \begin{mymatrix}{r} 3 \\ 1 \\ 0 \\ 3 \end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  Once again, we must check whether the equation
  $a_1\,\vect{u}_1 + a_2\,\vect{u}_2 + a_3\,\vect{u}_3 +
  a_4\,\vect{u}_4 = \vect{0}$ has a non-trivial solution. As before,
  we write the augmented matrix and solve:
  \begin{equation*}
    \begin{mymatrix}{rrrr|r}
      1 & 1 & 2 & 3 & 0 \\
      3 & 1 & 2 & 1 & 0 \\
      1 & 0 & 1 & 0 & 0 \\
      1 & 1 & 2 & 3 & 0 \\
    \end{mymatrix}
    \sim
    \ldots
    \sim
    \begin{mymatrix}{rrrr|r}
      \circled{1} & 1 & 2 & 3 & 0 \\
      0 & \circled{1} & 2 & 4 & 0 \\
      0 & 0 & \circled{1} & 1 & 0 \\
      0 & 0 & 0 & 0 & 0 \\
    \end{mymatrix}.
  \end{equation*}
  Since column 4 is not a pivot column, $a_4$ is a free
  variable. Therefore, the system has a non-trivial solution, and the
  vectors are linearly dependent.

  With a small amount of extra work, we can find an actual non-trivial
  solution of
  $a_1\,\vect{u}_1 + a_2\,\vect{u}_2 + a_3\,\vect{u}_3 +
  a_4\,\vect{u}_4 = \vect{0}$. All we have to do is set $a_4=1$ and do
  a back substitution. We find that $(a_1,a_2,a_3,a_4)=(1,-2,-1,1)$ is
  a solution. In other words,
  \begin{equation*}
    \vect{u}_1 - 2\vect{u}_2 - \vect{u}_3 + \vect{u}_4 = \vect{0}.
  \end{equation*}
  We can also use this information to write $\vect{u}_4$ as a linear
  combination of previous vectors, namely,
  $\vect{u}_4 = -\vect{u}_1 + 2\vect{u}_2 + \vect{u}_3$.
\end{solution}

The characterization of linear independence in
Theorem~\ref{thm:characterization-linear-independence} is mostly
useful for theoretical reasons. However, it can also help in solving
problems such as the following.

\begin{example}{Related sets of vectors}{related-linear-independence}
  Let $\vect{u},\vect{v},\vect{w}$ be linearly independent
  vectors in $\R^n$. Are the vectors
  $\vect{u}+\vect{v}$, $2\vect{u}+\vect{w}$, and $\vect{v}-5\vect{w}$
  linearly independent?
\end{example}

\begin{solution}
  By Theorem~\ref{thm:characterization-linear-independence}, to check
  whether the vectors are linearly independent, we must check whether
  the equation
  \begin{equation}\label{eqn:related-linear-independence1}
    a(\vect{u}+\vect{v}) + b(2\vect{u}+\vect{w}) +
    c(\vect{v}-5\vect{w})=\vect{0}
  \end{equation}
  has non-trivial solutions.  If it does, the vectors are linearly
  dependent, if it does not, they are linearly independent. We can
  simplify the equation as follows:
  \begin{equation}\label{eqn:related-linear-independence2}
    (a+2b)\vect{u} + (a+c)\vect{v} + (b-5c)\vect{w}=\vect{0}.
  \end{equation}
  Since $\vect{u}$, $\vect{v}$, and $\vect{w}$ are linearly
  independent, we know, again by
  Theorem~\ref{thm:characterization-linear-independence}, that
  equation {\eqref{eqn:related-linear-independence2}} only has the
  trivial solution. Therefore,
  \begin{eqnarray*}
    a + 2b & = & 0, \\
    a + c & = & 0, \\
    b - 5c & = & 0. 
  \end{eqnarray*}
  We can solve this system of three equations in three variables, and
  we find that it has the unique solution $a=b=c=0$. Therefore,
  $a=b=c=0$ is the only solution to equation
  {\eqref{eqn:related-linear-independence1}}, which means that the
  vectors $\vect{u}+\vect{v}$, $2\vect{u}+\vect{w}$, and
  $\vect{v}-5\vect{w}$ are linearly independent.
\end{solution}

% ----------------------------------------------------------------------
\subsection{Properties of linear independence}


* properties of linear independence: size; subset; reordering


% ======================================================================
\subsection{CONTINUE HERE} %###

The following corollary follows from the fact that if the augmented
matrix of a homogeneous system of linear equations has more columns
than rows, the system has infinitely many solutions.

\begin{corollary}{Linear dependence in $\R^{n}$}{linear-dependence-Rn}
  Let $\set{\vect{u}_{1},\cdots ,\vect{u}_{k}} $ be a set of vectors
  in $\R^{n}$.  If $k>n$, then the set is linearly dependent (i.e. NOT
  linearly independent).
\end{corollary}

\begin{proof}
  Form the $n \times k$-matrix $A$ having the vectors
  $\set{ \vect{u}_{1},\cdots ,\vect{u}_{k}} $ as its columns and
  suppose $k > n$. Then $A$ has rank $r \leq n <k$, so the system
  $AX=0$ has a non-trivial
  solution % by Theorem~\ref{thm:rank-homogeneous-solutions}
  and thus not linearly independent by
  Theorem~\ref{thm:linear-independence-combination}.
\end{proof}

\begin{example}{Linear dependence}{linear-dependence}
  Consider the vectors 
  \[
    \set{\begin{mymatrix}{r}
        1 \\
        4 
      \end{mymatrix}, 
      \begin{mymatrix}{r}
        2 \\
        3
      \end{mymatrix}, 
      \begin{mymatrix}{r}
        3 \\
        2
      \end{mymatrix} }
  \]
  Are these vectors linearly independent?
\end{example}

\begin{solution}
  This set contains three vectors in $\R^2$. By
  Corollary~\ref{cor:linear-dependence-Rn} these vectors are linearly
  dependent.  In fact, we can write
  \[
    (-1) \begin{mymatrix}{r}
      1 \\
      4 
    \end{mymatrix} + (2) 
    \begin{mymatrix}{r}
      2 \\
      3
    \end{mymatrix} = 
    \begin{mymatrix}{r}
      3 \\
      2
    \end{mymatrix}
  \]
  showing that this set is linearly dependent. 
\end{solution}

The third vector in the previous example is in the span of the first
two vectors. We could find a way to write this vector as a linear
combination of the other two vectors. It turns out that the linear
combination which we found is the \textbf{only} one, provided that the
set is linearly independent.

\begin{theorem}{Unique linear combination}{unique-linear-combination}
  Let $U \subseteq\R^n$ be an independent set.  Then any vector
  $\vect{x}\in\sspan(U)$ can be written uniquely as a linear
  combination of vectors of $U$.
\end{theorem}

\begin{proof}
  To prove this theorem, we will show that two linear combinations of vectors in $U$ that equal $\vect{x}$ must be the same. Let $U =\set{\vect{u}_1, \vect{u}_2, \ldots, \vect{u}_k}$.
  Suppose that there is a vector $\vect{x}\in \sspan(U)$ such that
  \begin{eqnarray*}
    \vect{x}
    &=& s_1\vect{u}_1 + s_2\vect{u}_2 + \cdots + s_k\vect{u}_k,
        \mbox{ for some } s_1, s_2, \ldots, s_k\in\R, \mbox{ and} \\
    \vect{x}
    &=& t_1\vect{u}_1 + t_2\vect{u}_2 + \cdots + t_k\vect{u}_k,
        \mbox{ for some } t_1, t_2, \ldots, t_k\in\R.
  \end{eqnarray*}
  Then
  $\vect{0}_n=\vect{x}-\vect{x} = (s_1-t_1)\vect{u}_1 +
  (s_2-t_2)\vect{u}_2 + \cdots + (s_k-t_k)\vect{u}_k$.

  Since $U$ is independent, the only linear combination that vanishes is
  the trivial one, so $s_i-t_i=0$ for all $i$, $1\leq i\leq k$.

  Therefore, $s_i=t_i$ for all $i$, $1\leq i\leq k$, and the
  representation is unique.Let $U \subseteq\R^n$ be an independent set.
  Then any vector $\vect{x}\in\sspan(U)$ can be written uniquely as a
  linear combination of vectors of $U$.
\end{proof}

\begin{theorem}{Invertible matrices}{invertible-matrices}
  Let $A$ be an invertible $n \times n$-matrix. Then the columns of
  $A$ are independent and span $\R^n$. Similarly, the rows of $A$ are
  independent and span the set of all $1 \times n$ vectors.
\end{theorem}

This theorem also allows us to determine if a matrix is invertible. If
an $n \times n$-matrix $A$ has columns which are independent, or span
$\R^n$, then it follows that $A$ is invertible. If it has rows that
are independent, or span the set of all $1 \times n$ vectors, then $A$
is invertible.
