\section{Some proofs}

In this section, we prove Theorems~\ref{thm:well-defined-determinant}
and {\ref{thm:determinant-row-operations}}. This section can be
skipped by readers who are not interested in the proofs.

% ----------------------------------------------------------------------
\subsection{Proof of Theorem~\ref{thm:well-defined-determinant}}

\begin{lemma}{Expanding along any column}{well-defined-column-cofactor}
  Let $A$ be an $n\times n$-matrix. Expanding along any two columns
  gives the same answer. 
\end{lemma}

\begin{proof}
  The result is proved by induction on $n$. This means, when we prove
  the result for $n\times n$-matrices, we already assume it is true
  for $(n-1)\times(n-1)$-matrices. We only sketch the proof in the
  case where $n=4$. We also only show that expanding along the first
  and second columns gives the same result. The proof for general $n$
  and for two arbitrary columns $i$ and $j$ is very similar.

  Expanding $A$ along the first column, we get
  \begin{equation}\label{eqn:well-defined-column-cofactor1}
    a_{11}\begin{absmatrix}{ccc}
      a_{22} & a_{23} & a_{24} \\
      a_{32} & a_{33} & a_{34} \\
      a_{42} & a_{43} & a_{44} \\
    \end{absmatrix}
    - a_{21}\begin{absmatrix}{ccc}
      a_{12} & a_{13} & a_{14} \\
      a_{32} & a_{33} & a_{34} \\
      a_{42} & a_{43} & a_{44} \\
    \end{absmatrix}
    + a_{31}\begin{absmatrix}{ccc}
      a_{12} & a_{13} & a_{14} \\
      a_{22} & a_{23} & a_{24} \\
      a_{42} & a_{43} & a_{44} \\
    \end{absmatrix}
    - a_{41}\begin{absmatrix}{ccc}
      a_{12} & a_{13} & a_{14} \\
      a_{22} & a_{23} & a_{24} \\
      a_{32} & a_{33} & a_{34} \\
    \end{absmatrix}.
  \end{equation}
  Expanding $A$ along the second column, we get
  \begin{equation}\label{eqn:well-defined-column-cofactor2}
    - a_{12}\begin{absmatrix}{ccc}
      a_{21} & a_{23} & a_{24} \\
      a_{31} & a_{33} & a_{34} \\
      a_{41} & a_{43} & a_{44} \\
    \end{absmatrix}
    + a_{22}\begin{absmatrix}{ccc}
      a_{11} & a_{13} & a_{14} \\
      a_{31} & a_{33} & a_{34} \\
      a_{41} & a_{43} & a_{44} \\
    \end{absmatrix}
    - a_{32}\begin{absmatrix}{ccc}
      a_{11} & a_{13} & a_{14} \\
      a_{21} & a_{23} & a_{24} \\
      a_{41} & a_{43} & a_{44} \\
    \end{absmatrix}
    + a_{42}\begin{absmatrix}{ccc}
      a_{11} & a_{13} & a_{14} \\
      a_{21} & a_{23} & a_{24} \\
      a_{31} & a_{33} & a_{34} \\
    \end{absmatrix}.
  \end{equation}
  To show these two expressions are equal, we expand each of the
  $3\times 3$-matrices along the first column. Expanding the matrices
  in {\eqref{eqn:well-defined-column-cofactor1}}, we get
  \begin{eqnarray*}
    &&
       a_{11}\paren{
       a_{22} \begin{absmatrix}{cc}
         a_{33} & a_{34} \\
         a_{43} & a_{44} \\
       \end{absmatrix}
    -a_{32} \begin{absmatrix}{cc}
      a_{23} & a_{24} \\
      a_{43} & a_{44} \\
    \end{absmatrix}
    +a_{42} \begin{absmatrix}{cc}
      a_{23} & a_{24} \\
      a_{33} & a_{34} \\
    \end{absmatrix}}
    \\
    && 
       -a_{21}\paren{
       a_{12} \begin{absmatrix}{cc}
         a_{33} & a_{34} \\
         a_{43} & a_{44} \\
       \end{absmatrix}
    -a_{32} \begin{absmatrix}{cc}
      a_{13} & a_{14} \\
      a_{43} & a_{44} \\
    \end{absmatrix}
    +a_{42} \begin{absmatrix}{cc}
      a_{13} & a_{14} \\
      a_{33} & a_{34} \\
    \end{absmatrix}}
    \\
    && 
       +a_{31}\paren{
       a_{12} \begin{absmatrix}{cc}
         a_{23} & a_{24} \\
         a_{43} & a_{44} \\
       \end{absmatrix}
    -a_{22} \begin{absmatrix}{cc}
      a_{13} & a_{14} \\
      a_{43} & a_{44} \\
    \end{absmatrix}
    +a_{42} \begin{absmatrix}{cc}
      a_{13} & a_{14} \\
      a_{23} & a_{24} \\
    \end{absmatrix}}
    \\
    && 
       -a_{41}\paren{
       a_{12} \begin{absmatrix}{cc}
         a_{23} & a_{24} \\
         a_{33} & a_{34} \\
       \end{absmatrix}
    -a_{22} \begin{absmatrix}{cc}
      a_{13} & a_{14} \\
      a_{33} & a_{34} \\
    \end{absmatrix}
    +a_{32} \begin{absmatrix}{cc}
      a_{13} & a_{14} \\
      a_{23} & a_{24} \\
    \end{absmatrix}}.
  \end{eqnarray*}
  Expanding the matrices in
  {\eqref{eqn:well-defined-column-cofactor2}}, we get 
  \begin{eqnarray*}
    && 
     -a_{12}\paren{
       a_{21} \begin{absmatrix}{cc}
         a_{33} & a_{34} \\
         a_{43} & a_{44} \\
       \end{absmatrix}
    -a_{31} \begin{absmatrix}{cc}
      a_{23} & a_{24} \\
      a_{43} & a_{44} \\
    \end{absmatrix}
    +a_{41} \begin{absmatrix}{cc}
      a_{23} & a_{24} \\
      a_{33} & a_{34} \\
    \end{absmatrix}}
    \\
    && 
       +a_{22}\paren{
       a_{11} \begin{absmatrix}{cc}
         a_{33} & a_{34} \\
         a_{43} & a_{44} \\
       \end{absmatrix}
    -a_{31} \begin{absmatrix}{cc}
      a_{13} & a_{14} \\
      a_{43} & a_{44} \\
    \end{absmatrix}
    +a_{41} \begin{absmatrix}{cc}
      a_{13} & a_{14} \\
      a_{33} & a_{34} \\
    \end{absmatrix}}
    \\
    && 
       -a_{32}\paren{
       a_{11} \begin{absmatrix}{cc}
         a_{23} & a_{24} \\
         a_{43} & a_{44} \\
       \end{absmatrix}
    -a_{21} \begin{absmatrix}{cc}
      a_{13} & a_{14} \\
      a_{43} & a_{44} \\
    \end{absmatrix}
    +a_{41} \begin{absmatrix}{cc}
      a_{13} & a_{14} \\
      a_{23} & a_{24} \\
    \end{absmatrix}}
    \\
    && 
       +a_{42}\paren{
       a_{11} \begin{absmatrix}{cc}
         a_{23} & a_{24} \\
         a_{33} & a_{34} \\
       \end{absmatrix}
    -a_{21} \begin{absmatrix}{cc}
      a_{13} & a_{14} \\
      a_{33} & a_{34} \\
    \end{absmatrix}
    +a_{31} \begin{absmatrix}{cc}
      a_{13} & a_{14} \\
      a_{23} & a_{24} \\
    \end{absmatrix}}.
  \end{eqnarray*}  
  Comparing the two expressions term by term, we see that they are
  equal. 
\end{proof}

\begin{lemma}{Expanding along any row}{well-defined-row-cofactor}
  Let $A$ be an $n\times n$-matrix. Expanding along any two rows gives
  the same answer.
\end{lemma}

\begin{proof}
  The proof is analogous to that of
  Lemma~\ref{lem:well-defined-column-cofactor}, expanding along rows instead
  of columns.
\end{proof}

\begin{lemma}{Expanding along a column and row}{well-defined-column-row-cofactor}
  Let $A$ be an $n\times n$-matrix. Expanding along the first column
  gives the same answer as expanding along the first row.
\end{lemma}

\begin{proof}
  Again, we only illustrate the proof in case of a $4\times 4$-matrix,
  but the general proof is very similar. Expanding $\det(A)$ along the
  first column, we get:
  Expanding $A$ along the first column, we get
  \begin{equation}\label{eqn:well-defined-row-column-cofactor1}
    a_{11}\begin{absmatrix}{ccc}
      a_{22} & a_{23} & a_{24} \\
      a_{32} & a_{33} & a_{34} \\
      a_{42} & a_{43} & a_{44} \\
    \end{absmatrix}
    - a_{21}\begin{absmatrix}{ccc}
      a_{12} & a_{13} & a_{14} \\
      a_{32} & a_{33} & a_{34} \\
      a_{42} & a_{43} & a_{44} \\
    \end{absmatrix}
    + a_{31}\begin{absmatrix}{ccc}
      a_{12} & a_{13} & a_{14} \\
      a_{22} & a_{23} & a_{24} \\
      a_{42} & a_{43} & a_{44} \\
    \end{absmatrix}
    - a_{41}\begin{absmatrix}{ccc}
      a_{12} & a_{13} & a_{14} \\
      a_{22} & a_{23} & a_{24} \\
      a_{32} & a_{33} & a_{34} \\
    \end{absmatrix}.
  \end{equation}
  Expanding $A$ along the first row, we get:
  \begin{equation}\label{eqn:well-defined-row-column-cofactor2}
    a_{11}\begin{absmatrix}{ccc}
      a_{22} & a_{23} & a_{24} \\
      a_{32} & a_{33} & a_{34} \\
      a_{42} & a_{43} & a_{44} \\
    \end{absmatrix}
    - a_{12}\begin{absmatrix}{ccc}
      a_{21} & a_{23} & a_{24} \\
      a_{31} & a_{33} & a_{34} \\
      a_{41} & a_{43} & a_{44} \\
    \end{absmatrix}
    + a_{13}\begin{absmatrix}{ccc}
      a_{21} & a_{22} & a_{24} \\
      a_{31} & a_{32} & a_{34} \\
      a_{41} & a_{42} & a_{44} \\
    \end{absmatrix}
    - a_{14}\begin{absmatrix}{ccc}
      a_{21} & a_{22} & a_{23} \\
      a_{31} & a_{32} & a_{33} \\
      a_{41} & a_{42} & a_{43} \\
    \end{absmatrix}.
  \end{equation}
  To show these two expressions are equal, let us expand each of the
  $3\times 3$-matrices in
  {\eqref{eqn:well-defined-row-column-cofactor1}} along the first row,
  except for the first matrix. This yields:
  \begin{eqnarray*}
    && 
       a_{11}\begin{absmatrix}{ccc}
         a_{22} & a_{23} & a_{24} \\
         a_{32} & a_{33} & a_{34} \\
         a_{42} & a_{43} & a_{44} \\
       \end{absmatrix}
    \\
    && 
    - a_{21}\paren{
    a_{12}\begin{absmatrix}{ccc}
      a_{33} & a_{34} \\
      a_{43} & a_{44} \\
    \end{absmatrix}
    -a_{13}\begin{absmatrix}{ccc}
      a_{32} & a_{34} \\
      a_{42} & a_{44} \\
    \end{absmatrix}
    +a_{14}\begin{absmatrix}{ccc}
      a_{32} & a_{33} \\
      a_{42} & a_{43} \\
    \end{absmatrix}
    }
    \\
    && 
    + a_{31}\paren{
    a_{12}\begin{absmatrix}{ccc}
      a_{23} & a_{24} \\
      a_{43} & a_{44} \\
    \end{absmatrix}
    -a_{13}\begin{absmatrix}{ccc}
      a_{22} & a_{24} \\
      a_{42} & a_{44} \\
    \end{absmatrix}
    +a_{14}\begin{absmatrix}{ccc}
      a_{22} & a_{23} \\
      a_{42} & a_{43} \\
    \end{absmatrix}
    }
    \\
    && 
    - a_{41}\paren{
    a_{12}\begin{absmatrix}{ccc}
      a_{23} & a_{24} \\
      a_{33} & a_{34} \\
    \end{absmatrix}
    -a_{13}\begin{absmatrix}{ccc}
      a_{22} & a_{24} \\
      a_{32} & a_{34} \\
    \end{absmatrix}
    +a_{14}\begin{absmatrix}{ccc}
      a_{22} & a_{23} \\
      a_{32} & a_{33} \\
    \end{absmatrix}
    }.
  \end{eqnarray*}
  On the other hand, let us expand each of the
  $3\times 3$-matrices in
  {\eqref{eqn:well-defined-row-column-cofactor2}} along the first
  column, except for the first matrix. This yields:
  \begin{eqnarray*}
    && 
       a_{11}\begin{absmatrix}{ccc}
         a_{22} & a_{23} & a_{24} \\
         a_{32} & a_{33} & a_{34} \\
         a_{42} & a_{43} & a_{44} \\
       \end{absmatrix}
    \\
    && 
    - a_{12}\paren{
    a_{21}\begin{absmatrix}{ccc}
      a_{33} & a_{34} \\
      a_{43} & a_{44} \\
    \end{absmatrix}
    -a_{31}\begin{absmatrix}{ccc}
      a_{23} & a_{24} \\
      a_{43} & a_{44} \\
    \end{absmatrix}
    +a_{41}\begin{absmatrix}{ccc}
      a_{23} & a_{24} \\
      a_{33} & a_{34} \\
    \end{absmatrix}
    }
    \\
    && 
    + a_{13}\paren{
    a_{21}\begin{absmatrix}{ccc}
      a_{32} & a_{34} \\
      a_{42} & a_{44} \\
    \end{absmatrix}
    -a_{31}\begin{absmatrix}{ccc}
      a_{22} & a_{24} \\
      a_{42} & a_{44} \\
    \end{absmatrix}
    +a_{41}\begin{absmatrix}{ccc}
      a_{22} & a_{24} \\
      a_{32} & a_{34} \\
    \end{absmatrix}
    }
    \\
    && 
    - a_{14}\paren{
    a_{21}\begin{absmatrix}{ccc}
      a_{32} & a_{33} \\
      a_{42} & a_{43} \\
    \end{absmatrix}
    -a_{31}\begin{absmatrix}{ccc}
      a_{22} & a_{23} \\
      a_{42} & a_{43} \\
    \end{absmatrix}
    +a_{41}\begin{absmatrix}{ccc}
      a_{22} & a_{23} \\
      a_{32} & a_{33} \\
    \end{absmatrix}
    }.
  \end{eqnarray*}  
  The two expressions can be seen to be equal by comparing them term
  by term.
\end{proof}

Together, Lemmas~\ref{lem:well-defined-column-cofactor},
{\ref{lem:well-defined-row-cofactor}}, and
{\ref{lem:well-defined-column-row-cofactor}} imply
Theorem~\ref{thm:well-defined-determinant}.

% ----------------------------------------------------------------------
\subsection{Proof of Theorem~\ref{thm:determinant-row-operations}}

\begin{lemma}{}{L2}
  Assume $A$, $B$ and $C$ are $n\times n$-matrices and
  $i\in\set{1,\ldots,n}$ is an index such that the following holds. 
  \begin{enumerate}
  \item The $k$th rows of all three matrices are identical, for $k\neq i$.

  \item Each entry in the $i$th row of $A$ is the sum of the
    corresponding entries in $i$th rows of $B$ and $C$, i.e.,
    $a_{ij}=b_{ij}+c_{ij}$, for all $j=1,\ldots,n$.
  \end{enumerate}
  Then $\det A=\det B+\det C$.
\end{lemma}

\begin{proof}
  Notice that all three matrices have the same cofactors
  $\cofactor{A}{i1},\ldots,\cofactor{A}{in}$, since these cofactors
  involve deleting the $i\th$ row (and the matrices $A$, $B$, and $C$
  only differ in the $i\th$ row). Expanding all determinants along the
  $i\th$ row, we have
  \begin{eqnarray*}
    \det(A)
    &=& a_{i1}\cofactor{A}{i1} + \ldots + a_{in}\cofactor{A}{in} \\
    &=& (b_{i1}+c_{i1})\cofactor{A}{i1} + \ldots + (b_{in}+c_{in})\cofactor{A}{in} \\
    &=& (b_{i1}\cofactor{B}{i1} + \ldots + b_{in}\cofactor{B}{in})
        + (c_{i1}\cofactor{C}{i1} + \ldots + c_{in}\cofactor{C}{in}) \\
    &=& \det(B) + \det(C).
  \end{eqnarray*}
\end{proof}

% ======================================================================
\subsection{CONTINUE HERE...}

First we recall the definition of a determinant. If $A=\mat{a_{ij} }$
is an $n\times n$-matrix, then $\det A$ is defined by computing the
expansion along the first row:
\begin{equation}
  \label{E1}
  \det A=\sum_{i=1}^n a_{1,i} \cofactor{A}{1,i}.
\end{equation}
If $n=1$ then $\det A=a_{1,1}$.

The following example is straightforward and strongly recommended as a
means for getting used to definitions.

\begin{example}{}{EX1}
  (1) Let $E_{ij}$ be the elementary matrix obtained by interchanging
  $i$th and $j$th rows of $I$.  Then $\det E_{ij}=-1$.

  (2) Let $E_{ik}$ be the elementary matrix obtained by multiplying
  the $i$th row of $I$ by $k$.  Then $\det E_{ik}=k$.

  (3) Let $E_{ijk}$ be the elementary matrix obtained by multiplying
  $i$th row of $I$ by $k$ and adding it to its $j$th row. Then
  $\det E_{ijk}=1$.

  (4) If $C$ and $B$ are such that $CB$ is defined and the $i$th row
  of $C$ consists of zeros, then the $i$th row of $CB$ consists of
  zeros.

  (5) If $E$ is an elementary matrix, then $\det E=\det E^T$.
\end{example}

Many of the proofs in section use the Principle of Mathematical
Induction. This concept is discussed in
Appendix~\ref{well-ordering-section} and is reviewed here for
convenience.  First we check that the assertion is true for $n=2$ (the
case $n=1$ is either completely trivial or meaningless).

Next, we assume that the assertion is true for $n-1$ (where $n\geq 3$)
and prove it for $n$.  Once this is accomplished, by the Principle of
Mathematical Induction we can conclude that the statement is true for
all $n\times n$-matrices for every $n\geq 2$.

If $A$ is an $n\times n$-matrix and $1\leq j \leq n$, then the matrix
obtained by removing $1$st column and $j$th row from $A$ is an
$(n-1)\times (n-1)$-matrix (we shall denote this matrix by $A(j)$
below). Since these matrices are used in computation of cofactors
$\cofactor{A}{1,i}$, for $1\leq i\neq n$, the inductive assumption applies
to these matrices.

Consider the following lemma.

\begin{lemma}{}{L1}
  If $A$ is an $n\times n$-matrix such that one of its rows consists
  of zeros, then $\det A=0$.
\end{lemma}

\begin{proof}
  We will prove this lemma using Mathematical Induction.

  If $n=2$ this is easy (check!).

  Let $n\geq 3$ be such that every matrix of size $(n-1)\times(n-1)$
  with a row consisting of zeros has determinant equal to zero.  Let
  $i$ be such that the $i$th row of $A$ consists of zeros.  Then we
  have $a_{ij}=0$ for $1\leq j\leq n$.

  Fix $j\in \set{1,2, \dots ,n}$ such that $j\neq i$. Then matrix
  $A(j)$ used in computation of $\cofactor{A}{1,j}$ has a row consisting of
  zeros, and by our inductive assumption $\cofactor{A}{1,j}=0$.

  On the other hand, if $j=i$ then $a_{1,j}=0$.  Therefore
  $a_{1,j}\cofactor{A}{1,j}=0$ for all $j$ and by \eqref{E1} we have
  \begin{equation*}
    \det A=\sum_{j=1}^n a_{1,j} \cofactor{A}{1,j}=0
  \end{equation*}
  as each of the summands is equal to 0.
\end{proof}

\begin{theorem}{} {T1}
  Let $A$ and $B$ be $n\times n$-matrices.
  \begin{enumerate}
  \item If $A$ is obtained by interchanging $i$th and $j$th rows of
    $B$ (with $i\neq j$), then $\det A=-\det B$.
  \item If $A$ is obtained by multiplying $i$th row of $B$ by $k$ then
    $\det A=k\det B$.
  \item If two rows of $A$ are identical then $\det A=0$.
  \item If $A$ is obtained by multiplying $i$th row of $B$ by $k$ and
    adding it to $j$th row of $B$ ($i\neq j$) then $\det A=\det B$.
  \end{enumerate}
\end{theorem}

\begin{proof}
  We prove all statements by induction. The case $n=2$ is easily
  checked directly (and it is strongly suggested that you do check
  it).

  We assume $n\geq 3$ and (1)--(4) are true for all matrices of size
  $(n-1)\times (n-1)$.

  (1) We prove the case when $j=i+1$, i.e., we are interchanging two
  consecutive rows.

  Let $l\in \set{1, \dots, n}\setminus \set{i,j}$.  Then $A(l)$ is
  obtained from $B(l)$ by interchanging two of its rows (draw a
  picture) and by our assumption
  \begin{equation}
    \label{E2}
    \cofactor{A}{1,l}=-\cofactor{B}{1,l}.
  \end{equation}

  Now consider $a_{1,i} \cofactor{A}{1,l}$. We have that $a_{1,i}=b_{1,j}$
  and also that $A(i)=B(j)$. Since $j=i+1$, we have
  \begin{equation*}
    (-1)^{1+j}=(-1)^{1+i+1}=-(-1)^{1+i}
  \end{equation*}
  and therefore $a_{1i}\cofactor{A}{1i}=-b_{1j} \cofactor{B}{1j}$ and
  $a_{1j}\cofactor{A}{1j}=-b_{1i} \cofactor{B}{1i}$.  Putting this together with
  \eqref{E2} into \eqref{E1} we see that if in the formula for
  $\det A$ we change the sign of each of the summands we obtain the
  formula for $\det B$.
  \begin{equation*}
    \det A=\sum_{l=1}^n a_{1l}\cofactor{A}{1l}
    =-\sum_{l=1}^n b_{1l} B_{1l}
    =\det B.
  \end{equation*}
  We have therefore proved the case of (1) when $j=i+1$. In order to
  prove the general case, one needs the following fact. If $i<j$, then
  in order to interchange $i$th and $j$th row one can proceed by
  interchanging two adjacent rows $2(j-i)+1$ times: First swap $i$th
  and $i+1$st, then $i+1$st and $i+2$nd, and so on.  After one
  interchanges $j-1$st and $j$th row, we have $i$th row in position of
  $j$th and $l$th row in position of $l-1$st for $i+1\leq l\leq
  j$. Then proceed backwards swapping adjacent rows until everything
  is in place.

  Since $2(j-i)+1$ is an odd number $(-1)^{2(j-i)+1}=-1$ and we have
  that $\det A=-\det B$.

  (2) This is like (1)\dots{} but much easier.  Assume that (2) is
  true for all $(n-1)\times (n-1)$-matrices.  We have that
  $a_{ji}=k b_{ji}$ for $1\leq j\leq n$.  In particular
  $a_{1i}=kb_{1i}$, and for $l\neq i$-matrix $A(l)$ is obtained from
  $B(l)$ by multiplying one of its rows by $k$.  Therefore
  $\cofactor{A}{1l}=k\cofactor{B}{1l}$ for $l\neq i$, and for all $l$ we have
  $a_{1l} \cofactor{A}{1l}=k b_{1l}\cofactor{B}{1l}$.  By \eqref{E1}, we have
  $\det A=k\det B$.

  (3) This is a consequence of (1). If two rows of $A$ are identical,
  then $A$ is equal to the matrix obtained by interchanging those two
  rows and therefore by (1) $\det A=-\det A$. This implies $\det A=0$.

  (4) Assume (4) is true for all $(n-1)\times (n-1)$-matrices and fix
  $A$ and $B$ such that $A$ is obtained by multiplying $i$th row of
  $B$ by $k$ and adding it to $j$th row of $B$ ($i\neq j$) then
  $\det A=\det B$.  If $k=0$ then $A=B$ and there is nothing to prove,
  so we may assume $k\neq 0$.

  Let $C$ be the matrix obtained by replacing the $j$th row of $B$ by
  the $i$th row of $B$ multiplied by $k$.  By Lemma~\ref{lem:L2}, we
  have that
  \begin{equation*}
    \det A=\det B+\det C
  \end{equation*}
  and we `only' need to show that $\det C=0$. But $i$th and $j$th rows
  of $C$ are proportional. If $D$ is obtained by multiplying the $j$th
  row of $C$ by $\frac 1k$ then by (2) we have $\det C=\frac 1k\det D$
  (recall that $k\neq 0$!).  But $i$th and $j$th rows of $D$ are
  identical, hence by (3) we have $\det D=0$ and therefore $\det C=0$.
\end{proof}

\begin{theorem}{}{T2}
  Let $A$ and $B$ be two $n\times
  n$-matrices. Then\index{determinant!product}
  \begin{equation*}
    \det (AB) =\det (A) \det (B)
  \end{equation*}
\end{theorem}

\begin{proof}
  If $A$ is an elementary matrix of either type, then multiplying by
  $A$ on the left has the same effect as performing the corresponding
  elementary row operation. Therefore the equality
  $\det (AB) =\det A\det B$ in this case follows by
  Example~\ref{exa:EX1} and Theorem~\ref{thm:T1}.

  If $C$ is the {\rref} of $A$ then we can write
  $A=E_1\cdot E_2\cdot\dots\cdot E_m\cdot C$ for some elementary
  matrices $E_1,\dots, E_m$.

  Now we consider two cases.

  Assume first that $C=I$. Then $A=E_1\cdot E_2\cdot \dots\cdot E_m$
  and $AB= E_1\cdot E_2\cdot \dots\cdot E_m B$.  By applying the above
  equality $m$ times, and then $m-1$ times, we have that
  \begin{align*}
    \det AB&=\det E_1\det E_2\cdot \det E_m\cdot \det B\\
           &=\det (E_1\cdot E_2\cdot\dots\cdot E_m) \det B\\
           &=\det A\det B.
  \end{align*}
  Now assume $C\neq I$. Since it is in {\rref}, its last row consists
  of zeros and by (4) of Example~\ref{exa:EX1} the last row of $CB$
  consists of zeros.  By Lemma~\ref{lem:L1} we have
  $\det C=\det (CB)=0$ and therefore
  \begin{equation*}
    \det A=\det (E_1\cdot E_2\cdot  E_m)\cdot  \det (C)
    =
    \det (E_1\cdot E_2\cdot  E_m)\cdot 0=0
  \end{equation*}
  and also
  \begin{equation*}
    \det AB=\det (E_1\cdot E_2\cdot  E_m)\cdot  \det (C B)
    =\det (E_1\cdot E_2\cdot\dots\cdot E_m) 0
    =0
  \end{equation*}
  hence $\det AB=0=\det A \det B$.
\end{proof}

The same `machine' used in the previous proof will be used again.

\begin{theorem}{}{T.T}
  Let $A$ be a matrix where $A^T$ is the transpose of $A$. Then,
  \begin{equation*}
    \det(A^T) = \det (A)
  \end{equation*}
\end{theorem}

\begin{proof}
  Note first that the conclusion is true if $A$ is elementary by (5)
  of Example~\ref{exa:EX1}.

  Let $C$ be the {\rref} of $A$. Then we can write
  $A= E_1\cdot E_2\cdot \dots\cdot E_m C$.  Then
  $A^T=C^T\cdot E_m^T\cdot \dots \cdot E_2^T\cdot E_1$.  By
  Theorem~\ref{thm:T2} we have
  \begin{equation*}
    \det (A^T)=\det (C^T)\cdot \det (E_m^T)\cdot \dots \cdot \det (E_2^T)\cdot \det(E_1).
  \end{equation*}
  By (5) of Example~\ref{exa:EX1} we have that $\det E_j=\det E_j^T$
  for all $j$.  Also, $\det C$ is either 0 or 1 (depending on whether
  $C=I$ or not) and in either case $\det C=\det C^T$. Therefore
  $\det A=\det A^T$.
\end{proof}

The above discussions allow us to now prove
Theorem~\ref{thm:well-defined-determinant}. It is restated below.

\begin{theorem}{}{}
  Expanding an $n\times n$-matrix along any row or column always gives
  the same result, which is the determinant.
\end{theorem}

\begin{proof}
  We first show that the determinant can be computed along any
  row. The case $n=1$ does not apply and thus let $n \geq 2$.

  % Assume the theorem is true for all $(n-1)\times (n-1)$-matrices.

  Let $A$be an $n\times n$-matrix and fix $j>1$. We need to prove that
  \begin{equation*}
    \det A=\sum_{i=1}^n a_{j,i} \cofactor{A}{j,i}.
  \end{equation*}
  Let us prove the case when $j=2$.

  Let $B$ be the matrix obtained from $A$ by interchanging its $1$st
  and $2$nd rows.  Then by Theorem~\ref{thm:T1} we have
  \begin{equation*}
    \det A=-\det B.
  \end{equation*}
  Now we have
  \begin{equation*}
    \det B=\sum_{i=1}^n b_{1,i} \cofactor{B}{1,i}.
  \end{equation*}
  Since $B$ is obtained by interchanging the $1$st and $2$nd rows of
  $A$ we have that $b_{1,i}=a_{2,i}$ for all $i$ and one can see that
  $\minor{B}{1,i}=\minor{A}{2,i}$.

  Further,
  \begin{equation*}
    \cofactor{B}{1,i}=(-1)^{1+i} minor B_{1,i}=- (-1)^{2+i} \minor{A}{2,i} = - \cofactor{A}{2,i}
  \end{equation*}
  hence $\det B=-\sum_{i=1}^n a_{2,i} \cofactor{A}{2,i}$, and therefore
  $\det A=-\det B= \sum_{i=1}^n a_{2,i} \cofactor{A}{2,i}$ as desired.

  The case when $j>2$ is very similar; we still have
  $\minor{B}{1,i}=\minor{A}{j,i}$ but checking that
  $\det B=-\sum_{i=1}^n a_{j,i} \cofactor{A}{j,i}$ is slightly more
  involved.

  Now the cofactor expansion along column $j$ of $A$ is equal to the
  cofactor expansion along row $j$ of $A^T$, which is by the above
  result just proved equal to the cofactor expansion along row 1 of
  $A^T$, which is equal to the cofactor expansion along column $1$ of
  $A$. Thus the cofactor cofactor along any column yields the same
  result.

  Finally, since $\det A=\det A^T$ by Theorem~\ref{thm:T.T}, we
  conclude that the cofactor expansion along row $1$ of $A$ is equal
  to the cofactor expansion along row $1$ of $A^T$, which is equal to
  the cofactor expansion along column $1$ of $A$. Thus the proof is
  complete.
\end{proof}
