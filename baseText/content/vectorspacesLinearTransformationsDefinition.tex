\section{Definition and examples}

\begin{outcome}
  \begin{enumerate}
  \item Understand the definition of a linear transformation in the
    context of vector spaces.
  \end{enumerate}
\end{outcome}

In Chapter~\ref{cha:linear-transformation}, we defined a linear
transformation $T:\R^n\to\R^m$ to be a function that preserves
addition and scalar multiplication. We now revisit this concept in the
more general setting of vector spaces $V$ and $W$.

\begin{definition}{Linear transformation}{linear-transformation-vector-space}
  Let $V$ and $W$ be vector spaces over some field $K$. A function
  $T: V \to W$ is called \textbf{linear} or a \textbf{linear
    transformation}%
  \index{linear transformation!on vector spaces} if it satisfies the
  following two conditions:
  \begin{enumerate}
  \item $T$ preserves addition, i.e., for all\/
    $\vect{v},\vect{w}\in V$, we have
    $T(\vect{v}+\vect{w}) = T(\vect{v}) + T(\vect{w})$;
  \item $T$ preserves scalar multiplication, i.e, for all\/
    $\vect{v}\in V$ and $k\in K$, we have
    $T(k\vect{v}) = kT(\vect{v})$.
  \end{enumerate}
\end{definition}

Our first example of a linear transformation is a matrix
transformation. We have already seen this in
Section~\ref{sec:matrix-of-transformation}.

\begin{example}{Matrix transformation}{matrix-transformation}
  Let $A$ be an $m\times n$-matrix. Then the function $T:\R^m\to\R^n$
  defined by $T(\vect{v})=A\vect{v}$ is a linear transformation,
  called a \textbf{matrix transformation}%
  \index{matrix transformation}. This was proved in
  Proposition~\ref{prop:matrix-are-linear}.
\end{example}

There are many interesting examples of linear transformations on
vector spaces other than $\R^n$. We will consider a few such examples.

\begin{example}{Derivative operator}{derivative-operator}
  Let $\Poly_3$ be the vector space of real polynomials of degree at
  most $3$. Show that the function $D:\Poly_3\to\Poly_2$ defined by
  \begin{equation*}
    D(p(x)) = p'(x),
  \end{equation*}
  is a linear transformation. Here, $p'(x)$ denotes the derivative of
  the polynomial $p(x)$. The function $D$ is called the
  \textbf{derivative operator}.
\end{example}

\begin{solution}
  First, we note that if $p(x)$ is a polynomial of degree at most $3$,
  then its derivative $p'(x)$ is a polynomial of degree at most
  $2$. Therefore, the derivative operator $D$ is a well-defined
  function from $\Poly_3$ to $\Poly_2$. To show that it preserves
  addition, consider any two polynomials $p(x),q(x)\in\Poly_3$. From
  calculus, we know that the derivative of $p(x)+q(x)$ is
  $p'(x)+q'(x)$. Therefore,
  \begin{equation*}
    D(p(x)+q(x)) ~=~ (p(x)+q(x))' ~=~ p'(x)+q'(x) ~=~ D(p(x)) + D(q(x)),
  \end{equation*}
  and so $D$ preserves addition. To show that it preserves scalar
  multiplication, consider $p(x)\in\Poly_3$ and $k\in\R$. From
  calculus, we know that the derivative of $kp(x)$ is $kp'(x)$, and
  therefore
  \begin{equation*}
    D(kp(x)) ~=~ (kp(x))' ~=~ kp'(x) ~=~ kD(p(x)).
  \end{equation*}
  Hence, $D$ preserves scalar multiplication. It follows that $D$ is a
  linear transformation.
\end{solution}

It is important to understand that we are not claiming that the
derivative $p'(x)$ of a polynomial $p(x)$ is a linear function. It is
of course a polynomial. Rather, what the above example shows is that
the act of {\em taking} the derivative is a linear operation, i.e.,
the derivative of a sum is the sum of the derivatives, and the
derivative of a constant times a function is a constant times the
derivative.

\begin{example}{Derivative operator examples}{derivative-operator-examples}
  Compute $D(x^3)$, $D(2x^2+x)$, and $D(ax^3+bx^2+cx+d)$.
\end{example}

\begin{solution}
  In each case, we simply take the derivative:
  \begin{eqnarray*}
    D(x^3) &=& 3x^2, \\
    D(2x^2+x) &=& 4x+1, \\
    D(ax^3+bx^2+cx+d) &=& 3ax^2 + 2bx + c.
  \end{eqnarray*}
\end{solution}

\begin{example}{The shift and unshift operators}{shift-unshift}
  Consider the vector space $\Seq_K$ of infinite sequences of elements
  of $K$. The function $\shift:\Seq_K\to\Seq_K$ is defined by shifting
  the entire sequence to the left and dropping the first element:
  \begin{equation*}
    \shift(a_0,a_1,a_2,a_3,\ldots) = (a_1,a_2,a_3,a_4,\ldots).
  \end{equation*}
  The function $\unshift:\Seq_K\to\Seq_K$ is defined by shifting the
  entire sequence to the right and adding $0$ as the new first
  element:
  \begin{equation*}
    \unshift(a_0,a_1,a_2,a_3,\ldots) = (0,a_0,a_1,a_2,\ldots).
  \end{equation*}


\end{example}


% ----------------------------------------------------------------------
% CONTINUE HERE...

Several important examples of linear transformations include the zero
transformation, the identity transformation, and the scalar
transformation.

\begin{example}{Linear transformations}{linear-transformations}
  Let $V$ and $W$ be vector spaces.

  \begin{enumerate}
  \item \textbf{The zero transformation}\index{zero transformation}

    $0:V\to W$ is defined by $0(\vect{v})=\vect{0}$ for all
    $\vect{v}\in V$.

  \item \textbf{The identity transformation}\index{identity transformation}

    $1_V:V\to V$ is defined by $1_V(\vect{v})=\vect{v}$ for all
    $\vect{v}\in V$.

  \item \textbf{The scalar transformation}\index{scalar
      transformation} Let $a\in\R$.

    $s_a:V\to V$ is defined by $s_a(\vect{v})=a\vect{v}$ for all
    $\vect{v}\in V$.
  \end{enumerate}
\end{example}

\begin{solution}
  We will show that the scalar transformation $s_a$ is linear, the
  rest are left as an exercise.

  By Definition~\ref{def:linear-transformation} we must show that for
  all scalars $k ,p $ and vectors $\vect{v}_1$ and $\vect{v}_2$ in
  $V$,
  $s_a(k \vect{v}_1 + p \vect{v}_2) = k s_a(\vect{v}_1)+ p
  s_a(\vect{v}_{2})$. Assume that $a$ is also a scalar.
  \begin{eqnarray*}
    s_a(k \vect{v}_1 + p \vect{v}_2)
    &=& a (k \vect{v}_1 + p \vect{v}_2) \\
    &=&  ak \vect{v}_1 + ap \vect{v}_2  \\
    &=&  k (a \vect{v}_1) + p(a \vect{v}_2)  \\
    &=& k s_a(\vect{v}_1)  + p s_a (\vect{v}_2)
  \end{eqnarray*}
  Therefore $s_a$ is a linear transformation.
\end{solution}

Consider the following important theorem.

\begin{theorem}{Properties of linear transformations}{properties}
  Let $V$ and $W$ be vector spaces, and $T:V \to W$ a linear
  transformation.  Then
  \begin{enumerate}
  \item $T$ preserves the zero vector.
    \begin{equation*}
      T(\vect{0})=\vect{0}
    \end{equation*}
  \item $T$ preserves additive inverses.
    For all $\vect{v}\in V$,
    \begin{equation*}
      T(-\vect{v})= -T(\vect{v})
    \end{equation*}
  \item $T$ preserves linear combinations.
    For all $\vect{v}_1, \vect{v}_2, \ldots, \vect{v}_m \in V$ and
    all $k_1, k_2, \ldots, k_m\in\R$,
    \begin{equation*}
      T(k_1\vect{v}_1 + k_2\vect{v}_2 + \ldots + k_m\vect{v}_m)
      = k_1T(\vect{v}_1) + k_2T(\vect{v}_2) + \ldots + k_mT(\vect{v}_m).
    \end{equation*}
  \end{enumerate}
\end{theorem}

\begin{proof}
  \begin{enumerate}
  \item Let $\vect{0}_V$ denote the zero vector of $V$ and let
    $\vect{0}_W$ denote the zero vector of $W$.  We want to prove that
    $T(\vect{0}_V)=\vect{0}_W$.  Let $\vect{v}\in V$.  Then
    $0\vect{v}=\vect{0}_V$ and
    \begin{equation*}
      T(\vect{0}_V)=T(0\vect{v})=0T(\vect{v})=\vect{0}_W.
    \end{equation*}
  \item Let $\vect{v}\in V$; then $-\vect{v}\in V$ is the additive
    inverse of $\vect{v}$, so $\vect{v} + (-\vect{v})=\vect{0}_V$.
    Thus
    \begin{eqnarray*}
      T(\vect{v} + (-\vect{v})) & = & T(\vect{0}_V) \\
      T(\vect{v}) + T(-\vect{v})) & = & \vect{0}_W \\
      T(-\vect{v}) & = & \vect{0}_W - T(\vect{v}) =  - T(\vect{v}).
    \end{eqnarray*}
  \item This result follows from preservation of addition and
    preservation of scalar multiplication.  A formal proof would be by
    induction on $m$.
  \end{enumerate}
\end{proof}

Consider the following example using the above theorem.

\begin{example}{Linear combination}{linear-transformation-combination2}
  Let $T:\Poly_2 \to \R$ be a linear transformation such that
  \begin{equation*}
    T(x^2+x)=-1; T(x^2-x)=1; T(x^2+1)=3.
  \end{equation*}
  Find $T(4x^2+5x-3)$.
\end{example}

\begin{solution}
  We provide two solutions to this problem.

  \textbf{Solution 1:}
  Suppose $a(x^2+x) + b(x^2-x) + c(x^2+1) = 4x^2+5x-3$.  Then
  \begin{equation*} (a+b+c)x^2 + (a-b)x + c = 4x^2+5x-3.\end{equation*}
  Solving for $a$, $b$, and $c$ results in the unique solution
  $a=6$, $b=1$, $c=-3$.

  Thus
  \begin{eqnarray*}
    T(4x^2+5x-3)
    & = & T(6(x^2+x) + (x^2-x) -3(x^2+1)) \\
    & = & 6T(x^2+x) + T(x^2-x) -3T(x^2+1) \\
    & = & 6(-1) + 1 -3(3) = -14.
  \end{eqnarray*}

  \textbf{Solution 2:}
  Notice that $S=\set{x^2+x, x^2-x, x^2+1}$ is a basis of $\Poly_2$,
  and thus $x^2$, $x$, and $1$ can each be written as a linear
  combination of elements of $S$.

  \begin{eqnarray*}
    x^2 & = & \textstyle \frac{1}{2}(x^2+x) + \frac{1}{2}(x^2-x) \\
    x & = & \textstyle \frac{1}{2}(x^2+x) - \frac{1}{2}(x^2-x) \\
    1 & = & (x^2+1)-\textstyle \frac{1}{2}(x^2+x) - \frac{1}{2}(x^2-x).
  \end{eqnarray*}
  Then
  \begin{eqnarray*}
    T(x^2)
    & = & \textstyle T\paren{\frac{1}{2}(x^2+x) + \frac{1}{2}(x^2-x)}
          =\frac{1}{2}T(x^2+x) + \frac{1}{2}T(x^2-x)\\
    & = & \textstyle \frac{1}{2}(-1) + \frac{1}{2}(1) = 0.  \\
    T(x)
    & = & \textstyle T\paren{\frac{1}{2}(x^2+x) - \frac{1}{2}(x^2-x)}
          = \frac{1}{2}T(x^2+x) - \frac{1}{2}T(x^2-x) \\
    & = & \textstyle \frac{1}{2}(-1) - \frac{1}{2}(1) = -1.\\
    T(1)
    & = & \textstyle T\paren{(x^2+1)-\frac{1}{2}(x^2+x) -
          \frac{1}{2}(x^2-x)}\\
    & = & \textstyle T(x^2+1)-\frac{1}{2}T(x^2+x) - \frac{1}{2}T(x^2-x) \\
    & = & \textstyle 3-\frac{1}{2}(-1) - \frac{1}{2}(1) = 3.
  \end{eqnarray*}
  Therefore,
  \begin{eqnarray*}
    T(4x^2+5x-3) & = & 4T(x^2) + 5T(x) -3T(1) \\
                 & = & 4(0) + 5(-1) - 3(3)=-14.
  \end{eqnarray*}
  The advantage of \textbf{Solution 2} over \textbf{Solution 1} is
  that if you were now asked to find $T(-6x^2-13x+9)$, it is easy to
  use $T(x^2)=0$, $T(x)=-1$ and $T(1)= 3$:
  \begin{eqnarray*}
    T(-6x^2-13x+9) & = & -6T(x^2)-13T(x)+9T(1) \\
                   & = & -6(0)-13(-1)+9(3)=13+27=40.
  \end{eqnarray*}
  More generally,
  \begin{eqnarray*}
    T(ax^2+bx+c) & = & aT(x^2)+bT(x)+cT(1) \\
                 & = & a(0)+b(-1)+c(3)=-b+3c.
  \end{eqnarray*}
\end{solution}

Suppose two linear transformations act in the same way on $\vect{v}$
for all vectors. Then we say that these transformations are equal.

\begin{definition}{Equal transformations}{equal-transformations}
  Let $S$ and $T$ be linear transformations from $V$ to $W$. Then
  $S = T$ if and only if for every $\vect{v} \in V$,
  \begin{equation*}
    S (\vect{v}) = T (\vect{v})
  \end{equation*}
\end{definition}

The definition above requires that two transformations have the same
action on every vector in order for them to be equal. The next theorem
argues that it is only necessary to check the action of the
transformations on basis vectors.

\begin{theorem}{Transformation of a spanning set}{transformation-spanning-set}
  Let $V$ and $W$ be vector spaces and suppose that $S$ and $T$ are
  linear transformations from $V$ to $W$. Then in order for $S$ and
  $T$ to be equal, it suffices that $S(\vect{v}_i) = T(\vect{v}_i)$
  where $V=\sspan \set{\vect{v}_1, \vect{v}_2, \ldots, \vect{v}_n}$.
\end{theorem}

This theorem tells us that a linear transformation is completely
determined by its actions on a spanning set. We can also examine the
effect of a linear transformation on a basis.

\begin{theorem}{Transformation of a basis}{transformation-basis}
  Suppose $V$ and $W$ are vector spaces and let
  $\set{\vect{w}_1, \vect{w}_2, \ldots, \vect{w}_n}$ be any given
  vectors in $W$ that may not be distinct. Then there exists a basis
  $\set{\vect{v}_1, \vect{v}_2, \ldots, \vect{v}_n}$ of $V$ and a
  unique linear transformation $T: V \to W$ with
  $T (\vect{v}_i) = \vect{w}_i$.

  Furthermore, if
  \begin{equation*}
    \vect{v} = k_1\vect{v}_1+k_2\vect{v}_2+\ldots+ k_n\vect{v}_n
  \end{equation*}
  is a vector of $V$, then
  \begin{equation*}
    T(\vect{v}) = k_1\vect{w}_1+k_2\vect{w}_2+\ldots+ k_n\vect{w}_n.
  \end{equation*}
\end{theorem}
