\section{Application: The matrix exponential}

\begin{outcome}
  \begin{enumerate}
  \item Compute $e^A$, $\sin A$, and $\cos A$, for a square matrix $A$.
  \item Apply any analytic function to a square matrix.
  \end{enumerate}
\end{outcome}

From calculus, recall that a function is called \textbf{analytic}%
\index{analytic function} if it can be defined by a power series. For example:
\begin{eqnarray*}
  e^{x} &=& 1 + x + \frac{1}{2}x^2 + \frac{1}{3!}x^3 + \frac{1}{4!}x^4 + \ldots \\
  \sin x &=& x - \frac{1}{3!}x^3 + \frac{1}{5!}x^5 - \frac{1}{7!}x^7 \pm \ldots \\
  \cos x &=& 1 - \frac{1}{2}x^2 + \frac{1}{4!}x^4 - \frac{1}{6!}x^6 \pm \ldots
\end{eqnarray*}
We know from calculus that the above power series converge for all
real numbers $x$. Since it makes sense to compute the $n\th$ power of
a square matrix, in principle it also makes sense to plug a matrix
into a power series. For a square matrix $A$, we can define
\begin{eqnarray*}
  e^{A} &=& I + A + \frac{1}{2}A^2 + \frac{1}{3!}A^3 + \frac{1}{4!}A^4 + \ldots \\
  \sin A &=& A - \frac{1}{3!}A^3 + \frac{1}{5!}A^5 - \frac{1}{7!}A^7 \pm \ldots \\
  \cos A &=& I - \frac{1}{2}A^2 + \frac{1}{4!}A^4 - \frac{1}{6!}A^6 \pm \ldots
\end{eqnarray*}
The goal of this section is to investigate whether these power series
converge, and if yes, how to compute the sum of the series. We begin
with the case of a diagonal matrix.

\begin{example}{Exponential of a diagonal matrix}{exponential-diagonal}
  Compute $e^{D}$, where
  \begin{equation*}
    D = \begin{mymatrix}{rr}
      x & 0 \\
      0 & y \\
    \end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  By definition,
  \begin{eqnarray*}
    e^{D}
    &=& I + D + \frac{1}{2}D^2 + \frac{1}{3!}D^3 + \ldots \\
    &=& \begin{mymatrix}{rr} 1 & 0 \\ 0 & 1 \end{mymatrix}
        + \begin{mymatrix}{cc} x & 0 \\ 0 & y \end{mymatrix}
        + \frac{1}{2}\begin{mymatrix}{cc} x^2 & 0 \\ 0 & y^2 \end{mymatrix}
        + \frac{1}{3!}\begin{mymatrix}{cc} x^3 & 0 \\ 0 & y^3 \end{mymatrix}
        + \ldots \\
    &=& \begin{mymatrix}{cc}
      1 + x + \frac{1}{2}x^2 + \frac{1}{3!}x^3 + \ldots & 0 \\
      0 & 1 + y + \frac{1}{2}y^2 + \frac{1}{3!}y^3 + \ldots \\
    \end{mymatrix} \\
    &=& \begin{mymatrix}{cc}
      e^{x} & 0 \\
      0 & e^{y} \\
    \end{mymatrix}.
  \end{eqnarray*}
  Therefore, the exponential of a diagonal matrix is computed by
  taking the exponential of each diagonal entry. Note that this
  proves, in particular, that the sum converges.
\end{solution}

The same argument also works for applying other analytic functions to
diagonal matrices, for example:
\begin{eqnarray*}
  \sin\begin{mymatrix}{cc} x & 0 \\ 0 & y \end{mymatrix}
  &=& \begin{mymatrix}{cc} \sin x & 0 \\ 0 & \sin y \end{mymatrix}, \\
  \cos\begin{mymatrix}{cc} x & 0 \\ 0 & y \end{mymatrix}
  &=& \begin{mymatrix}{cc} \cos x & 0 \\ 0 & \cos y \end{mymatrix}.
\end{eqnarray*}
But how can we compute the matrix exponential of a non-diagonal
matrix?  This can be done by diagonalization. The following theorem
shows how:

\begin{theorem}{Matrix functions by diagonalization}{matrix-exponential}
  Suppose $A$ is a diagonalizable square matrix, with
  $A=PDP^{-1}$. Then%
  \index{matrix!exponential of}%
  \index{matrix!sine and cosine of}%
  \index{exponential function!of a matrix}%
  \index{sine!of a matrix}%
  \index{cosine!of a matrix}
  \begin{eqnarray*}
    e^{A} &=& Pe^{D}P^{-1}, \\
    \sin A &=& P(\sin D)P^{-1}, \\
    \cos A &=& P(\cos D)P^{-1}.
  \end{eqnarray*}
\end{theorem}

\begin{proof}
  We have
  \begin{eqnarray*}
    e^{A}
    &=& I + A + \frac{1}{2}A^2 + \frac{1}{3!}A^3 + \ldots \\
    &=& I + PDP^{-1} + \frac{1}{2}(PDP^{-1})^2 + \frac{1}{3!}(PDP^{-1})^3 + \ldots \\
    &=& PIP^{-1} + PDP^{-1} + \frac{1}{2}PD^2P^{-1} + \frac{1}{3!}PD^3P^{-1} + \ldots \\
    &=& P(I + D + \frac{1}{2}D^2 + \frac{1}{3!}D^3 + \ldots)P^{-1} \\
    &=& Pe^{D}P^{-1}.
  \end{eqnarray*}
  The proof for $\sin$ and $\cos$ is similar. Indeed, the same method
  works for any analytic function.
\end{proof}  

\begin{example}{A matrix exponential}{matrix-exponential}
  Compute $e^{A}$, where
  \begin{equation*}
    A = \begin{mymatrix}{rr}
      -8 & 10 \\
      -5 & 7 \\
    \end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  We first diagonalize $A$. Following the usual method, we find that
  the eigenvalues are $\eigenvar=-3$ and $\eigenvar=2$, with
  corresponding eigenvectors
  \begin{equation*}
    \begin{mymatrix}{r} 2 \\ 1 \end{mymatrix}
    \quad\mbox{and}\quad
    \begin{mymatrix}{r} 1 \\ 1 \end{mymatrix}.
  \end{equation*}
  Therefore, we can diagonalize $A$ as $A=PDP^{-1}$, where
  \begin{equation*}
    P = \begin{mymatrix}{rr} 2 & 1 \\ 1 & 1 \end{mymatrix},
    \quad
    D = \begin{mymatrix}{rr} -3 & 0 \\ 0 & 2 \end{mymatrix},
    \quad\mbox{and}\quad
    P^{-1} = \begin{mymatrix}{rr} 1 & -1 \\ -1 & 2 \end{mymatrix}.
  \end{equation*}
  By Theorem~\ref{thm:matrix-exponential}, we have
  \begin{equation*}
    e^{A}
    = Pe^{D}P^{-1}
    = \begin{mymatrix}{rr} 2 & 1 \\ 1 & 1 \end{mymatrix}
    \begin{mymatrix}{cc} e^{-3} & 0 \\ 0 & e^{2} \end{mymatrix}
    \begin{mymatrix}{rr} 1 & -1 \\ -1 & 2 \end{mymatrix}
    = \begin{mymatrix}{cc}
      2e^{-3}-e^2 & -2e^{-3}+2e^2 \\
      e^{-3}-e^2  & -e^{-3}+2e^2 \\
    \end{mymatrix}.
  \end{equation*}
\end{solution}
