\section{Linear transformations}

\begin{outcome}
  \begin{enumerate}
  \item Determine whether a function $T:\R^n\to\R^m$ is linear.
  \item Find the matrix corresponding to a linear transformation
    $T:\R^n\to\R^m$.
  \end{enumerate}
\end{outcome}

In calculus, a \textbf{function}%
\index{function} $f:\R\to\R$ is a rule that maps a real number
$x\in\R$ to a real number $f(x)\in\R$. In linear algebra, we can
generalize this concept to vectors. A \textbf{vector function}%
\index{vector function} $T:\R^n\to\R^m$ is a rule that inputs an
$n$-dimensional vector $\vect{v}\in\R^n$ and outputs an
$m$-dimensional vector $T(\vect{v})\in\R^m$. The following are some
examples of vector functions:
\begin{equation}\label{eq:vector-functions}
  T_1\tup{\begin{mymatrix}{c} x \\ y \end{mymatrix}}
  = \begin{mymatrix}{c} x^2 \\ x+y \\ y^2 \end{mymatrix},\quad
  T_2\tup{\begin{mymatrix}{c} x \\ y \\ z \end{mymatrix}}
  = \begin{mymatrix}{c} x+y \\ x+y+z \\ 0 \end{mymatrix},\quad
  T_3\tup{\begin{mymatrix}{c} x \\ y \\ z \end{mymatrix}}
  = \begin{mymatrix}{c} e^{x+z} \\ \sqrt{y} \end{mymatrix}.
\end{equation}
Of these, the first is a function $T_1:\R^2\to\R^3$, the second is a
function $T_2:\R^3\to\R^3$, and the third is a function
$T_3:\R^3\to\R^2$.  We can evaluate a vector function by applying it
to a vector, for example,
\begin{equation*}
  T_1\tup{\begin{mymatrix}{c} 1 \\ 2 \end{mymatrix}}
  = \begin{mymatrix}{c} 1^2 \\ 1+2 \\ 2^2 \end{mymatrix}
  = \begin{mymatrix}{c} 1 \\ 3 \\ 4 \end{mymatrix},\quad
  T_1\tup{\begin{mymatrix}{c} 0 \\ 1 \end{mymatrix}}
  = \begin{mymatrix}{c} 0^2 \\ 1+1 \\ 1^2 \end{mymatrix}
  = \begin{mymatrix}{c} 0 \\ 1 \\ 1 \end{mymatrix},
\end{equation*}
and so on. The study of arbitrary vector functions and their
derivatives and integrals is the subject of {\em multi-variable
  calculus}%
\index{calculus!multi-variable}%
\index{multi-variable calculus}. In linear algebra, we will only be
concerned with \textbf{linear vector functions}%
\index{vector function!linear|see{linear transformation}}, which are
also called \textbf{linear transformations}%
\index{linear transformation}. They are defined as follows.

\begin{definition}{Linear transformation}{linear-transformation}
  A vector function $T:\R^{n}\to \R^{m}$ is called a \textbf{linear
    transformations}%
  \index{linear transformation} if it satisfies the following two
  conditions:
  \begin{enumerate}
  \item $T$ respects addition, i.e., for all\/
    $\vect{v},\vect{w}\in\R^n$, we have
    $T(\vect{v}+\vect{w}) = T(\vect{v}) + T(\vect{w})$;
  \item $T$ respects scalar multiplication, i.e, for all\/
    $\vect{v}\in\R^n$ and scalars $k$, we have
    $T(k\vect{v}) = kT(\vect{v})$.
  \end{enumerate}
\end{definition}

\begin{example}{Linear and non-linear transformations}{linear-transformation}
  Which of the vector functions in {\eqref{eq:vector-functions}} are
  linear transformations?
\end{example}

\begin{solution}
  \begin{enumerate}
  \item[(a)] The function $T_1$ is not a linear transformation. For
    example, let $\vect{v}=\begin{mymatrix}{r} 1 \\
      0 \end{mymatrix}$. Then
    \begin{equation*}
      T_1(\vect{v})
      ~=~ T_1\tup{\begin{mymatrix}{c} 1 \\ 0 \end{mymatrix}}
      ~=~ \begin{mymatrix}{c} 1 \\ 1 \\ 0 \end{mymatrix}
      \quad\mbox{and}\quad
      T_1(2\vect{v})
      ~=~ \tup{\begin{mymatrix}{c} 2 \\ 0 \end{mymatrix}}
      ~=~ \begin{mymatrix}{c} 4 \\ 2 \\ 0 \end{mymatrix}
      ~\neq~ 2\begin{mymatrix}{c} 1 \\ 1 \\ 0 \end{mymatrix}.
    \end{equation*}
    Since $T_1(2\vect{v}) \neq 2T_1(\vect{v})$, the vector function
    $T_1$ does not respect scalar multiplication, and therefore it is
    not a linear transformation.
  \item[(b)] The function $T_2$ is a linear transformation. For
    example, to prove that $T_2$ respects addition, consider two
    arbitrary vectors
    \begin{equation*}
      \vect{v} =
      \begin{mymatrix}{c}
        x_1 \\
        y_1 \\
        z_1 \\
      \end{mymatrix}
      \quad\mbox{and}\quad
      \vect{w} =
      \begin{mymatrix}{c}
        x_2 \\
        y_2 \\
        z_2 \\
      \end{mymatrix}.
    \end{equation*}
    We have
    \begin{equation*}
      T_2(\vect{v}+\vect{w})
      ~=~ T_2\tup{
        \begin{mymatrix}{c}
          x_1+x_2 \\
          y_1+y_2 \\
          z_1+z_2 \\
        \end{mymatrix}}
      ~=~ \begin{mymatrix}{c}
        (x_1+x_2)+(y_1+y_2) \\
        (x_1+x_2)+(y_1+y_2)+(z_1+z_2) \\
        0
      \end{mymatrix}
    \end{equation*}
    and
    \begin{equation*}
      T_2(\vect{v})+T_2(\vect{w})
      ~=~
      \begin{mymatrix}{c}
        x_1+y_1 \\
        x_1+y_1+z_1 \\
        0 \\
      \end{mymatrix}
      + \begin{mymatrix}{c}
        x_2+y_2 \\
        x_2+y_2+z_2 \\
        0 \\
      \end{mymatrix}
      ~=~ \begin{mymatrix}{c}
        (x_1+y_1)+(x_2+y_2) \\
        (x_1+y_1+z_1)+(x_2+y_2+z_2) \\
        0 \\
      \end{mymatrix}.
    \end{equation*}
    Since the two sides are evidently equal, $T_2$ respects
    addition. The fact that it respects scalar multiplication can be
    shown by a similar calculation.
  \item[(c)] The function $T_3$ is not a linear transformation. For
    example, consider
    $\vect{v}=\begin{mymatrix}{c}0\\1\\0\end{mymatrix}$ and
    $\vect{w}=\begin{mymatrix}{c}1\\1\\0\end{mymatrix}$.
    Then
    \begin{equation*}
      T_3(\vect{v}+\vect{w})
      ~=~ T_3\tup{\begin{mymatrix}{c} 1 \\ 2 \\ 0\end{mymatrix}}
      ~=~ \begin{mymatrix}{c} e \\ \sqrt{2} \end{mymatrix},
    \end{equation*}
    and
    \begin{equation*}
      T_3(\vect{v})+T_3(\vect{w})
      ~=~ \begin{mymatrix}{c} 1 \\ 1 \end{mymatrix}
      + \begin{mymatrix}{c} e \\ 1 \end{mymatrix}
      ~=~ \begin{mymatrix}{c} e+1 \\ 2 \end{mymatrix}.
    \end{equation*}
    Since $T_3(\vect{v}+\vect{w})\neq T_3(\vect{v})+T_3(\vect{w})$,
    the vector function $T_3$ does not respect addition, and therefore
    it is not linear.
  \end{enumerate}
\end{solution}

An easy fact about linear transformation is that they preserve the
origin, i.e., they satisfy $T(\vect{0}) = \vect{0}$. This can be seen,
for example, by considering
$T(\vect{0}) = T(\vect{0}+\vect{0}) = T(\vect{0}) + T(\vect{0})$ and
then subtracting $T(\vect{0})$ from both sides of the equation.  This
gives an easier way to see that $T_3$ in the above example is not a
linear transformation, since $T_3(\vect{0}) \neq \vect{0}$. On the
other hand, of course not every function that preserves the origin is
linear. For example, $T_1$ is not linear although it satisfies
$T_1(\vect{0})=\vect{0}$.

An important example of linear transformations are the so-called
\textbf{matrix transformations}%
\index{matrix transformation}%
\index{linear transformation!matrix transformation}.

\begin{proposition}{Matrix transformations are linear transformations}{matrix-are-linear}
  Let $A$ be an $m\times n$-matrix, and consider the vector function
  $T:\R^{n}\to \R^{m}$ defined by $T(\vect{v}) = A\vect{v}$. Then $T$
  is a linear transformation.
\end{proposition}

\begin{proof}
  This follows from the laws of matrix multiplication. Namely, by the
  distributive law, we have
  $A(\vect{v}+\vect{w}) = A\vect{v} + A\vect{w}$, showing that $T$
  respects addition. And by the compatibility of matrix multiplication
  and scalar multiplication, we ahve $A(k\vect{v}) = k(A\vect{v})$,
  showing that $T$ respects scalar multiplication.
\end{proof}

In fact, matrix transformations are not just an example of linear
transformations, but they are essentially the {\em only} example. One
of the central theorems in linear algebra is that all linear
transformations $T:\R^n\to\R^m$ are in fact matrix transformations.
Therefore, a matrix can be regarded as a notation for a linear
transformation, and vice versa. This is the subject of the following
theorem.

\begin{theorem}{Linear transformations are matrix transformations}{matrix-of-linear-transformation}
  Let $T:\R^n\to\R^m$ be any linear transformation. Then there exists
  an $m\times n$-matrix $A$ such that for all $\vect{v}\in\R^n$,
  \begin{equation*}
    T(\vect{v}) = A\vect{v}.
  \end{equation*}
  In other words, $T$ is a matrix transformation.
\end{theorem}

\begin{proof}
  Suppose $T:\R^{n}\to \R^{m}$ is a linear transformation and consider
  the standard basis $\set{\vect{e}_1,\ldots,\vect{e}_n}$ of $\R^n$.
  For all $i$, define $\vect{u}_i = T(\vect{e}_i)$, and let $A$ be the
  matrix that has $\vect{u}_1,\ldots,\vect{u}_n$ as its columns. We
  claim that $A$ is the desired matrix, i.e., that
  $T(\vect{v}) = A\vect{v}$ holds for all $\vect{v}\in\R^n$.

  To see this, let
  \begin{equation*}
    \vect{v} =
    \begin{mymatrix}{c} x_1 \\ \vdots \\ x_n \end{mymatrix}
  \end{equation*}
  be some arbitrary element of $\R^n$. Then
  $\vect{v} = x_1\vect{e}_1 + \ldots + x_n\vect{e}_n$, and we have:
  \begin{equation*}
    \begin{array}{rcl@{\quad}l}
      T(\vect{v})
      &=& T(x_1\vect{e}_1 + \ldots + x_n\vect{e}_n)
      \\
      &=& T(x_1\vect{e}_1) + \ldots + T(x_n\vect{e}_n)
      & \mbox{by linearity}\\
      &=& x_1T(\vect{e}_1) + \ldots + x_nT(\vect{e}_n)
      & \mbox{by linearity}\\
      &=& x_1\vect{u}_1 + \ldots + x_n\vect{u}_n
      & \mbox{by definition of $\vect{u}_i$}\\
      &=& A\vect{v}
      & \mbox{by the column method of matrix multiplication.}
    \end{array}
  \end{equation*}
\end{proof}

In summary, the matrix corresponding to the linear transformation $T$
has as its columns the vectors $T(\vect{e}_1),\ldots,T(\vect{e}_n)$,
i.e., the images of the standard basis vectors. We can visualize this
matrix as follows:
\begin{equation*}
  A=
  \begin{mymatrix}{ccc}
    | &  & | \\
    T(\vect{e}_1) & \cdots & T(\vect{e}_n) \\
    | &  & |
  \end{mymatrix}.
\end{equation*}

\begin{example}{The matrix of a linear transformation}{matrix-of-linear-transformation}
  Suppose $T:\R^{3}\rightarrow \R^{2}$ is a linear transformation where
  \begin{equation*}
    T\tup{\begin{mymatrix}{r} 1 \\ 0 \\ 0 \end{mymatrix}}
    = \begin{mymatrix}{r} 1 \\ 2 \end{mymatrix},\quad
    T\tup{\begin{mymatrix}{r} 0 \\ 1 \\ 0 \end{mymatrix}}
    = \begin{mymatrix}{r} 9 \\ -3 \end{mymatrix},\quad\mbox{and}\quad
    T\tup{\begin{mymatrix}{r} 0 \\ 0 \\ 1 \end{mymatrix}}
    = \begin{mymatrix}{r} 1 \\ 1 \end{mymatrix}.
  \end{equation*}
  Find the matrix $A$ such that $T(\vect{v})=A\vect{v}$ for all
  $\vect{v}$.
\end{example}

\begin{solution}
  By Theorem~\ref{thm:matrix-of-linear-transformation}, the columns of
  $A$ are $T(\vect{e}_1)$, $T(\vect{e}_2)$, and $T(\vect{e}_3)$. Therefore,
  \begin{equation*}
    A=\begin{mymatrix}{rrr}
      1 & 9 & 1 \\
      2 & -3 & 1
    \end{mymatrix}.
  \end{equation*}
\end{solution}

\begin{example}{The matrix of a linear transformation}{matrix-of-linear-transformation2}
  Let $T:\R^3\to\R^2$ be the linear transformation defined by
  \begin{equation*}
    T\tup{\begin{mymatrix}{c} x \\ y \\ z \end{mymatrix}}
    =
    \begin{mymatrix}{c} x+y \\ x+2y-z \end{mymatrix},
  \end{equation*}
  for all $x,y,z\in\R$. Find the matrix of this linear transformation.
\end{example}

\begin{solution}
  We compute the images of the standard basis vectors:
  \begin{eqnarray*}
    T(\vect{e}_1)
    &=& T\tup{\begin{mymatrix}{c} 1 \\ 0 \\ 0 \end{mymatrix}}
    ~=~ \begin{mymatrix}{c} 1 \\ 1 \end{mymatrix},\\
    T(\vect{e}_2)
    &=& T\tup{\begin{mymatrix}{c} 0 \\ 1 \\ 0 \end{mymatrix}}
    ~=~ \begin{mymatrix}{c} 1 \\ 2 \end{mymatrix},\\
    T(\vect{e}_3)
    &=& T\tup{\begin{mymatrix}{c} 0 \\ 0 \\ 1 \end{mymatrix}}
    ~=~ \begin{mymatrix}{c} 0 \\ -1 \end{mymatrix}.
  \end{eqnarray*}
  The matrix $A$ has $T(\vect{e}_1)$, $T(\vect{e}_2)$, and
  $T(\vect{e}_3)$ as its columns. Therefore,
  \begin{equation*}
    A=\begin{mymatrix}{rrr}
      1 & 1 & 0 \\
      1 & 2 & -1
    \end{mymatrix}.
  \end{equation*}
\end{solution}

\begin{example}{Matrix of a projection map}{projection-matrix}
  Let $\vect{u} = \begin{mymatrix}{r} 1 \\ 2 \\ 3 \end{mymatrix}$ and
  let $T: \R^3 \to \R^3$ be the projection%
  \index{projection!as a linear function} map defined by
  \begin{equation*}
    T(\vect{v}) = \func{proj}_{\vect{u}}\tup{\vect{v}}
  \end{equation*}
  for all $\vect{v} \in \R^3$.
  \begin{enumerate}
  \item[(a)] Is $T$ a linear transformation?
  \item[(b)] If yes, find the matrix of $T$.
  \end{enumerate}
\end{example}

\begin{solution}
  \begin{enumerate}
  \item[(a)] Recall the formula for the projection of $\vect{v}$ onto
    $\vect{u}$:
    \begin{equation*}
      \func{proj}_{\vect{u}}\tup{\vect{v}}
      = \frac{\vect{v}\dotprod \vect{u}}{\vect{u}\dotprod \vect{u}}\,\vect{u}.
    \end{equation*}
    In the situation we are interested in, $\vect{u}$ is a fixed vector,
    and $\vect{v}$ is the input to the function $T$. Given any two
    vectors $\vect{v},\vect{w}$, and using the distributive laws of the dot
    product and scalar multiplication, we have:
    \begin{equation*}
      \func{proj}_{\vect{u}}\tup{\vect{v}+\vect{w}}
      = \frac{(\vect{v}+\vect{w})\dotprod \vect{u}}{\vect{u}\dotprod \vect{u}}\,\vect{u}
      = \tup{\frac{\vect{v}\dotprod \vect{u}}{\vect{u}\dotprod \vect{u}}
        + \frac{\vect{w}\dotprod \vect{u}}{\vect{u}\dotprod \vect{u}}}\,\vect{u}
      = \frac{\vect{v}\dotprod \vect{u}}{\vect{u}\dotprod \vect{u}}\,\vect{u}
      + \frac{\vect{w}\dotprod \vect{u}}{\vect{u}\dotprod \vect{u}}\,\vect{u}
      = \func{proj}_{\vect{u}}\tup{\vect{v}}
      + \func{proj}_{\vect{u}}\tup{\vect{w}}.
    \end{equation*}
    Therefore, the function
    $T(\vect{v}) = \func{proj}_{\vect{u}}\tup{\vect{v}}$ respects
    addition. Also, given any scalar $k$, we have
    \begin{equation*}
      \func{proj}_{\vect{u}}\tup{k\vect{v}}
      = \frac{(k\vect{v})\dotprod \vect{u}}{\vect{u}\dotprod \vect{u}}\,\vect{u}
      = \tup{k\frac{\vect{v}\dotprod \vect{u}}{\vect{u}\dotprod \vect{u}}}\,\vect{u}
      = k\tup{\frac{\vect{v}\dotprod \vect{u}}{\vect{u}\dotprod \vect{u}}\,\vect{u}}
      = k\,\func{proj}_{\vect{u}}\tup{\vect{v}}.
    \end{equation*}
    Therefore, the function $T$ respects scalar multiplication. It
    follows that $T$ is a linear function.

  \item[(b)] To find the matrix of $T$, we must compute the images of
    the standard basis vectors $T(\vect{e}_1),\ldots,T(\vect{e}_3)$. We compute
    \begin{eqnarray*}
      T(\vect{e}_1)
      &=& \func{proj}_{\vect{u}}\tup{\vect{e}_1}
      ~=~ \tup{\frac{\vect{e}_1\dotprod \vect{u}}{\vect{u}\dotprod \vect{u}}}\vect{u}
      ~=~ \frac{1}{14}\begin{mymatrix}{r} 1 \\ 2 \\ 3 \end{mymatrix},\\
      T(\vect{e}_2)
      &=& \func{proj}_{\vect{u}}\tup{\vect{e}_2}
      ~=~ \tup{\frac{\vect{e}_2\dotprod \vect{u}}{\vect{u}\dotprod \vect{u}}}\vect{u}
      ~=~ \frac{1}{14}\begin{mymatrix}{r} 1 \\ 2 \\ 3 \end{mymatrix},\\
      T(\vect{e}_3)
      &=& \func{proj}_{\vect{u}}\tup{\vect{e}_3}
      ~=~ \tup{\frac{\vect{e}_3\dotprod \vect{u}}{\vect{u}\dotprod \vect{u}}}\vect{u}
      ~=~ \frac{1}{14}\begin{mymatrix}{r} 1 \\ 2 \\ 3 \end{mymatrix}.
    \end{eqnarray*}
    Hence the matrix of $T$ is
    \begin{equation*}
      A = \frac{1}{14}\begin{mymatrix}{rrr}
        1 & 2 & 3 \\
        2 & 4 & 6 \\
        3 & 6 & 9
      \end{mymatrix}.
    \end{equation*}
  \end{enumerate}
\end{solution}

% ======================================================================
\subsection{CONTINUE HERE}

Two important examples of linear transformations are the zero
transformation\index{zero transformation} and identity
transformation\index{identity transformation}. The zero transformation
defined by $T\tup{\vect{x} } = \vect{0}$ for all $\vect{x}$ is an
example of a linear transformation. Similarly the identity
transformation defined by $T\tup{\vect{x} } = \vect{x}$ is also
linear. Take the time to prove these using the method demonstrated in
Example~\ref{exa:linear-transformation}.
