\section{Linear independence}

\begin{outcome}
\begin{enumerate}
\item[A.] Determine if a set is linearly independent.
\end{enumerate}
\end{outcome}

In this section, we will again explore concepts introduced earlier in terms of $\R^n$ and extend them to apply to abstract vector spaces. 

\begin{definition}{Linear independence}{linear-independence-vector-space}
Let $V$ be a vector space. If $\{\vect{v}_{1},\cdots ,\vect{v}_{n}\} \subseteq V,$ then it is linearly independent
\index{linearly independent} if
\begin{equation*}
\sum_{i=1}^{n}a_{i}\vect{v}_{i}=\vect{0} \;\mbox{implies}\;
a_{1}=\cdots =a_{n}=0
\end{equation*}
where the $a_i$ are real numbers. 
\end{definition}

The
set of vectors is called linearly dependent if it is not linearly independent.
\index{linearly dependent}

\begin{example}{Linear independence}{linear-independence-poly}
Let $S \subseteq \Poly_2$ be a set of polynomials given by
\[
S = \set{x^2 + 2x - 1, 2x^2 - x + 3 }
\]
Determine if $S$ is linearly independent. 
\end{example}

\begin{solution}
To determine if this set $S$ is linearly independent, we write
\[
a ( x^2 + 2x -1 ) + b(2x^2 - x + 3) = 0x^2 + 0x + 0
\]
If it is linearly independent, then $a=b=0$ will be the only solution. We proceed as follows. 
\begin{eqnarray*}
a ( x^2 + 2x -1 ) + b(2x^2 - x + 3) &=& 0x^2 + 0x + 0 \\
ax^2 + 2ax - a + 2bx^2 - bx + 3b &=& 0x^2 + 0x + 0 \\
(a+2b)x^2 + (2a -b)x  - a + 3b &=&  0x^2 + 0x + 0
\end{eqnarray*}

It follows that
\begin{eqnarray*}
a + 2b &=& 0 \\
2a - b &=& 0 \\
-a + 3b &=& 0
\end{eqnarray*}

The augmented matrix and resulting {\rref} are given by
\[
\begin{mymatrix}{rr|r}
1 & 2 & 0 \\
2 & -1 & 0 \\
-1 & 3 & 0 
\end{mymatrix} 
\rightarrow \cdots \rightarrow
\begin{mymatrix}{rr|r}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0 
\end{mymatrix} 
\]

Hence the solution is $a=b=0$ and the set is linearly independent. 
\end{solution}

The next example shows us what it means for a set to be dependent.

\begin{example}{Dependent set}{dependent}
Determine if the set $S$ given below is independent. 
\[
S=\set{
\begin{mymatrix}{c} -1 \\ 0 \\ 1 \end{mymatrix},
\begin{mymatrix}{c} 1 \\ 1 \\ 1 \end{mymatrix},
\begin{mymatrix}{c} 1 \\ 3 \\ 5 \end{mymatrix} }
\]
\end{example}

\begin{solution}
To determine if $S$ is linearly independent, we look for solutions to
\[ 
a\begin{mymatrix}{c} -1 \\ 0 \\ 1 \end{mymatrix}
+b\begin{mymatrix}{c} 1 \\ 1 \\ 1 \end{mymatrix}
+c\begin{mymatrix}{c} 1 \\ 3 \\ 5 \end{mymatrix}
=\begin{mymatrix}{c} 0 \\ 0 \\ 0 \end{mymatrix}
\]
Notice that this equation has nontrivial solutions, 
for example $a=2$, $b=3$ and $c=-1$. Therefore $S$ is dependent. 
\end{solution}

The following is an important result regarding dependent sets.

\begin{lemma}{Dependent sets}{dependent}
Let $V$ be a vector space and suppose $W = \set{\vect{v}_1, \vect{v}_2, \cdots, \vect{v}_k }$ is a subset of $V$. Then $W$ is dependent if and only if $\vect{v}_i$ can be written as a linear combination of $\set{\vect{v}_1, \vect{v}_2, \cdots, \vect{v}_{i-1}, \vect{v}_{i+1}, \cdots,  \vect{v}_k }$ for some $i \leq k$. 
\end{lemma}

Revisit Example \ref{exa:dependent} with this in mind. Notice that we can write one of the three vectors as a combination of the others.
\[
\begin{mymatrix}{c} 1 \\ 3 \\ 5 \end{mymatrix}
=
2\begin{mymatrix}{c} -1 \\ 0 \\ 1 \end{mymatrix}
+3\begin{mymatrix}{c} 1 \\ 1 \\ 1 \end{mymatrix}
\]

By Lemma \ref{lem:dependent} this set is dependent. 

If we know that one particular set is linearly independent, we can use this information to determine if a related set is linearly independent. Consider the following example.

\begin{example}{Related independent sets}{related-independent}
Let $V$ be a vector space and suppose $S \subseteq V$ is a set of linearly independent vectors given by $S = \set{\vect{u}, \vect{v}, \vect{w} }$. Let $R \subseteq V$ be given by $R = \set{2\vect{u} - \vect{w}, \vect{w} + \vect{v}, 3\vect{v} + \frac{1}{2} \vect{u} }$. Show that $R$ is also linearly independent. 
\end{example}

\begin{solution}
To determine if $R$ is linearly independent, we write 
\[
a(2\vect{u} - \vect{w}) + b(\vect{w} + \vect{v}) + c( 3\vect{v} + \vspace{0.05in}\frac{1}{2}\vect{u}) = \vect{0} \]
If the set is linearly independent, the only solution will be $a=b=c=0$. We proceed as follows.  
\begin{eqnarray*}
a(2\vect{u} - \vect{w}) + b(\vect{w} + \vect{v}) + c( 3\vect{v} + \vspace{0.05in}\frac{1}{2} \vect{u}) &=& \vect{0} \\
2a\vect{u} - a\vect{w} + b\vect{w} + b\vect{v}  + 3c\vect{v} + \vspace{0.05in}\frac{1}{2}c\vect{u} &=& \vect{0}\\
(2a + \vspace{0.05in}\frac{1}{2}c) \vect{u} + (b+3c)\vect{v} + (-a + b) \vect{w} &=& \vect{0}
\end{eqnarray*}

We know that the set $S = \set{\vect{u}, \vect{v}, \vect{w} }$ is linearly independent, which implies that the coefficients in the last line of this equation must all equal $0$. 
In other words:
\begin{eqnarray*}
2a + \vspace{0.05in}\frac{1}{2} c &=& 0 \\
b + 3c &=& 0 \\
-a + b &=& 0 
\end{eqnarray*}

The augmented matrix and resulting {\rref} are given by:
\[
\begin{mymatrix}{rrr|r}
2 & 0 & \vspace{0.05in}\frac{1}{2} & 0 \\
0 & 1 & 3 & 0 \\
-1 & 1 & 0 & 0 
\end{mymatrix}
\rightarrow \cdots \rightarrow
\begin{mymatrix}{rrr|r}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 
\end{mymatrix}
\]
Hence the solution is $a=b=c=0$ and the set is linearly independent. 
\end{solution}

The following theorem was discussed in terms in $\R^n$. We consider it here in the general case.

\begin{theorem}{Unique representation}{unique-representation}
Let $V$ be a vector space and let $U = \set{\vect{v}_1, \cdots, \vect{v}_k } \subseteq V$ be an independent set. If $\vect{v} \in \func{span} \;U$, then $\vect{v}$ can be written uniquely as a linear combination of the vectors in $U$. 
\end{theorem}

Consider the span of a linearly independent set of vectors. Suppose we take a vector which is not in this span and add it to the set. The following lemma claims that the resulting set is still linearly independent. 

\begin{lemma}{Adding to a linearly independent set}{adding-linearly-independent}
Suppose $\vect{v}\notin \func{span}\set{\vect{u}_{1},\cdots ,\vect{u}_{k}} $ and $\set{\vect{u}_{1},\cdots ,
\vect{u}_{k}} $ is linearly independent. Then the set
\begin{equation*}
\set{\vect{u}_{1},\cdots ,\vect{u}_{k},\vect{v} }
\end{equation*}
is also linearly independent.
\end{lemma}

\begin{proof}
Suppose $\sum_{i=1}^{k}c_{i}\vect{u}_{i}+d\vect{v}=
\vect{0}.$ It is required to verify that each $c_{i}=0$ and that $d=0.$
But if $d\neq 0,$ then you can solve for $\vect{v}$ as a linear
combination of the vectors, $\set{\vect{u}_{1},\cdots ,\vect{u}
_{k}} $, 
\begin{equation*}
\vect{v}=-\sum_{i=1}^{k}\tup{\frac{c_{i}}{d}} \vect{u}_{i}
\end{equation*}
contrary to the assumption that $\vect{v}$ is not in the span of the $\vect{u}_{i}$. Therefore, $d=0.$ But then $\sum_{i=1}^{k}c_{i}
\vect{u}_{i}=\vect{0}$ and the linear independence of $\set{\vect{u}
_{1},\cdots ,\vect{u}_{k}} $ implies each $c_{i}=0$ also. 
\end{proof}

Consider the following example.

\begin{example}{Adding to a linearly independent set}{adding-lin-ind}
Let $S \subseteq M_{22}$ be a linearly independent set given by 
\[
S  = \set{\begin{mymatrix}{rr}
1 & 0 \\
0 & 0 
\end{mymatrix}, \begin{mymatrix}{rr}
0 & 1 \\
0 & 0 
\end{mymatrix} }
\]
Show that the set $R \subseteq M_{22}$ given by 
\[
R = \set{\begin{mymatrix}{rr}
1 & 0 \\
0 & 0 
\end{mymatrix}, \begin{mymatrix}{rr}
0 & 1 \\
0 & 0 
\end{mymatrix}, \begin{mymatrix}{rr}
0 & 0 \\
1 & 0 
\end{mymatrix} }
\]
is also linearly independent.
\end{example}

\begin{solution}
Instead of writing a linear combination of the matrices which equals
$0$ and showing that the coefficients must equal $0$, we can instead
use Lemma \ref{lem:adding-linearly-independent}.

To do so, we show that 
\[
\begin{mymatrix}{rr}
0 & 0 \\
1 & 0 
\end{mymatrix}
\notin
\func{span}\set{\begin{mymatrix}{rr}
1 & 0 \\
0 & 0 
\end{mymatrix}, \begin{mymatrix}{rr}
0 & 1 \\
0 & 0 
\end{mymatrix} }
\]

Write 
\begin{eqnarray*}
\begin{mymatrix}{rr}
0 & 0 \\
1 & 0 
\end{mymatrix}
&=&  a\begin{mymatrix}{rr}
1 & 0 \\
0 & 0 
\end{mymatrix} +  b\begin{mymatrix}{rr}
0 & 1 \\
0 & 0 
\end{mymatrix} \\
&=&
\begin{mymatrix}{rr}
a & 0 \\
0 & 0 
\end{mymatrix} +  \begin{mymatrix}{rr}
0 & b \\
0 & 0 
\end{mymatrix} \\
&=& \begin{mymatrix}{rr}
a & b \\
0 & 0 
\end{mymatrix}
\end{eqnarray*}

Clearly there are no possible $a,b$ to make this equation true. Hence the new matrix does not lie in the span of the matrices in $S$. By Lemma \ref{lem:adding-linearly-independent}, $R$ is also linearly independent.
\end{solution}
