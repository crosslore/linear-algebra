\subsection{The singular value decomposition}

We begin this section with an important definition.

\begin{definition}{Singular values}{singular-values}
Let $A$ be an $m\times n$-matrix. The singular values of $A$ are the square roots of the positive
eigenvalues of $A^TA$\index{singular values}.
\end{definition}

Singular Value Decomposition (SVD) can be thought of as 
a generalization of orthogonal diagonalization of a symmetric matrix
to an arbitrary $m\times n$-matrix. This decomposition is the focus of this section. 

The following is a useful result that will help when computing the SVD of matrices.

\begin{proposition}{}{same-non-zero-eigenvalues}
Let $A$ be an $m \times n$-matrix. Then $A^TA$ and $AA^T$ have the same \bf{non-zero} eigenvalues.
\end{proposition}

\begin{proof}
Suppose $A$ is an $m\times n$-matrix, and suppose that  $\lambda$ is a non-zero eigenvalue of $A^TA$.
Then there exists a non-zero vector $X\in \R^n$ such that

\begin{equation}\label{non-zero}
(A^TA)X=\lambda X.
\end{equation}

Multiplying both sides of this equation by $A$ yields:
\begin{eqnarray*}
A(A^TA)X & = & A\lambda X\\
(AA^T)(AX) & = & \lambda (AX).
\end{eqnarray*}
Since $\lambda\neq 0$ and $X\neq 0_n$, $\lambda X\neq 0_n$,
and thus by equation~\eqref{non-zero},
$(A^TA)X\neq 0_m$; thus $A^T(AX)\neq 0_m$, 
implying that $AX\neq 0_m$.

Therefore $AX$ is an eigenvector of $AA^T$ corresponding to eigenvalue
$\lambda$.  An analogous argument can be used to show that every
non-zero eigenvalue of $AA^T$ is an eigenvalue of $A^TA$, thus
completing the proof.
\end{proof}

Given an $m\times n$-matrix $A$, we will see how to express $A$ as a product
\[ A=U\Sigma V^T\]
where
\begin{itemize}
\item $U$ is an $m\times m$ orthogonal matrix whose columns are
eigenvectors of $AA^T$.
\item $V$ is an $n\times n$ orthogonal matrix whose columns are
eigenvectors of $A^TA$.
\item $\Sigma$ is an $m\times n$-matrix whose only non-zero values
lie on its main diagonal, and are the singular values of $A$.
\end{itemize}

How can we find such a decomposition? We are aiming to decompose $A$ in the following form:

\begin{equation*}
A=U\begin{mymatrix}{cc}
\sigma & 0 \\ 
0 & 0
\end{mymatrix} V^T 
\end{equation*}

where $\sigma $ is of the form 
\[
\sigma =\begin{mymatrix}{ccc}
\sigma _{1} &  & 0 \\ 
& \ddots &  \\ 
0 &  & \sigma _{k}
\end{mymatrix}
\]

Thus $A^T=V\begin{mymatrix}{cc}
\sigma & 0 \\ 
0 & 0
\end{mymatrix} U^T$ and it follows that 
\begin{equation*}
A^TA=V\begin{mymatrix}{cc}
\sigma & 0 \\ 
0 & 0
\end{mymatrix} U^TU\begin{mymatrix}{cc}
\sigma & 0 \\ 
0 & 0
\end{mymatrix} V^T=V\begin{mymatrix}{cc}
\sigma ^{2} & 0 \\ 
0 & 0
\end{mymatrix} V^T
\end{equation*}
and so $A^TAV=V\begin{mymatrix}{cc}
\sigma ^{2} & 0 \\ 
0 & 0
\end{mymatrix}$. Similarly, $AA^TU=U\begin{mymatrix}{cc}
\sigma ^{2} & 0 \\ 
0 & 0
\end{mymatrix}$. Therefore, you would find an orthonormal basis of eigenvectors
for $AA^T$ make them the columns of a matrix such that the
corresponding eigenvalues are decreasing. This gives $U$. You could then do
the same for $A^TA$ to get $V$.

We formalize this discussion in the following theorem. 

\begin{theorem}{Singular value decomposition}{sing-val-decomp}
Let $A$ be an $m\times n$-matrix. Then there exist
orthogonal matrices $U$ and $V$ of the appropriate size such that $A= U \Sigma V^T$ where $\Sigma$ is of the form
\[
\Sigma = 
\begin{mymatrix}{cc}
\sigma & 0 \\ 
0 & 0
\end{mymatrix}
\]
and $\sigma $ is of the form 
\[
\sigma =\begin{mymatrix}{ccc}
\sigma _{1} &  & 0 \\ 
& \ddots &  \\ 
0 &  & \sigma _{k}
\end{mymatrix}
\]
for the $\sigma _{i}$ the singular values of $A$.
\end{theorem}

\begin{proof}
There exists an orthonormal basis, $\set{\vect{v}_{i}} _{i=1}^{n}$ such that $
A^TA\vect{v}_{i}=\sigma _{i}^{2}\vect{v}_{i}$ where $\sigma
_{i}^{2}>0$ for $i=1,\ldots,k,\tup{\sigma _{i}>0} $ and equals zero
if $i>k$. Thus for $i>k$, $A\vect{v}_{i}=\vect{0}$ because 
\begin{equation*}
 A\vect{v}_{i}\dotprod A\vect{v}_{i} = A^TA\vect{v}_{i} \dotprod \vect{v}_{i}  = \vect{0} \dotprod \vect{v}_{i} =0.
\end{equation*}
For $i=1,\ldots,k$, define $\vect{u}_{i}\in \R^{m}$ by 
\begin{equation*}
\vect{u}_{i}= \sigma _{i}^{-1}A\vect{v}_{i}.
\end{equation*}

Thus $A\vect{v}_{i}=\sigma _{i}\vect{u}_{i}$. Now 
\begin{eqnarray*}
\vect{u}_{i} \dotprod \vect{u}_{j} &=&  \sigma _{i}^{-1}A
\vect{v}_{i} \dotprod \sigma _{j}^{-1}A\vect{v}_{j}  = \sigma_{i}^{-1}\vect{v}_{i} \dotprod \sigma _{j}^{-1}A^TA\vect{v}_{j} \\
&=& \sigma _{i}^{-1}\vect{v}_{i} \dotprod \sigma _{j}^{-1}\sigma _{j}^{2} \vect{v}_{j} =
\frac{\sigma _{j}}{\sigma _{i}}\tup{\vect{v}_{i} \dotprod \vect{v}_{j}}
=\delta _{ij}.
\end{eqnarray*}
Thus $\set{\vect{u}_{i}} _{i=1}^{k}$ is an orthonormal set of
vectors in $\R^{m}$. Also, 
\begin{equation*}
AA^T\vect{u}_{i}=AA^T\sigma _{i}^{-1}A\vect{v}_{i}=\sigma
_{i}^{-1}AA^TA\vect{v}_{i}=\sigma _{i}^{-1}A\sigma _{i}^{2}\vect{v}
_{i}=\sigma _{i}^{2}\vect{u}_{i}.
\end{equation*}
Now extend $\set{\vect{u}_{i}} _{i=1}^{k}$ to an orthonormal
basis for all of $\R^{m}$, $\set{\vect{u}_{i}} _{i=1}^{m}$
and let 
\begin{equation*}
U= \begin{mymatrix}{ccc}
\vect{u}_{1} & \cdots & \vect{u}_{m}
\end{mymatrix}
\end{equation*}
while $V= \mat{\vect{v}_{1},\ldots,\vect{v}_{n}}$. Thus $U$
is the matrix which has the $\vect{u}_{i}$ as columns and $V$ is defined
as the matrix which has the $\vect{v}_{i}$ as columns. Then 
\begin{equation*}
U^TAV=\begin{mymatrix}{c}
\vect{u}_{1}^T \\ 
\vdots \\ 
\vect{u}_{k}^T \\ 
\vdots \\ 
\vect{u}_{m}^T
\end{mymatrix} A\mat{\vect{v}_{1},\ldots,\vect{v}_{n}}
\end{equation*}
\begin{equation*}
=\begin{mymatrix}{c}
\vect{u}_{1}^T \\ 
\vdots \\ 
\vect{u}_{k}^T \\ 
\vdots \\ 
\vect{u}_{m}^T
\end{mymatrix} \begin{mymatrix}{cccccc}
\sigma _{1}\vect{u}_{1} & \cdots & \sigma _{k}\vect{u}_{k} & \vect{0}
& \cdots & \vect{0}
\end{mymatrix} =\begin{mymatrix}{cc}
\sigma & 0 \\ 
0 & 0
\end{mymatrix}
\end{equation*}
where $\sigma $ is given in the statement of the theorem. 
\end{proof}

The singular value decomposition has as an immediate corollary which is given in the following interesting result. 

\begin{corollary}{Rank and singular values}{rank-singular-values}
Let $A$ be an $m\times n$-matrix. Then the rank of $A$ and $A^T$equals
the number of singular values.
\end{corollary}

%%\begin{proof}
%%Since $V$ and $U$ are unitary, it follows that 
%%\begin{eqnarray*}
%%\rank(A) &=&\rank(U^TAV) =
%%\rank\mat{
%%\begin{array}{cc}
%%\sigma & 0 \\ 
%%0 & 0
%%\end{array}
%%} \\
%%&=&\text{number of singular values.}
%%\end{eqnarray*}
%%Also since $U,V$ are unitary, 
%%\begin{equation*}
%%\rank(A^T) =\rank(V^TA^{\ast
%%}U) =\rank((U^TAV)^T)
%%\end{equation*}
%%\begin{equation*}
%%=\rank\tup{\mat{
%%\begin{array}{cc}
%%\sigma & 0 \\ 
%%0 & 0
%%\end{array}
%%} ^T} =\text{number of singular values.}
%%\end{equation*}
%%\end{proof}
%%
%%\medskip

Let's compute the Singular Value Decomposition of a simple matrix.

\begin{example}{Singular value decomposition}{SVD}
Let 
$A=\begin{mymatrix}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{mymatrix}$.
Find the Singular Value Decomposition (SVD) of $A$.
\end{example}

\begin{solution}
To begin, we compute $AA^T$ and $A^TA$.
\[ AA^T = \begin{mymatrix}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{mymatrix}
\begin{mymatrix}{rr} 1 & 3 \\ -1 & 1 \\ 3 & 1  \end{mymatrix}
= \begin{mymatrix}{rr} 11 & 5 \\ 5 & 11  \end{mymatrix}.\]

\[ A^TA = \begin{mymatrix}{rr} 1 & 3 \\ -1 & 1 \\ 3 & 1  \end{mymatrix}
\begin{mymatrix}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{mymatrix}
= \begin{mymatrix}{rrr} 10 & 2 & 6 \\ 2 & 2 & -2\\
6 & -2 & 10 \end{mymatrix}.\]

Since $AA^T$ is $2\times 2$ while $A^T A$ is $3\times 3$, and $AA^T$
and $A^TA$ have the same {\em non-zero} eigenvalues (by Proposition~\ref{prop:same-non-zero-eigenvalues}), we compute the characteristic polynomial  $c_{AA^T}(x)$ (because it's
easier to compute than $c_{A^TA}(x)$).

\begin{eqnarray*}
c_{AA^T}(x)& = &\det(xI-AA^T)= \begin{absmatrix}{cc}
x-11 & -5 \\ -5 & x-11 \end{absmatrix}\\
& = &(x-11)^2 - 25 \\
& = & x^2-22x+121-25\\
& = & x^2-22x+96\\
& = & (x-16)(x-6)
\end{eqnarray*}

Therefore, the eigenvalues of $AA^T$ are $\lambda_1=16$ and $\lambda_2=6$.

The eigenvalues of $A^TA$ are $\lambda_1=16$, $\lambda_2=6$, and
$\lambda_3=0$, and the singular values of $A$ are $\sigma_1=\sqrt{16}=4$ and
$\sigma_2=\sqrt{6}$.
By convention, we list the eigenvalues (and corresponding singular values)
in non-increasing order (i.e., from largest to smallest).

{\bf To find the matrix $V$}:

To construct the matrix $V$ we need to find eigenvectors for $A^TA$.
Since the eigenvalues of $AA^T$ are distinct, the corresponding
eigenvectors are orthogonal, and we need only normalize them.

$\lambda_1=16$: solve $(16I-A^TA)Y= 0$.

\[ \begin{mymatrix}{rrr|r}
6 & -2 & -6 & 0 \\ -2 & 14 & 2 & 0 \\ -6 & 2 & 6 & 0
\end{mymatrix}
\rightarrow
\begin{mymatrix}{rrr|r}
1 & 0 & -1 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0
\end{mymatrix},
\mbox{ so }
Y=\begin{mymatrix}{r} t \\ 0 \\ t \end{mymatrix}
=t\begin{mymatrix}{r} 1 \\ 0 \\ 1 \end{mymatrix},
t\in \R. \]

$\lambda_2=6$: solve $(6I-A^TA)Y= 0$.

\[ \begin{mymatrix}{rrr|r}
-4 & -2 & -6 & 0 \\ -2 & 4 & 2 & 0 \\ -6 & 2 & -4 & 0
\end{mymatrix}
\rightarrow
\begin{mymatrix}{rrr|r}
1 & 0 & 1 & 0 \\ 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 0
\end{mymatrix},
\mbox{ so }
Y=\begin{mymatrix}{r} -s \\ -s \\ s \end{mymatrix}
=s\begin{mymatrix}{r} -1 \\ -1 \\ 1 \end{mymatrix},
s\in \R. \]

$\lambda_3=0$: solve $(-A^TA)Y= 0$.
\[ \begin{mymatrix}{rrr|r}
-10 & -2 & -6 & 0 \\ -2 & -2 & 2 & 0 \\ -6 & 2 & -10 & 0
\end{mymatrix}
\rightarrow
\begin{mymatrix}{rrr|r}
1 & 0 & 1 & 0 \\ 0 & 1 & -2 & 0 \\ 0 & 0 & 0 & 0
\end{mymatrix},
\mbox{ so }
Y=\begin{mymatrix}{r} -r \\ 2r \\ r \end{mymatrix}
=r\begin{mymatrix}{r} -1 \\ 2 \\ 1 \end{mymatrix},
r\in \R. \]


Let
\[ V_1=\frac{1}{\sqrt{2}}\begin{mymatrix}{r} 1\\ 0\\ 1 \end{mymatrix},
V_2=\frac{1}{\sqrt{3}}\begin{mymatrix}{r} -1\\ -1\\ 1 \end{mymatrix},
V_3=\frac{1}{\sqrt{6}}\begin{mymatrix}{r} -1\\ 2\\ 1 \end{mymatrix}.\]

Then
\[ V=\frac{1}{\sqrt{6}}\begin{mymatrix}{rrr}
\sqrt 3 & -\sqrt 2 & -1  \\
0 & -\sqrt 2 & 2 \\
\sqrt 3 & \sqrt 2 & 1 \end{mymatrix}.\]

Also,
\[ \Sigma = \begin{mymatrix}{rrr} 4 & 0 & 0 \\
0 & \sqrt 6 & 0 \end{mymatrix},\]
and we use $A$, $V^T$, and $\Sigma$ to find $U$.

Since $V$ is orthogonal and $A=U\Sigma V^T$, it follows that $AV=U\Sigma$.
Let $V=\begin{mymatrix}{ccc} V_1 & V_2 & V_3 \end{mymatrix}$, and
let $U=\begin{mymatrix}{cc} U_1 & U_2 \end{mymatrix}$, where
$U_1$ and $U_2$ are the two columns of $U$.

Then we have
\begin{eqnarray*}
A\begin{mymatrix}{ccc} V_1 & V_2 & V_3 \end{mymatrix}
&=& \begin{mymatrix}{cc} U_1 & U_2 \end{mymatrix}\Sigma\\
\begin{mymatrix}{ccc} AV_1 & AV_2 & AV_3 \end{mymatrix}
&=& \begin{mymatrix}{ccc} \sigma_1U_1 + 0U_2 &
0U_1 + \sigma_2 U_2 & 0 U_1 + 0 U_2 \end{mymatrix} \\
&=& \begin{mymatrix}{ccc} \sigma_1U_1 & \sigma_2 U_2 &
0 \end{mymatrix}
\end{eqnarray*}
which implies that $AV_1=\sigma_1U_1 = 4U_1$ and
$AV_2=\sigma_2U_2 = \sqrt 6 U_2$.

Thus,
\[ U_1 = \frac{1}{4}AV_1 
= \frac{1}{4}
\begin{mymatrix}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{mymatrix}
\frac{1}{\sqrt{2}}\begin{mymatrix}{r} 1\\ 0\\ 1 \end{mymatrix}
= \frac{1}{4\sqrt 2}\begin{mymatrix}{r} 4\\ 4 \end{mymatrix}
= \frac{1}{\sqrt 2}\begin{mymatrix}{r} 1\\ 1 \end{mymatrix},\]
and
\[ U_2 = \frac{1}{\sqrt 6}AV_2 
= \frac{1}{\sqrt 6}
\begin{mymatrix}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{mymatrix}
\frac{1}{\sqrt{3}}\begin{mymatrix}{r} -1\\ -1\\ 1 \end{mymatrix}
=\frac{1}{3\sqrt 2}\begin{mymatrix}{r} 3\\ -3 \end{mymatrix}
=\frac{1}{\sqrt 2}\begin{mymatrix}{r} 1\\ -1 \end{mymatrix}.
\]
Therefore,
\[ U=\frac{1}{\sqrt{2}}\begin{mymatrix}{rr} 1 & 1 \\
1 & -1 \end{mymatrix},\]
and
\begin{eqnarray*}
A & = & \begin{mymatrix}{rrr} 1 & -1 & 3 \\ 3 & 1 & 1 \end{mymatrix}\\
& = & \tup{\frac{1}{\sqrt{2}}\begin{mymatrix}{rr} 1 & 1 \\
1 & -1 \end{mymatrix}}
\begin{mymatrix}{rrr} 4 & 0 & 0 \\
0 & \sqrt 6 & 0 \end{mymatrix}
\tup{\frac{1}{\sqrt{6}}\begin{mymatrix}{rrr}
\sqrt 3 & 0 & \sqrt 3  \\
-\sqrt 2 & -\sqrt 2 & \sqrt2 \\
-1 & 2 & 1 \end{mymatrix}}.
\end{eqnarray*}
\end{solution}

Here is another example. 

\begin{example}{Finding the SVD}{SVD2}
Find an SVD for
$A=\begin{mymatrix}{r} -1 \\ 2\\ 2 \end{mymatrix}$.
\end{example}

\begin{solution}
Since $A$ is $3\times 1$, $A^T A$ is a $1\times 1$-matrix
whose eigenvalues are easier to find than the eigenvalues of
the $3\times 3$-matrix $AA^T$.

\[ A^TA=\begin{mymatrix}{ccc} -1 & 2 & 2 \end{mymatrix}
\begin{mymatrix}{r} -1 \\ 2 \\ 2 \end{mymatrix}
=\begin{mymatrix}{r} 9 \end{mymatrix}.\]

Thus $A^TA$ has eigenvalue $\lambda_1=9$, and
the eigenvalues of $AA^T$ are $\lambda_1=9$, $\lambda_2=0$, and
$\lambda_3=0$. 
Furthermore, $A$ has only one singular value, $\sigma_1=3$.

{\bf To find the matrix $V$}:
To do so we find an eigenvector for $A^TA$ and
normalize it.
In this case, finding a unit eigenvector is trivial:
$V_1=\begin{mymatrix}{r} 1 \end{mymatrix}$, and
\[ V=\begin{mymatrix}{r} 1 \end{mymatrix}.\]

Also,
$\Sigma =\begin{mymatrix}{r} 3 \\ 0\\ 0 \end{mymatrix}$,
and we use $A$, $V^T$, and $\Sigma$ to find $U$.


Now $AV=U\Sigma$, with
$V=\begin{mymatrix}{r} V_1 \end{mymatrix}$,
and $U=\begin{mymatrix}{rrr} U_1 & U_2 & U_3 \end{mymatrix}$,
where $U_1$, $U_2$, and $U_3$ are the columns of $U$.
Thus
\begin{eqnarray*}
A\begin{mymatrix}{r} V_1 \end{mymatrix}
&=& \begin{mymatrix}{rrr} U_1 & U_2 & U_3 \end{mymatrix}\Sigma\\
\begin{mymatrix}{r} AV_1 \end{mymatrix}
&=& \begin{mymatrix}{r} \sigma_1 U_1+0U_2+0U_3 \end{mymatrix}\\
&=& \begin{mymatrix}{r} \sigma_1 U_1 \end{mymatrix}
\end{eqnarray*}
This gives us $AV_1=\sigma_1 U_1= 3U_1$, so

\[ U_1 = \frac{1}{3}AV_1 
= \frac{1}{3}
\begin{mymatrix}{r} -1 \\ 2 \\ 2 \end{mymatrix}
\begin{mymatrix}{r} 1 \end{mymatrix}
= \frac{1}{3}
\begin{mymatrix}{r} -1 \\ 2 \\ 2 \end{mymatrix}.\]

The vectors $U_2$ and $U_3$ are eigenvectors of $AA^T$ corresponding
to the eigenvalue $\lambda_2=\lambda_3=0$.
Instead of solving the system $(0I-AA^T)X= 0$ and then using the
Gram-Schmidt process on the resulting set of
two basic eigenvectors, the following approach may be used.


Find vectors $U_2$ and $U_3$ by first extending $\set{U_1}$ to a basis of
$\R^3$, then using the Gram-Schmidt algorithm to orthogonalize the basis,
and finally normalizing the vectors.

Starting with $\set{3U_1}$ instead of $\set{U_1}$ makes the
arithmetic a bit easier.
It is easy to verify that

\[ \set{\begin{mymatrix}{r} -1 \\ 2 \\ 2 \end{mymatrix},
\begin{mymatrix}{r} 1 \\ 0 \\ 0 \end{mymatrix},
\begin{mymatrix}{r} 0 \\ 1 \\ 0 \end{mymatrix}}\]
is a basis of $\R^3$.  Set

\[ E_1 = \begin{mymatrix}{r} -1 \\ 2 \\ 2 \end{mymatrix},
X_2 = \begin{mymatrix}{r} 1 \\ 0 \\ 0 \end{mymatrix},
X_3 =\begin{mymatrix}{r} 0 \\ 1 \\ 0 \end{mymatrix},\]

and apply the Gram-Schmidt algorithm to
$\set{E_1, X_2, X_3}$.

This gives us

\[ E_2 = \begin{mymatrix}{r} 4 \\ 1 \\ 1 \end{mymatrix}
\mbox{ and }
E_3 = \begin{mymatrix}{r} 0 \\ 1 \\ -1 \end{mymatrix}.\]

Therefore,
\[ U_2 = \frac{1}{\sqrt{18}}
 \begin{mymatrix}{r} 4 \\ 1 \\ 1 \end{mymatrix},
U_3 = \frac{1}{\sqrt 2}
\begin{mymatrix}{r} 0 \\ 1 \\ -1 \end{mymatrix},\]
and

\[ U = \begin{mymatrix}{rrr} -\frac{1}{3} & \frac{4}{\sqrt{18}} & 0 \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & \frac{1}{\sqrt 2} \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & -\frac{1}{\sqrt 2} \end{mymatrix}.\]

Finally,

\[ A = 
\begin{mymatrix}{r} -1 \\ 2 \\ 2 \end{mymatrix}
=
\begin{mymatrix}{rrr} -\frac{1}{3} & \frac{4}{\sqrt{18}} & 0 \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & \frac{1}{\sqrt 2} \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & -\frac{1}{\sqrt 2} \end{mymatrix}
\begin{mymatrix}{r} 3 \\ 0 \\ 0 \end{mymatrix}
\begin{mymatrix}{r} 1 \end{mymatrix}.\]

\end{solution}

Consider another example.

\begin{example}{Find the SVD}{SVD3}
Find a singular value decomposition for the matrix 
\begin{equation*}
A= \begin{mymatrix}{ccc}
\frac{2}{5}\sqrt{2}\sqrt{5} & \frac{4}{5}\sqrt{2}\sqrt{5} & 0 \\ 
\frac{2}{5}\sqrt{2}\sqrt{5} & \frac{4}{5}\sqrt{2}\sqrt{5} & 0
\end{mymatrix}
\end{equation*}
\end{example}

First consider $A^TA$
\begin{equation*}
\begin{mymatrix}{ccc}
\frac{16}{5} & \frac{32}{5} & 0 \\ 
\frac{32}{5} & \frac{64}{5} & 0 \\ 
0 & 0 & 0
\end{mymatrix}
\end{equation*}
What are some eigenvalues and eigenvectors? Some computing shows these are

\begin{equation*}
\set{\begin{mymatrix}{c}
0 \\ 
0 \\ 
1
\end{mymatrix} ,\begin{mymatrix}{c}
-\frac{2}{5}\sqrt{5} \\ 
\frac{1}{5}\sqrt{5} \\ 
0
\end{mymatrix} } \leftrightarrow 0,\set{\begin{mymatrix}{c}
\frac{1}{5}\sqrt{5} \\ 
\frac{2}{5}\sqrt{5} \\ 
0
\end{mymatrix} } \leftrightarrow 16
\end{equation*}
Thus the matrix $V$ is given by 
\begin{equation*}
V=\begin{mymatrix}{ccc}
\frac{1}{5}\sqrt{5} & -\frac{2}{5}\sqrt{5} & 0 \\ 
\frac{2}{5}\sqrt{5} & \frac{1}{5}\sqrt{5} & 0 \\ 
0 & 0 & 1
\end{mymatrix}
\end{equation*}
Next consider $AA^T$
\begin{equation*}
\begin{mymatrix}{cc}
8 & 8 \\ 
8 & 8
\end{mymatrix}
\end{equation*}
Eigenvectors and eigenvalues are

\begin{equation*}
\set{\begin{mymatrix}{c}
-\frac{1}{2}\sqrt{2} \\ 
\frac{1}{2}\sqrt{2}
\end{mymatrix} } \leftrightarrow 0,\set{\begin{mymatrix}{c}
\frac{1}{2}\sqrt{2} \\ 
\frac{1}{2}\sqrt{2}
\end{mymatrix} } \leftrightarrow 16
\end{equation*}
Thus you can let $U$ be given by 
\begin{equation*}
U=\begin{mymatrix}{cc}
\frac{1}{2}\sqrt{2} & -\frac{1}{2}\sqrt{2} \\ 
\frac{1}{2}\sqrt{2} & \frac{1}{2}\sqrt{2}
\end{mymatrix}
\end{equation*}
Lets check this. $U^TAV=$ 
\begin{equation*}
\begin{mymatrix}{cc}
\frac{1}{2}\sqrt{2} & \frac{1}{2}\sqrt{2} \\ 
-\frac{1}{2}\sqrt{2} & \frac{1}{2}\sqrt{2}
\end{mymatrix} \begin{mymatrix}{ccc}
\frac{2}{5}\sqrt{2}\sqrt{5} & \frac{4}{5}\sqrt{2}\sqrt{5} & 0 \\ 
\frac{2}{5}\sqrt{2}\sqrt{5} & \frac{4}{5}\sqrt{2}\sqrt{5} & 0
\end{mymatrix} \begin{mymatrix}{ccc}
\frac{1}{5}\sqrt{5} & -\frac{2}{5}\sqrt{5} & 0 \\ 
\frac{2}{5}\sqrt{5} & \frac{1}{5}\sqrt{5} & 0 \\ 
0 & 0 & 1
\end{mymatrix}
\end{equation*}
\begin{equation*}
=\begin{mymatrix}{ccc}
4 & 0 & 0 \\ 
0 & 0 & 0
\end{mymatrix}
\end{equation*}

This illustrates that if you have a good way to find the eigenvectors and
eigenvalues for a Hermitian matrix which has non-negative eigenvalues, then
you also have a good way to find the singular value decomposition of an
arbitrary matrix.
