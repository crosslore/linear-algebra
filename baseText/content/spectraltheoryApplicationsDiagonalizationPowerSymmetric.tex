%%NOTE: This section must follow the sections on Orthogonality
\section{Application: Raising a symmetric matrix to a high power}

We already have seen how to use matrix diagonalization to compute
powers of matrices. This requires computing eigenvalues of the matrix
$A$, and finding an invertible matrix of eigenvectors $P$ such that
$P^{-1}AP$ is diagonal. In this section we will see that if the matrix
$A$ is symmetric (see Definition~\ref{def:symmetric-and-antisymmetric}),
then we can actually find such a matrix $P$ that is an orthogonal
matrix of eigenvectors. Thus $P^{-1}$ is simply its transpose
$P^T$, and $P^TAP$ is diagonal. When this happens we say that $A$ is
\textbf{orthogonally diagonalizable}
\index{matrix!orthogonally diagonalizable}

In fact this happens if and only if $A$ is a symmetric matrix as
shown in the following important theorem.

\begin{theorem}{Principal axis theorem}{principal-axis}
The following conditions are equivalent for an $n \times n$-matrix $A$:
\begin{enumerate}
\item $A$ is symmetric. 
\item $A$ has an orthonormal set of eigenvectors. 
\item $A$ is  orthogonally diagonalizable.
\end{enumerate}
\end{theorem}

\begin{proof}
The complete proof is beyond this course, but to give an idea assume
that $A$ has an orthonormal set of eigenvectors, and let $P$ consist
of these eigenvectors as columns. Then $P^{-1}=P^T$, and $P^TAP=D$ a diagonal matrix. But then $A=PDP^T$, and
\[ A^T=(PDP^T)^T = (P^T)^TD^TP^T=PDP^T=A\]
so $A$ is symmetric. 

Now given a symmetric matrix $A$, one shows that eigenvectors
corresponding to different eigenvalues are always orthogonal. So it
suffices to apply the Gram-Schmidt process on the set of basic
eigenvectors of each eigenvalue to obtain an orthonormal set of eigenvectors. 
\end{proof}

We demonstrate this in the following example. 

\begin{example}{Orthogonal diagonalization of a symmetric matrix}{orthogonal-diagonalization}
Let $A=\begin{mymatrix}{rrr}
1 & 0 & 0 \\
0 & \vspace{0.05in}\frac{3}{2} & \vspace{0.05in}\frac{1}{2} \\
0 & \vspace{0.05in}\frac{1}{2} & \vspace{0.05in}\frac{3}{2}
\end{mymatrix}$. 
Find an orthogonal matrix $P$ such that $P^{T}AP$ is a diagonal
matrix.
\end{example}

\begin{solution}
In this case, verify that the eigenvalues are 2 and 1. First
we will find an eigenvector for the eigenvalue $2$. This involves row
reducing the following augmented matrix. 
\begin{equation*}
\begin{mymatrix}{ccc|c}
2 - 1 & 0 & 0 & 0 \\ 
0 & 2-\vspace{0.05in}\frac{3}{2} & -\vspace{0.05in}\frac{1}{2} & 0 \\ 
0 & -\vspace{0.05in}\frac{1}{2} & 2-\vspace{0.05in}\frac{3}{2} & 0
\end{mymatrix}
\end{equation*}
The {\rref} is 
\begin{equation*}
\begin{mymatrix}{rrr|r}
1 & 0 & 0 & 0 \\ 
0 & 1 & -1 & 0 \\ 
0 & 0 & 0 & 0
\end{mymatrix}
\end{equation*}
and so an eigenvector is 
\begin{equation*}
\begin{mymatrix}{c}
0 \\ 
1 \\ 
1
\end{mymatrix} 
\end{equation*}
Finally to obtain an eigenvector of length one (unit eigenvector) we simply  divide this vector by its length to yield:
\begin{equation*}
\begin{mymatrix}{c}
0 \\ 
\frac{1}{\sqrt{2}} \\ 
\frac{1}{\sqrt{2}}
\end{mymatrix} 
\end{equation*}

Next consider the case of the eigenvalue $1$. To obtain basic eigenvectors, the matrix which needs to be
row reduced in this case is 
\begin{equation*}
\begin{mymatrix}{ccc|c}
1-1 & 0 & 0 & 0 \\ 
0 & 1-\vspace{0.05in}\frac{3}{2} & -\vspace{0.05in}\frac{1}{2} & 0 \\ 
0 & -\vspace{0.05in}\frac{1}{2} & 1-\vspace{0.05in}\frac{3}{2} & 0
\end{mymatrix}
\end{equation*}
The {\rref} is 
\begin{equation*}
\begin{mymatrix}{rrr|r}
0 & 1 & 1 & 0 \\ 
0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0
\end{mymatrix} 
\end{equation*}
Therefore, the eigenvectors are of the form 
\begin{equation*}
\begin{mymatrix}{c}
s \\ 
-t \\ 
t
\end{mymatrix} 
\end{equation*}
Note that all these vectors are automatically orthogonal to
eigenvectors corresponding to the first eigenvalue. This follows from the fact that $A$
is symmetric, as mentioned earlier.

We obtain basic eigenvectors 
\begin{equation*}
\begin{mymatrix}{r}
1 \\ 
0 \\ 
0
\end{mymatrix} \text{ and }\begin{mymatrix}{r}
0 \\ 
-1 \\ 
1
\end{mymatrix} 
\end{equation*}
Since they are themselves orthogonal (by luck here) we do not need to
use the Gram-Schmidt process and instead simply normalize these
vectors to obtain 
\begin{equation*}
\begin{mymatrix}{r}
1 \\ 
0 \\ 
0
\end{mymatrix} \text{ and }\begin{mymatrix}{c}
0 \\ 
-\frac{1}{\sqrt{2}} \\ 
\frac{1}{\sqrt{2}}
\end{mymatrix} 
\end{equation*}
An orthogonal matrix $P$ to orthogonally diagonalize $A$  is then obtained by letting
these basic vectors be the columns. 
\begin{equation*}
P= \begin{mymatrix}{ccc}
0 & 1 & 0 \\ 
-\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}} \\ 
\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}
\end{mymatrix} 
\end{equation*}
We verify this works. $P^{T}AP$ is of the form 
\begin{equation*}
\begin{mymatrix}{ccc}
0 & -\vspace{0.05in}\frac{1}{\sqrt{2}} & \vspace{0.05in}\frac{1}{\sqrt{2}}
\\ 
1 & 0 & 0 \\ 
0 & \vspace{0.05in}\frac{1}{\sqrt{2}} & \vspace{0.05in}\frac{1}{\sqrt{2}}
\end{mymatrix} \begin{mymatrix}{ccc}
1 & 0 & 0 \\ 
0 & \vspace{0.05in}\frac{3}{2} & \vspace{0.05in}\frac{1}{2} \\ 
0 & \vspace{0.05in}\frac{1}{2} & \vspace{0.05in}\frac{3}{2}
\end{mymatrix} \begin{mymatrix}{ccc}
0 & 1 & 0 \\ 
-\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}} \\ 
\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}
\end{mymatrix}
\end{equation*}
\begin{equation*}
=\allowbreak \begin{mymatrix}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 2
\end{mymatrix} 
\end{equation*}
which is the desired diagonal matrix.
\end{solution}

We can now apply this technique to efficiently compute high powers of a symmetric matrix. 

\begin{example}{Powers of a symmetric matrix}{powers-symmetric-matrix}
Let $A=\begin{mymatrix}{rrr}
1 & 0 & 0 \\
0 & \vspace{0.05in}\frac{3}{2} & \vspace{0.05in}\frac{1}{2} \\
0 & \vspace{0.05in}\frac{1}{2} & \vspace{0.05in}\frac{3}{2}
\end{mymatrix}$. 
Compute $A^7$. 
\end{example}

\begin{solution}
We found in Example~\ref{exa:orthogonal-diagonalization} that $P^TAP=D$ is diagonal, where 

\begin{equation*}
P= \begin{mymatrix}{ccc}
0 & 1 & 0 \\ 
-\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}} \\ 
\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}
\end{mymatrix} \text{ and } 
D = \begin{mymatrix}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 2
\end{mymatrix}
\end{equation*}

Thus $A=PDP^T$ and $A^7=PDP^T \; PDP^T \; \cdots PDP^T = PD^7P^T$ which gives:

\[ 
\begin{array}{rr}
A^7 & = 
\begin{mymatrix}{ccc}
0 & 1 & 0 \\ 
-\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}} \\ 
\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}
\end{mymatrix}
\begin{mymatrix}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 2
\end{mymatrix} ^7
\begin{mymatrix}{ccc}
0 & -\vspace{0.05in}\frac{1}{\sqrt{2}} & \vspace{0.05in}\frac{1}{\sqrt{2}}\\ 
1 & 0 & 0 \\ 
0 & \vspace{0.05in}\frac{1}{\sqrt{2}} & \vspace{0.05in}\frac{1}{\sqrt{2}}
\end{mymatrix}  \\
& = 
\begin{mymatrix}{ccc}
0 & 1 & 0 \\ 
-\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}} \\ 
\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}
\end{mymatrix}
\begin{mymatrix}{ccc}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 2^7
\end{mymatrix} 
\begin{mymatrix}{ccc}
0 & -\vspace{0.05in}\frac{1}{\sqrt{2}} & \vspace{0.05in}\frac{1}{\sqrt{2}}\\ 
1 & 0 & 0 \\ 
0 & \vspace{0.05in}\frac{1}{\sqrt{2}} & \vspace{0.05in}\frac{1}{\sqrt{2}}
\end{mymatrix}  \\
& = 
\begin{mymatrix}{ccc}
0 & 1 & 0 \\ 
-\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}} \\ 
\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}
\end{mymatrix}
\begin{mymatrix}{ccc}
0 & -\vspace{0.05in}\frac{1}{\sqrt{2}} & \vspace{0.05in}\frac{1}{\sqrt{2}}\\ 
1 & 0 & 0 \\ 
0 & \vspace{0.05in}\frac{2^7}{\sqrt{2}} & \vspace{0.05in}\frac{2^7}{\sqrt{2}}
\end{mymatrix}  \\
& = 
\begin{mymatrix}{ccc}
1 & 0 & 0 \\ 
0 & \vspace{0.05in}\frac{2^7+1}{2} & \vspace{0.05in}\frac{2^7-1}{2}\\ 
0 & \vspace{0.05in}\frac{2^7-1}{2} & \vspace{0.05in}\frac{2^7+1}{2}
\end{mymatrix}  \\

\end{array}
\]
\end{solution}
