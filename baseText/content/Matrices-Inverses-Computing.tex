% ----------------------------------------------------------------------
\subsection{Computing inverses}
\label{ssec:computing-inverses}

In Example~\ref{exa:verifying-inverse}, we verified that a matrix $A$
had an inverse. But we did not actually compute the inverse: the
inverse $B$ was already given, and we merely checked that $AB=I$ and
$BA=I$. We now explore a method for finding the inverse when it is not
already known what it is.

\begin{example}{Finding the inverse of a matrix}{finding-inverse}
  Find the inverse of the matrix
  \begin{equation*}
    A=\begin{mymatrix}{rr}
      1 & -2 \\
      2 & -3
    \end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  To find $A^{-1}$, we need to find a matrix $\begin{mymatrix}{rr}
    x & z \\
    y & w
  \end{mymatrix} $ such that
  \begin{equation*}
    \begin{mymatrix}{rr}
      1 & -2 \\
      2 & -3
    \end{mymatrix} \begin{mymatrix}{rr}
      x & z \\
      y & w
    \end{mymatrix} =\begin{mymatrix}{rr}
      1 & 0 \\
      0 & 1
    \end{mymatrix}.
  \end{equation*}
  We can multiply these two matrices, and see that in order for this
  equation to be true, we must solve the systems of equations
  \begin{equation*}
    \begin{array}{c}
      x  - 2y = 1, \\
      2x - 3y = 0,
    \end{array}
  \end{equation*}
  and
  \begin{equation*}
    \begin{array}{c}
      z  - 2w = 0, \\
      2z - 3w = 1.
    \end{array}
  \end{equation*}
  Writing the augmented matrix for these two systems gives
  \begin{equation*}
    \begin{mymatrix}{rr|r}
      1 & -2 & 1 \\
      2 & -3 & 0
    \end{mymatrix}
    % \label{inverse-1a}
  \end{equation*}
  for the first system and
  \begin{equation*}
    \begin{mymatrix}{rr|r}
      1 & -2 & 0 \\
      2 & -3 & 1
    \end{mymatrix}
  \end{equation*}
  for the second one. Note that both systems have $A$ as their coefficient
  matrix. Since both systems have the same coefficient matrix, they both
  require exactly the same row operations, and we can use the method of
  Example~\ref{exa:multiple-systems} to solve both systems at the same
  time.  To do so, we create a single augmented matrix containing both of the
  right-hand sides:
  \begin{equation*}
    \begin{mymatrix}{rr|rr}
      1 & -2 & 1 & 0 \\
      2 & -3 & 0 & 1
    \end{mymatrix}.
  \end{equation*}
  Then we perform row operations until the coefficient matrix is in
  {\rref}:
  \begin{equation}\label{eqn:finding-inverse}
    \begin{mymatrix}{rr|rr}
      1 & -2 & 1 & 0 \\
      2 & -3 & 0 & 1
    \end{mymatrix}
    \stackrel{R_2\rowop R_2-2R_1}{\roweq}
    \begin{mymatrix}{rr|rr}
      1 & -2 &  1 & 0 \\
      0 &  1 & -2 & 1
    \end{mymatrix}
    \stackrel{R_1\rowop R_1+2R_2}{\roweq}
    \begin{mymatrix}{rr|rr}
      1 & 0 & -3 & 2 \\
      0 & 1 & -2 & 1
    \end{mymatrix}.
  \end{equation}
  This corresponds to the following {\rref}s for the two original
  systems of equations:
  \begin{equation*}
    \begin{mymatrix}{rr|r}
      1 & 0 & -3 \\
      0 & 1 & -2
    \end{mymatrix}
    \quad\mbox{and}\quad
    \begin{mymatrix}{rr|rr}
      1 & 0 & 2 \\
      0 & 1 & 1
    \end{mymatrix}.
  \end{equation*}
  The solution of the first system is $x=-3$ and $y=-2$. The solution
  for the second system is $z=2$ and $w=1$. If we take the values
  found for $x$, $y$, $z$, and $w$ and put them into our inverse
  matrix, we see that the inverse is
  \begin{equation*}
    A^{-1} =
    \begin{mymatrix}{rr}
      x & z \\
      y & w
    \end{mymatrix}
    =
    \begin{mymatrix}{rr}
      -3 & 2 \\
      -2 & 1
    \end{mymatrix}.
  \end{equation*}
  Notice that this is exactly the right-hand side in the last
  augmented matrix of {\eqref{eqn:finding-inverse}}. In other words,
  all we really had to do to find the inverses were the row operations
  in {\eqref{eqn:finding-inverse}}. The inverse can be read off
  directly from the result.
\end{solution}

The example suggests a general method for finding the inverse of a
matrix, which we summarize in the following algorithm.

\begin{algorithm}{Finding the inverse of a matrix}{matrix-inversion-algorithm}
  Suppose $A$ is an $n\times n$-matrix. To find $A^{-1}$ if it
  exists\index{matrix!finding the inverse}, form the augmented
  $n\times 2n$-matrix
  \begin{equation*}
    \mat{A\mid I}.
  \end{equation*}
  If possible, do row operations until you obtain an
  $n\times 2n$-matrix of the form
  \begin{equation*}
    \mat{I\mid B}.
  \end{equation*}
  If this can be done, then $A$ is invertible and $A^{-1}=B$. If it is
  not possible (i.e., if the {\rref} of $A$ has less than
  $n$ pivot entries), then $A$ is not invertible.
\end{algorithm}

This algorithm shows how to find the inverse if it exists. It also
tells us if $A$ does not have an inverse.

\begin{example}{Finding the inverse of a matrix}{finding-inverse2}
  Let $A=\begin{mymatrix}{rrr}
    1 & 2 & 2 \\
    1 & 0 & 2 \\
    3 & 1 & -1
  \end{mymatrix}$. Find $A^{-1}$ if it exists.
\end{example}

\begin{solution}
  We set up the augmented matrix and reduce it to {\rref}.
  \begin{eqnarray*}
    \mat{A\mid I} &=&
    \begin{mymatrix}{rrr|rrr}
      1 & 2 &  2 & 1 & 0 & 0 \\
      1 & 0 &  2 & 0 & 1 & 0 \\
      3 & 1 & -1 & 0 & 0 & 1
    \end{mymatrix}\\[1ex]
    &\stackrel{R_2\rowop R_2-R_1}{\stackrel{R_3\rowop R_3-3R_1}{\roweq}}&
    \begin{mymatrix}{rrr|rrr}
      1 &  2 &  2 &  1 & 0 & 0 \\
      0 & -2 &  0 & -1 & 1 & 0 \\
      0 & -5 & -7 & -3 & 0 & 1
    \end{mymatrix}\\[1ex]
    &\stackrel{R_1\rowop 7R_1}{\stackrel{R_3\rowop -2R_3}{\roweq}}&
    \begin{mymatrix}{rrr|rrr}
      7 & 14 & 14 &  7 & 0 &  0 \\
      0 & -2 &  0 & -1 & 1 &  0 \\
      0 & 10 & 14 &  6 & 0 & -2
    \end{mymatrix}\\[1ex]
    &\stackrel{R_1\rowop R_1+7R_2}{\stackrel{R_3\rowop R_3+5R_2}{\roweq}}&
    \begin{mymatrix}{rrr|rrr}
      7 &  0 & 14 &  0 & 7 &  0 \\
      0 & -2 &  0 & -1 & 1 &  0 \\
      0 &  0 & 14 &  1 & 5 & -2
    \end{mymatrix}\\[1ex]
    &\stackrel{R_1\rowop R_1-R_3}{\roweq}&
    \begin{mymatrix}{rrr|rrr}
      7 &  0 &  0 & -1 & 2 &  2 \\
      0 & -2 &  0 & -1 & 1 &  0 \\
      0 &  0 & 14 &  1 & 5 & -2
    \end{mymatrix}\\[-1ex]
    &\stackrel{R_1\rowop \frac{1}{7}R_1}{\stackrel{R_2\rowop -\frac{1}{2}R_2}{\stackrel{R_3\rowop \frac{1}{14}R_3}{\roweq}}}&
    \def\arraystretch{1.5}
    \begin{mymatrix}{rrr|rrr}
      1 & 0 & 0 & -\frac{1}{7} & \frac{2}{7} & \frac{2}{7} \\
      0 & 1 & 0 & \frac{1}{2} & -\frac{1}{2} & 0           \\
      0 & 0 & 1 & \frac{1}{14} & \frac{5}{14} & -\frac{1}{7}
    \end{mymatrix}.
  \end{eqnarray*}
  Notice that the last augmented matrix is of the form
  $\mat{I\mid B}$, where the left-hand side is the $3 \times 3$
  identity matrix.  Therefore, the inverse is the $3 \times 3$-matrix
  on the right-hand side, given by
  \begin{equation*}
    A^{-1} ~=~
    \def\arraystretch{1.5}
    \begin{mymatrix}{rrr}
      -\frac{1}{7} & \frac{2}{7} & \frac{2}{7} \\
      \frac{1}{2} & -\frac{1}{2} & 0 \\
      \frac{1}{14} & \frac{5}{14} & -\frac{1}{7}
    \end{mymatrix}.
  \end{equation*}
\end{solution}

When looking for the inverse of a matrix, it can happen that the
left-hand side cannot be row reduced to the identity matrix. The
following is an example of this situation.

\begin{example}{A non-invertible matrix}{matrix-no-inverse}
  Let $A=\begin{mymatrix}{rrr}
    1 & -2 & 2 \\
    1 &  0 & 2 \\
    2 & -2 & 4
  \end{mymatrix}$. Find $A^{-1}$ if it exists.%
  \index{matrix!inverse!does not exist}%
  \index{inverse!of a matrix!does not exist}
\end{example}

\begin{solution}
  We write the augmented matrix
  \begin{equation*}
    \mat{A\mid I}
    ~=~
    \begin{mymatrix}{rrr|rrr}
      1 & -2 & 2 & 1 & 0 & 0 \\
      1 &  0 & 2 & 0 & 1 & 0 \\
      2 & -2 & 4 & 0 & 0 & 1
    \end{mymatrix}
  \end{equation*}
  and proceed to do row operations attempting to obtain
  $\mat{I\mid A^{-1}}$. After a few row operations, we have
  \begin{equation*}
    \begin{mymatrix}{rrr|rrr}
      \circled{1} & -2 & 2 & 1 & 0 & 0 \\
      0 & \circled{2} & 0 & -1 & 1 & 0 \\
      0 & 0 & 0 & -1 & -1 & 1
    \end{mymatrix}.
  \end{equation*}
  At this point, we see that the coefficient matrix has rank $2$,
  i.e., there are only two pivot entries. This means there is no way
  to obtain $I$ on the left-hand side of this augmented matrix.
  Hence, there is no way to complete the algorithm, and the inverse of
  $A$ does not exist.
\end{solution}

If the algorithm provides an inverse, it is always possible to
double-check that your answer is correct.  To do so, use the method
demonstrated in Example~\ref{exa:verifying-inverse}. Check that the
products $AA^{-1}$ and $A^{-1}A$ both equal the identity
matrix. Through this method, you can always ensure that you have
calculated $A^{-1}$ properly.

% ----------------------------------------------------------------------
\subsection{Using the inverse to solve a system of equations}

One way in which the inverse of a matrix is useful is to find the
solution of a system of linear equations.  Recall from Definition~\ref{def:matrix-form} that we can write a system of equations in
matrix form, which is in the form
\begin{equation*}
  A\vect{x}=\vect{b}.
\end{equation*}
Suppose we find the inverse $A^{-1}$ of the matrix $A$. Then we can
multiply both sides of this equation by $A^{-1}$ on the left and
simplify to obtain
\begin{equation*}
  \vect{x} = A^{-1}\vect{b}.
\end{equation*}
Therefore we can find $\vect{x}$, the solution to the system, by
computing $\vect{x} = A^{-1}\vect{b}$. Note that once we have found
$A^{-1}$, we can easily get the solution for different right-hand
sides (different $\vect{b}$). It is always just $A^{-1}\vect{b}$.

\begin{example}{Using the inverse to solve a system of equations}{inverse-to-solve-system}
  Consider the following system of equations. Use the inverse of a
  suitable matrix to solve this system.
  \begin{equation*}
    \begin{array}{c}
      x+z=1 \\
      x-y+z=3 \\
      x+y-z=2
    \end{array}
  \end{equation*}
\end{example}

\begin{solution}
  First, we can write the system in matrix form
  \begin{equation*}
    A\vect{x} =
    \begin{mymatrix}{rrr}
      1 & 0 & 1 \\
      1 & -1 & 1 \\
      1 & 1 & -1
    \end{mymatrix} \begin{mymatrix}{r}
      x \\
      y \\
      z
    \end{mymatrix} =\begin{mymatrix}{r}
      1 \\
      3 \\
      2
    \end{mymatrix} = \vect{b}.
  \end{equation*}
  The inverse of $A$ is
  \begin{equation*}
    A^{-1} =
    \def\arraystretch{1.2}
    \begin{mymatrix}{rrr}
      0 & \frac{1}{2} & \frac{1}{2} \\
      1 & -1 & 0 \\
      1 & -\frac{1}{2} & -\frac{1}{2}
    \end{mymatrix}.
  \end{equation*}
  From here, the solution to the system $A\vect{x}=\vect{b}$ is found
  by $\vect{x}=A^{-1}\vect{b}$, i.e.,
  \begin{equation*}
    \def\arraystretch{1.2}
    \begin{mymatrix}{r}
      x \\
      y \\
      z
    \end{mymatrix}
    =
    \begin{mymatrix}{rrr}
      0 & \frac{1}{2} & \frac{1}{2} \\
      1 & -1 & 0 \\
      1 & -\frac{1}{2} & -\frac{1}{2}
    \end{mymatrix} \begin{mymatrix}{r}
      1 \\
      3 \\
      2
    \end{mymatrix} =\begin{mymatrix}{r}
      \frac{5}{2} \\
      -2 \\
      -\frac{3}{2}
    \end{mymatrix}.
  \end{equation*}
\end{solution}

What if the right-hand side had been $\vect{b}=\begin{mymatrix}{r}
  0 \\
  1 \\
  3
\end{mymatrix}$? In this case, the solution would be given by
\begin{equation*}
  \def\arraystretch{1.2}
  \begin{mymatrix}{r}
    x \\
    y \\
    z
  \end{mymatrix} = \begin{mymatrix}{rrr}
    0 & \frac{1}{2} & \frac{1}{2} \\
    1 & -1 & 0 \\
    1 & -\frac{1}{2} & -\frac{1}{2}
  \end{mymatrix} \begin{mymatrix}{r}
    0 \\
    1 \\
    3
  \end{mymatrix} =\begin{mymatrix}{r}
    2 \\
    -1 \\
    -2
  \end{mymatrix}.
\end{equation*}
This illustrates that for a system $A\vect{x}=\vect{b}$ where $A^{-1}$ exists,
it is easy to find the solution when the vector $\vect{b}$ is changed.

% ----------------------------------------------------------------------
\subsection{Properties of the inverse}

The following are some algebraic properties of matrix inverses.

\begin{theorem}{Properties of the inverse}{inverse-properties}
  Let $A$ and $B$ be $n \times n$-matrices, $I$ the $n\times
  n$-identity matrix. Then the following hold.
  \begin{enumerate}
  \item $I$ is invertible and $I^{-1} = I$.
  \item If $A$ and $B$ are invertible then $AB$ is invertible, and
    $(AB)^{-1} = B^{-1}A^{-1}$.
  \item If $A$ is invertible then so is $A^{-1}$, and
    $(A^{-1})^{-1} = A$.
  \item If $A$ is invertible then so is $A^k$, and
    $(A^k)^{-1} = (A^{-1})^k$.
  \item If $A$ is invertible and $p$ is a non-zero scalar, then $pA$
    is invertible and $(pA)^{-1} = \frac{1}{p}A^{-1}$.
  \end{enumerate}
\end{theorem}

% ----------------------------------------------------------------------
\subsection{Right and left inverses}

So far, we have only talked about the inverses of square matrices. But
what about matrices that are not square? Can they be invertible? It
turns out that non-square matrices can never be invertible. However,
they can have left inverses or right inverses.

\begin{definition}{Left and right inverses}{left-and-right-inverse}
  Let $A$ be an $m\times n$-matrix and $B$ an $n\times m$-matrix.
  We say that $B$ is a \textbf{left inverse}%
  \index{inverse!left inverse}\index{left inverse}%
  \index{matrix!left inverse}\index{matrix!inverse!left inverse} of
  $A$ if
  \begin{equation*}
    BA=I.
  \end{equation*}
  We say that $B$ is a \textbf{right inverse}%
  \index{inverse!right inverse}\index{right inverse}%
  \index{matrix!right inverse}\index{matrix!inverse!right inverse} of
  $A$ if
  \begin{equation*}
    AB=I.
  \end{equation*}
  If $A$ has a left inverse, we also say that $A$ is
  \textbf{left invertible}. Similarly, if $A$ has a right inverse, we
  say that $A$ is \textbf{right invertible}.
\end{definition}

\begin{example}{Right inverse}{right-inverse}
  Let
  \begin{equation*}
    A = \begin{mymatrix}{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
    \end{mymatrix}
    \quad\mbox{and}\quad
    B = \begin{mymatrix}{rr}
      1 & 0 \\
      0 & 1 \\
      0 & 0 \\
    \end{mymatrix}.
  \end{equation*}
  Show that $B$ is a right inverse, but not a left inverse, of $A$.
\end{example}

\begin{solution}
  We compute
  \begin{equation*}
    AB
    ~=~ \begin{mymatrix}{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
    \end{mymatrix}
    \begin{mymatrix}{rr}
      1 & 0 \\
      0 & 1 \\
      0 & 0 \\
    \end{mymatrix}
    ~=~ \begin{mymatrix}{rrr}
      1 & 0 \\
      0 & 1 \\
    \end{mymatrix}
    ~=~ I,
  \end{equation*}
  \begin{equation*}
    BA
    ~=~ \begin{mymatrix}{rr}
      1 & 0 \\
      0 & 1 \\
      0 & 0 \\
    \end{mymatrix}
    \begin{mymatrix}{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
    \end{mymatrix}
    ~=~ \begin{mymatrix}{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 0 \\
    \end{mymatrix}
    ~\neq~ I.
  \end{equation*}
  Therefore, $B$ is a right inverse, but not a left inverse, of $A$.
\end{solution}

Recall from Definition~\ref{def:invertible-matrix} that $B$ is called
an \textbf{inverse}%
\index{inverse!of a matrix}\index{matrix!inverse} of $A$ if it is both
a left inverse and a right inverse. A crucial fact is that invertible
matrices are always square.

\begin{theorem}{Invertible matrices are square}{invertible-square}
  Let $A$ be an $m\times n$-matrix.
  \begin{itemize}
  \item If $A$ is left invertible, then $m\geq n$.
  \item If $A$ is right invertible, then $m\leq n$.
  \item If $A$ is invertible, then $m=n$.
  \end{itemize}
  In particular, only square matrices can be invertible.
\end{theorem}

\begin{proof}
  To prove the first claim, assume that $A$ is left invertible, i.e.,
  assume that $BA=I$ for some $n\times m$-matrix $B$. We must show
  that $m\geq n$. Assume, for the sake of obtaining a contradiction,
  that this is not the case, i.e., that $m<n$. Then the matrix $A$ has
  more columns than rows. It follows that the homogeneous system of
  equations $A\vect{x}=\vect{0}$ has a non-trivial solution; let
  $\vect{x}$ be such a solution. We obtain a contradiction by a
  similar method as in
  Example~\ref{exa:non-invertible-matrix}. Namely, we have
  \begin{equation*}
    \vect{x} ~=~ I\,\vect{x} ~=~ (BA)\vect{x} ~=~ B(A\vect{x}) ~=~ B\vect{0} ~=~
    \vect{0},
  \end{equation*}
  contradicting the fact that $\vect{x}$ was non-trivial.  Since we
  got a contradiction from the assumption that $m<n$, it follows that
  $m\geq n$.

  The second claim is proved similarly, but exchanging the roles of
  $A$ and $B$.  The third claim follows directly from the first two
  claims, because every invertible matrix is both left and right
  invertible.
\end{proof}

Of course, not all square matrices are invertible. In particular, zero
matrices are not invertible, along with many other square matrices.
