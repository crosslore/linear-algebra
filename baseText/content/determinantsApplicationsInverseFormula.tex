\section{Application: A formula for the inverse of a matrix}

The determinant of a matrix also provides a way to find the inverse of
a matrix.  Recall the definition of the inverse of a matrix from
Definition~\ref{def:invertible-matrix}. If $A$ is an
$n\times n$-matrix, we say that $A^{-1}$ is the inverse of $A$ if
$AA^{-1} = I$ and $A^{-1}A=I$.

We now define a new matrix called the \textbf{cofactor matrix} of $A$.
The cofactor matrix of $A$ is the matrix whose $\ijth$ entry is the
$\ijth$ cofactor of $A$.

\begin{definition}{The cofactor matrix}{cofactor-matrix}
  Let $A$ be an $n\times n$-matrix. Then the \textbf{cofactor matrix
    of $A$}%
  \index{cofactor matrix}%
  \index{matrix!cofactor matrix}, denoted $\cof(A)$, is defined by
  \begin{equation*}
    \cof(A)
    ~=~ \mat{\cofactor{A}{ij}}
    ~=~ \begin{mymatrix}{cccc}
      \cofactor{A}{11} & \cofactor{A}{12} & \cdots & \cofactor{A}{1n} \\
      \cofactor{A}{21} & \cofactor{A}{22} & \cdots & \cofactor{A}{2n} \\
      \vdots & \vdots & \ddots & \vdots \\
      \cofactor{A}{n1} & \cofactor{A}{n2} & \cdots & \cofactor{A}{nn} \\
    \end{mymatrix},
  \end{equation*}
  where $\cofactor{A}{ij}$ is the $\ijth$ cofactor of $A$.
\end{definition}

We will use the cofactor matrix to create a formula for the inverse of
$A$. First, we define the \textbf{adjugate}%
\index{adjugate}%
\index{matrix!adjugate} of $A$, denoted $\adj(A)$, to be the transpose
of the cofactor matrix:
\begin{equation*}
  \adj(A) = \cof(A)^T.
\end{equation*}
The adjugate is also sometimes called the 
\textbf{classical adjoint}%
\index{classical adjoint}%
\index{matrix!classical adjoint} of $A$.

\begin{example}{Cofactor matrix and adjugate}{cofactor-matrix-and-adjugate}
  Find the cofactor matrix and the adjugate of $A$, where
  \begin{equation*}
    A = \begin{mymatrix}{rrr}
      1 & 2 & 3 \\
      3 & 0 & 1 \\
      1 & 2 & 1 \\
    \end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  We first find $\adj(A)$. To do so, we need to compute the cofactors
  of $A$. We have:
  \begin{eqnarray*}
    \cofactor{A}{11} ~=~ +\minor{A}{11}
    &=&
    \begin{absmatrix}{ccc}
      \strikeh{3.2em}{\strikev{3.2em}{1}} & 2 & 3 \\
      3 & 0 & 1 \\
      1 & 2 & 1 \\
    \end{absmatrix}
    ~=~ \begin{absmatrix}{rr}
      0 & 1 \\
      2 & 1 \\
    \end{absmatrix}
    ~=~ -2,
    \\
    \cofactor{A}{12} ~=~ -\minor{A}{12}
    &=&
    -\begin{absmatrix}{ccc}
      \strikeh{3.2em}{1} & \strikev{3.2em}{2} & 3 \\
      3 & 0 & 1 \\
      1 & 2 & 1 \\
    \end{absmatrix}
    ~=~ -\begin{absmatrix}{rr}
      3 & 1 \\
      1 & 1 \\
    \end{absmatrix}
    ~=~ -2,
    \\
    \cofactor{A}{13} ~=~ +\minor{A}{13}
    &=&
    \begin{absmatrix}{ccc}
      \strikeh{3.2em}{1} & 2 & \strikev{3.2em}{3} \\
      3 & 0 & 1 \\
      1 & 2 & 1 \\
    \end{absmatrix}
    ~=~ \begin{absmatrix}{rr}
      3 & 0 \\
      1 & 2 \\
    \end{absmatrix}
    ~=~ 6,
    \\
    \cofactor{A}{21} ~=~ -\minor{A}{21}
    &=&
    -\begin{absmatrix}{ccc}
      \strikev{3.2em}{1} & 2 & 3 \\
      \strikeh{3.2em}{3} & 0 & 1 \\
      1 & 2 & 1 \\
    \end{absmatrix}
    ~=~ -\begin{absmatrix}{rr}
      2 & 3 \\
      2 & 1 \\
    \end{absmatrix}
    ~=~ 4,
  \end{eqnarray*}
  and so on. Continuing in this way, we find the cofactor matrix
  \begin{equation*}
    \cof(A) 
    =
    \begin{mymatrix}{rrr}
      -2 & -2 & 6 \\
      4 & -2 & 0 \\
      2 & 8 & -6
    \end{mymatrix}.
  \end{equation*}
  Finally, the adjugate is the transpose of the cofactor matrix:
  \begin{equation*}
    \adj(A) = \cof(A)^T = 
    \begin{mymatrix}{rrr}
      -2 & 4 & 2 \\
      -2 & -2 & 8 \\
      6 & 0 & -6
    \end{mymatrix}.
  \end{equation*}  
\end{solution}

The following theorem provides a formula for $A^{-1}$ using the
determinant and the adjugate of $A$.

\begin{theorem}{Formula for the inverse}{inverse-and-determinant}
  Let $A$ be an $n\times n$-matrix. Then
  \begin{equation*}
    A \, \adj(A) = \adj(A)\,A = \det(A)\,I.
  \end{equation*}
  Moreover, if $A$ is invertible if and only if $\det(A) \neq 0$. In this
  case%
  \index{determinant!matrix inverse formula}%
  \index{inverse!of a matrix!formula for}%
  \index{matrix!inverse!formula for},
  we have:
  \begin{equation*}
    A^{-1} = \frac{1}{\det(A)} \adj(A).
  \end{equation*}
\end{theorem}

\begin{proof}
  Recall that the $(i,j)$-entry of $\adj(A)$ is equal to
  $\cofactor{A}{ji}$.  Thus the $(i,j)$-entry of $B=A\,\adj(A)$ is:
  \begin{eqnarray*}
    B_{ij}
    &=& a_{i1}\adj(A)_{1j} + a_{i2}\adj(A)_{2j} + \ldots + a_{in}\adj(A)_{nj} \\
    &=& a_{i1}\cofactor{A}{j1} + a_{i2}\cofactor{A}{j2} + \ldots + a_{in}\cofactor{A}{jn}.
  \end{eqnarray*}
  By the cofactor expansion theorem, we see that this expression for
  $B_{ij}$ is equal to the determinant of the matrix obtained from $A$
  by replacing its $j$th row by $\mat{a_{i1}, a_{i2}, \dots a_{in}}$, 
  i.e., by its $i$th row.

  If $i=j$ then this matrix is $A$ itself and therefore
  $B_{ii}=\det(A)$. If on the other hand $i\neq j$, then this matrix
  has its $i$th row equal to its $j$th row, and therefore $B_{ij}=0$
  in his case. Thus we obtain:
  \begin{equation*}
    A \, \adj(A) = {\det(A)} I.
  \end{equation*}
  By a similar argument (using columns instead of rows), we can verify that:
  \begin{equation*}
    \adj(A)\,A = {\det(A)} I.
  \end{equation*}
  This proves the first part of the theorem. For the second part,
  assume that $A$ is invertible. Then by
  Theorem~\ref{thm:determinant-invertible}, $\det(A)\neq 0$. Dividing the
  formula from the first part of the theorem by $\det(A)$, we obtain
  \begin{equation*}
    A\paren{\frac{1}{\det(A)}\adj(A)} ~=~ \paren{\frac{1}{\det(A)}\adj(A)}A ~=~ I,
  \end{equation*}
  and therefore 
  \begin{equation*}
    A^{-1} = \frac{1}{\det(A)} \adj(A).
  \end{equation*}
  This completes the proof.
\end{proof}


% ======================================================================
\subsection{CONTINUE HERE...}

In the specific case where $A$ is a $2 \times 2$-matrix given by
\begin{equation*}
  A = \begin{mymatrix}{rr}
    a & b \\
    c & d
  \end{mymatrix}
\end{equation*}
then $\adj(A)$ is given by
\begin{equation*}
  \adj(A) = 
  \begin{mymatrix}{rr}
    d & -b \\
    -c & a
  \end{mymatrix}
\end{equation*}
In general, $\adj(A)$ can always be found by taking the
transpose of the cofactor matrix of $A$. 

% The proof of this Theorem is below, after two examples demonstrating this concept. 

Notice that the first formula holds for any $n \times n$-matrix $A$,
and in the case $A$ is invertible we actually have a formula for
$A^{-1}$.

Consider the following example.

\begin{example}{Find inverse using the determinant}{inverse-and-determinant}
  Find the inverse of the matrix
  \begin{equation*}
    A=\begin{mymatrix}{rrr}
      1 & 2 & 3 \\
      3 & 0 & 1 \\
      1 & 2 & 1
    \end{mymatrix}
  \end{equation*}
  using the formula in Theorem~\ref{thm:inverse-and-determinant}.
\end{example}

\begin{solution} 
  According to Theorem~\ref{thm:inverse-and-determinant}, 
  \begin{equation*}
    A^{-1} = \frac{1}{\det(A)} \adj(A)
  \end{equation*}
  First we will find the determinant of this matrix. Using
  Theorems~\ref{thm:switching-rows},
  {\ref{thm:multiplying-row-by-scalar}}, and
  {\ref{thm:adding-multiple-of-row}}, we can first simplify the matrix
  through row operations. First, add $-3$ times the first row to the
  second row. Then add $-1$ times the first row to the third row to
  obtain
  \begin{equation*}
    B = \begin{mymatrix}{rrr}
      1 & 2 & 3 \\
      0 & -6 & -8 \\
      0 & 0 & -2
    \end{mymatrix}
  \end{equation*}
  By Theorem~\ref{thm:adding-multiple-of-row}, $\det(A) = \det
  (B)$. By Theorem~\ref{thm:determinant-of-triangular-matrix},
  $\det(B) = 1 \times -6 \times -2 = 12$. Hence, $\det(A) = 12$.

  ...

  Therefore, from
  Theorem~\ref{thm:inverse-and-determinant}, the inverse of $A$ is
  given by
  \begin{equation*}
    \def\arraystretch{1.4}
    A^{-1}
    =
    \frac{1}{12}\begin{mymatrix}{rrr}
      -2 & -2 & 6 \\
      4 & -2 & 0 \\
      2 & 8 & -6
    \end{mymatrix} ^{T}=\allowbreak \begin{mymatrix}{rrr}
      -\frac{1}{6} & \frac{1}{3} & 
      \frac{1}{6} \\
      -\frac{1}{6} & -\frac{1}{6} & 
      \frac{2}{3} \\
      \frac{1}{2} & 0 & -\frac{1}{2}
    \end{mymatrix} 
  \end{equation*}
  Remember that we can always verify our answer for $A^{-1}$. Compute
  the product $AA^{-1}$ and $A^{-1}A$ and make sure each product is
  equal to $I$.

  Compute $A^{-1}A$ as follows
  \begin{equation*}
    \def\arraystretch{1.4}
    A^{-1}A = 
    \allowbreak \begin{mymatrix}{rrr}
      -\frac{1}{6} & \frac{1}{3} & 
      \frac{1}{6} \\
      -\frac{1}{6} & -\frac{1}{6} & 
      \frac{2}{3} \\
      \frac{1}{2} & 0 & -\frac{1}{2}
    \end{mymatrix} \begin{mymatrix}{rrr}
      1 & 2 & 3 \\
      3 & 0 & 1 \\
      1 & 2 & 1
    \end{mymatrix} = \begin{mymatrix}{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{mymatrix}
    =
    I
  \end{equation*}
  You can verify that $AA^{-1} = I$ and hence our answer is correct. 
\end{solution} 

We will look at another example of how to use this formula to find
$A^{-1}$.

\begin{example}{Find the inverse from a formula}{inverse-formula}
  Find the inverse of the matrix
  \begin{equation*}
    \def\arraystretch{1.4}
    A=\begin{mymatrix}{rrr}
      \frac{1}{2} & 0 & \frac{1}{2} \\
      -\frac{1}{6} & \frac{1}{3} & -
      \frac{1}{2} \\
      -\frac{5}{6} & \frac{2}{3} & -
      \frac{1}{2}
    \end{mymatrix}
  \end{equation*}
  using the formula given in Theorem~\ref{thm:inverse-and-determinant}.
\end{example}

\begin{solution} First we need to find $\det(A)$.  This step is left
  as an exercise and you should verify that
  $\det(A) = \frac{1}{6}$.  The inverse is therefore
  equal to
  \begin{equation*}
    A^{-1}
    =
    \frac{1}{(1/6)}\;
    \adj(A)
    =
    6\;
    \adj(A)
  \end{equation*}
  We continue to calculate as follows. Here we show the $2 \times 2$
  determinants needed to find the cofactors.
  \begin{equation*}
    \def\arraystretch{1.4}
    A^{-1}
    =
    6\begin{mymatrix}{rrr}
      \begin{absmatrix}{rr}
        \frac{1}{3} & -\frac{1}{2} \\
        \frac{2}{3} & -\frac{1}{2}
      \end{absmatrix}
      &
      -\begin{absmatrix}{rr}
        -\frac{1}{6} & -\frac{1}{2} \\
        -\frac{5}{6} & -\frac{1}{2}
      \end{absmatrix}
      &
      \begin{absmatrix}{rr}
        -\frac{1}{6} & \frac{1}{3} \\
        -\frac{5}{6} & \frac{2}{3}
      \end{absmatrix}
      \\\\[-2ex]
      -\begin{absmatrix}{rr}
        0 & \frac{1}{2} \\
        \frac{2}{3} & -\frac{1}{2}
      \end{absmatrix}
      &
      \begin{absmatrix}{rr}
        \frac{1}{2} & \frac{1}{2} \\
        -\frac{5}{6} & -\frac{1}{2}
      \end{absmatrix}
      &
      -\begin{absmatrix}{rr}
        \frac{1}{2} & 0 \\
        -\frac{5}{6} & \frac{2}{3}
      \end{absmatrix}
      \\\\[-2ex]
      \begin{absmatrix}{rr}
        0 & \frac{1}{2} \\
        \frac{1}{3} & -\frac{1}{2}
      \end{absmatrix}
      &
      -\begin{absmatrix}{rr}
        \frac{1}{2} & \frac{1}{2} \\
        -\frac{1}{6} & -\frac{1}{2}
      \end{absmatrix}
      &
      \begin{absmatrix}{rr}
        \frac{1}{2} & 0 \\
        -\frac{1}{6} & \frac{1}{3}
      \end{absmatrix}
    \end{mymatrix} ^{T}
  \end{equation*}
  Expanding all the $2\times 2$ determinants, this yields
  \begin{equation*}
    \def\arraystretch{1.4}
    A^{-1}
    =
    6\begin{mymatrix}{rrr}
      \frac{1}{6} & \frac{1}{3} & \frac{1}{6} \\
      \frac{1}{3} & \frac{1}{6} & -\frac{1}{3} \\
      -\frac{1}{6} & \frac{1}{6} & \frac{1}{6}
    \end{mymatrix} ^{T}= \begin{mymatrix}{rrr}
      1 & 2 & -1 \\
      2 & 1 & 1 \\
      1 & -2 & 1
    \end{mymatrix}
  \end{equation*}
  Again, you can always check your work by multiplying $A^{-1}A$ and
  $AA^{-1}$ and ensuring these products equal $I$.
  \begin{equation*}
    \def\arraystretch{1.4}
    A^{-1}A = 
    \begin{mymatrix}{rrr}
      1 & 2 & -1 \\
      2 & 1 & 1 \\
      1 & -2 & 1
    \end{mymatrix} \begin{mymatrix}{rrr}
      \frac{1}{2} & 0 & \frac{1}{2} \\
      -\frac{1}{6} & \frac{1}{3} & -
      \frac{1}{2} \\
      -\frac{5}{6} & \frac{2}{3} & -
      \frac{1}{2}
    \end{mymatrix} = \begin{mymatrix}{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \end{mymatrix}
  \end{equation*}
  This tells us that our calculation for $A^{-1}$ is correct. It is
  left to the reader to verify that $AA^{-1} = I$.
\end{solution}

The verification step is very important, as it is a simple way to
check your work! If you multiply $A^{-1}A$ and $AA^{-1}$ and these
products are not both equal to $I$, be sure to go back and double
check each step.  One common error is to forget to take the transpose
of the cofactor matrix, so be sure to complete this step.

This method for finding the inverse of $A$ is useful in many
contexts. In particular, it is useful with complicated matrices where
the entries are functions, rather than numbers.

Consider the following example.

\begin{example}{Inverse for non-constant matrix}{inverse-non-constant-matrix}
  Suppose
  \begin{equation*}
    A(t) =\begin{mymatrix}{ccc}
      e^{t} & 0 & 0 \\
      0 & \cos t & \sin t \\
      0 & -\sin t & \cos t
    \end{mymatrix}
  \end{equation*}
  Show that $A(t) ^{-1}$ exists and then find it.
\end{example}

\begin{solution}
  First note $\det(A(t)) = e^{t}(\cos^2 t + \sin^2 t) = e^{t}\neq 0$
  so $A(t) ^{-1}$ exists.

  The cofactor matrix is
  \begin{equation*}
    C(t) =\begin{mymatrix}{ccc}
      1 & 0 & 0 \\
      0 & e^{t}\cos t & e^{t}\sin t \\
      0 & -e^{t}\sin t & e^{t}\cos t
    \end{mymatrix}
  \end{equation*}
  and so the inverse is
  \begin{equation*}
    \frac{1}{e^{t}}\begin{mymatrix}{ccc}
      1 & 0 & 0 \\
      0 & e^{t}\cos t & e^{t}\sin t \\
      0 & -e^{t}\sin t & e^{t}\cos t
    \end{mymatrix} ^{T}= \begin{mymatrix}{ccc}
      e^{-t} & 0 & 0 \\
      0 & \cos t & -\sin t \\
      0 & \sin t & \cos t
    \end{mymatrix} 
  \end{equation*}
\end{solution}
