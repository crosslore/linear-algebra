\section{Application: A formula for the inverse of a matrix}

The determinant of a matrix also provides a way to find the inverse of a matrix.
Recall the definition of the inverse of a matrix in Definition~\ref{def:invertible-matrix}.
We say that $A^{-1}$, an $n \times n$-matrix, is the inverse of $A$, also $n \times n$, if $AA^{-1} = I$ and $A^{-1}A=I$. 

We now define a new matrix called the \textbf{cofactor matrix} of $A$.  
The cofactor matrix of $A$ is the matrix whose $\ijth$ entry is the $\ijth$ cofactor of $A$.
The formal definition is as follows.

\begin{definition}{The cofactor matrix}{cofactor-matrix}
Let $A=\mat{a_{ij}}$ be an $n\times n$-matrix. Then the
\textbf{cofactor matrix of $A$}\index{cofactor matrix}, denoted 
$\func{cof}(A)$, is defined by $\func{cof}(A) =\mat{
\cof{A}{ij}} $ where  $\cof{A}{ij}$ is the $\ijth$ cofactor of $A$.
\end{definition}

Note that $\cof{A}{ij}$ denotes the $\ijth$ entry of the cofactor matrix.

We will use the cofactor matrix to create a formula for the inverse of $A$. First,
we define the \textbf{adjugate}\index{adjugate} of $A$ to be the transpose of 
the cofactor matrix. We can also call this matrix the \textbf{classical adjoint}\index{classical adjoint} of $A$,
and we denote it by $\func{adj} (A)$. 

In the specific case where $A$ is a $2 \times 2$-matrix given by
\begin{equation*}
A = \begin{mymatrix}{rr}
a & b \\
c & d
\end{mymatrix}
\end{equation*}
then $\func{adj}(A)$ is given by
\begin{equation*}
\func{adj}(A) = 
\begin{mymatrix}{rr}
d & -b \\
-c & a
\end{mymatrix}
\end{equation*}

In general, $\func{adj}(A)$ can always be found by taking the transpose of the cofactor matrix of $A$. The following theorem provides a formula for $A^{-1}$ using the determinant and adjugate of $A$.

\begin{theorem}{The inverse and the determinant}{inverse-and-determinant}
Let $A$ be an  $n\times n$-matrix. Then  
\begin{equation*}
A \; \func{adj}(A) = \func{adj}(A)A = {\det (A)} I
\end{equation*}

Moreover $A$ is invertible if and only if  $\det (A) \neq 0$. In this case\index{determinant!matrix inverse formula} we have: 
\begin{equation*}
A^{-1} = \frac{1}{\det (A)}  \func{adj}(A)
\end{equation*}
\end{theorem}

%The proof of this Theorem is below, after two examples demonstrating this concept. 

Notice that the first formula holds for any $n \times n$-matrix $A$, and in the case $A$ is invertible we actually have a formula for $A^{-1}$.

Consider the following example.

\begin{example}{Find inverse using the determinant}{inverse-and-determinant}
Find the inverse of the matrix
\begin{equation*}
A=\begin{mymatrix}{rrr}
1 & 2 & 3 \\
3 & 0 & 1 \\
1 & 2 & 1
\end{mymatrix}
\end{equation*}
using the formula in Theorem~\ref{thm:inverse-and-determinant}.
\end{example}

\begin{solution} 
According to Theorem~\ref{thm:inverse-and-determinant}, 
\begin{equation*}
A^{-1} = \frac{1}{\det (A)} \func{adj}(A)
\end{equation*}

First we will find the determinant of this matrix. Using Theorems~\ref{thm:switching-rows}, {\ref{thm:multiplying-row-by-scalar}},
and {\ref{thm:adding-multiple-of-row}}, we can first simplify the matrix through row operations. First, add $-3$ times the first row to the second row. Then
add $-1$ times the first row to the third row to obtain
\begin{equation*}
B = \begin{mymatrix}{rrr}
1 & 2 & 3 \\
0 & -6 & -8 \\
0 & 0 & -2
\end{mymatrix}
\end{equation*}
By Theorem~\ref{thm:adding-multiple-of-row}, $\det (A) = \det (B)$. By Theorem~\ref{thm:determinant-of-triangular-matrix}, 
$\det (B) = 1 \times -6 \times -2 = 12$. Hence, $\det (A) = 12$. 

Now, we need to find $\func{adj} (A)$. To do so, first we will find the cofactor matrix of $A$. 
This is given by
\begin{equation*}
\func{cof}(A) 
=
\begin{mymatrix}{rrr}
-2 & -2 & 6 \\
4 & -2 & 0 \\
2 & 8 & -6
\end{mymatrix} 
\end{equation*}
Here, the $\ijth$ entry is the $\ijth$ cofactor of the original matrix $A$ which you can verify. Therefore, from Theorem~\ref{thm:inverse-and-determinant},
 the inverse of $A$ is given by 
\begin{equation*}
A^{-1}
=
\vspace{0.05in}\frac{1}{12}\begin{mymatrix}{rrr}
-2 & -2 & 6 \\
4 & -2 & 0 \\
2 & 8 & -6
\end{mymatrix} ^{T}=\allowbreak \begin{mymatrix}{rrr}
-\vspace{0.05in}\frac{1}{6} & \vspace{0.05in}\frac{1}{3} & \vspace{0.05in}
\frac{1}{6} \\
-\vspace{0.05in}\frac{1}{6} & -\vspace{0.05in}\frac{1}{6} & \vspace{0.05in}
\frac{2}{3} \\
\vspace{0.05in}\frac{1}{2} & 0 & -\vspace{0.05in}\frac{1}{2}
\end{mymatrix} 
\end{equation*}

Remember that we can always verify our answer for $A^{-1}$. Compute the product $AA^{-1}$ and $A^{-1}A$ and make
sure each product is equal to $I$.

Compute $A^{-1}A$ as follows
\begin{equation*}
A^{-1}A = 
\allowbreak \begin{mymatrix}{rrr}
-\vspace{0.05in}\frac{1}{6} & \vspace{0.05in}\frac{1}{3} & \vspace{0.05in}
\frac{1}{6} \\
-\vspace{0.05in}\frac{1}{6} & -\vspace{0.05in}\frac{1}{6} & \vspace{0.05in}
\frac{2}{3} \\
\vspace{0.05in}\frac{1}{2} & 0 & -\vspace{0.05in}\frac{1}{2}
\end{mymatrix} \begin{mymatrix}{rrr}
1 & 2 & 3 \\
3 & 0 & 1 \\
1 & 2 & 1
\end{mymatrix} = \begin{mymatrix}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{mymatrix}
=
I
\end{equation*}
You can verify that $AA^{-1} = I$ and hence our answer is correct. 
\end{solution} 

We will look at another example of how to use this formula to find $A^{-1}$.

\begin{example}{Find the inverse from a formula}{inverse-formula}
Find the inverse of the matrix
\begin{equation*}
A=\begin{mymatrix}{rrr}
\vspace{0.05in}\frac{1}{2} & 0 & \vspace{0.05in}\frac{1}{2} \\
-\vspace{0.05in}\frac{1}{6} & \vspace{0.05in}\frac{1}{3} & -\vspace{0.05in}
\frac{1}{2} \\
-\vspace{0.05in}\frac{5}{6} & \vspace{0.05in}\frac{2}{3} & -\vspace{0.05in}
\frac{1}{2}
\end{mymatrix}
\end{equation*}
using the formula given in Theorem~\ref{thm:inverse-and-determinant}.
\end{example}

\begin{solution} First we need to find $\det (A)$. 
This step is left as an exercise and you should verify that $\det (A) = \vspace{0.05in}\frac{1}{6}$.
The inverse is therefore equal to
\begin{equation*}
A^{-1}
=
\frac{1}{(1/6)}\;
\func{adj} (A)
=
6\;
\func{adj} (A)
\end{equation*}

We continue to calculate as follows. Here we show the $2 \times 2$ determinants needed to find the cofactors. 
\begin{equation*}
A^{-1}
=
6\begin{mymatrix}{rrr}
\begin{absmatrix}{rr}
\vspace{0.05in}\frac{1}{3} & -\vspace{0.05in}\frac{1}{2} \\
\vspace{0.05in}\frac{2}{3} & -\vspace{0.05in}\frac{1}{2}
\end{absmatrix} & -\begin{absmatrix}{rr}
-\vspace{0.05in}\frac{1}{6} & -\vspace{0.05in}\frac{1}{2} \\
-\vspace{0.05in}\frac{5}{6} & -\vspace{0.05in}\frac{1}{2}
\end{absmatrix} & \begin{absmatrix}{rr}
-\vspace{0.05in}\frac{1}{6} & \vspace{0.05in}\frac{1}{3} \\
-\vspace{0.05in}\frac{5}{6} & \vspace{0.05in}\frac{2}{3}
\end{absmatrix} \\
-\begin{absmatrix}{rr}
0 & \vspace{0.05in}\frac{1}{2} \\
\vspace{0.05in}\frac{2}{3} & -\vspace{0.05in}\frac{1}{2}
\end{absmatrix} & \begin{absmatrix}{rr}
\vspace{0.05in}\frac{1}{2} & \vspace{0.05in}\frac{1}{2} \\
-\vspace{0.05in}\frac{5}{6} & -\vspace{0.05in}\frac{1}{2}
\end{absmatrix} & -\begin{absmatrix}{rr}
\vspace{0.05in}\frac{1}{2} & 0 \\
-\vspace{0.05in}\frac{5}{6} & \vspace{0.05in}\frac{2}{3}
\end{absmatrix} \\
\begin{absmatrix}{rr}
0 & \vspace{0.05in}\frac{1}{2} \\
\vspace{0.05in}\frac{1}{3} & -\vspace{0.05in}\frac{1}{2}
\end{absmatrix} & -\begin{absmatrix}{rr}
\vspace{0.05in}\frac{1}{2} & \vspace{0.05in}\frac{1}{2} \\
-\vspace{0.05in}\frac{1}{6} & -\vspace{0.05in}\frac{1}{2}
\end{absmatrix} & \begin{absmatrix}{rr}
\vspace{0.05in}\frac{1}{2} & 0 \\
-\vspace{0.05in}\frac{1}{6} & \vspace{0.05in}\frac{1}{3}
\end{absmatrix}
\end{mymatrix} ^{T}
\end{equation*}

Expanding all the $2\times 2$ determinants, this yields
\begin{equation*}
A^{-1}
=
6\begin{mymatrix}{rrr}
\vspace{0.05in}\frac{1}{6} & \vspace{0.05in}\frac{1}{3} & \vspace{0.05in}\frac{1}{6} \\
\vspace{0.05in}\frac{1}{3} & \vspace{0.05in}\frac{1}{6} & -\vspace{0.05in}\frac{1}{3} \\
-\vspace{0.05in}\frac{1}{6} & \vspace{0.05in}\frac{1}{6} & \vspace{0.05in}\frac{1}{6}
\end{mymatrix} ^{T}= \begin{mymatrix}{rrr}
1 & 2 & -1 \\
2 & 1 & 1 \\
1 & -2 & 1
\end{mymatrix}
\end{equation*}

Again, you can always check your work by multiplying $A^{-1}A$ and $AA^{-1}$ and ensuring these products equal $I$.
\begin{equation*}
A^{-1}A = 
\begin{mymatrix}{rrr}
1 & 2 & -1 \\
2 & 1 & 1 \\
1 & -2 & 1
\end{mymatrix} \begin{mymatrix}{rrr}
\vspace{0.05in}\frac{1}{2} & 0 & \vspace{0.05in}\frac{1}{2} \\
-\vspace{0.05in}\frac{1}{6} & \vspace{0.05in}\frac{1}{3} & -\vspace{0.05in}
\frac{1}{2} \\
-\vspace{0.05in}\frac{5}{6} & \vspace{0.05in}\frac{2}{3} & -\vspace{0.05in}
\frac{1}{2}
\end{mymatrix} = \begin{mymatrix}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{mymatrix}
\end{equation*}
This tells us that our calculation for $A^{-1}$ is correct. It is left to the reader to verify that $AA^{-1} = I$. 
\end{solution}

The verification step is very important, as it is a simple way to check your work! If you multiply $A^{-1}A$ and $AA^{-1}$ and
these products are not both equal to $I$, be sure to go back and double check each step. 
One common error is to forget to take the transpose of the cofactor matrix, so be sure to complete this step.

We will now prove Theorem~\ref{thm:inverse-and-determinant}.

%Ilijas' proof
\begin{proof} (of Theorem~\ref{thm:inverse-and-determinant})
Recall that the $(i,j)$-entry of $\func{adj}(A)$ is equal to $\cof{A}{ji}$.  Thus the $(i,j)$-entry of   $B=A\cdot \func{adj}(A)$ is : 
\[
B_{ij}=\sum_{k=1}^n a_{ik} \func{adj} (A)_{kj}= \sum_{k=1}^n a_{ik} \cof{A}{jk}
\]
By the cofactor expansion theorem, we see that this expression for $B_{ij}$ is equal to the determinant of the
matrix obtained from $A$ by replacing its $j$th  row  by $a_{i1}, a_{i2}, \dots a_{in}$ --- i.e., its $i$th row. 

If $i=j$ then this matrix is $A$ itself and therefore $B_{ii}=\det
A$. If on the other hand $i\neq j$, then this matrix has its $i$th row
equal to its $j$th row, and therefore $B_{ij}=0$ in his case. Thus we obtain: 
\begin{equation*}
A \; \func{adj}(A) = {\det (A)} I
\end{equation*}
Similarly we can verify that:
\begin{equation*}
\func{adj}(A)A = {\det (A)} I
\end{equation*}
And this proves the first part of the theorem. 

Further if $A$ is  invertible, then by Theorem~\ref{thm:determinant-of-product} we have:
\[ 1 = \det (I) = \det (A A^{-1}) = \det (A) \det (A^{-1}) \]
and thus $\det (A) \neq 0$. Equivalently, if  $\det (A) = 0$, then $A$ is not invertible. 

Finally if $\det (A) \neq 0$, then the above formula shows that $A$ is invertible and that:
\begin{equation*}
A^{-1} = \frac{1}{\det (A)} \func{adj}(A)
\end{equation*}

This completes the proof.
\end{proof}


%%the following is Kuttler's proof
%\begin{proof}
%First if $A$ is  invertible, then by Theorem~\ref{thm:determinant-of-product} we have:
%\[ 1 = \det (I) = \det (A A^{-1}) = \det (A) \det (A^{-1}) \]
%and thus $\det (A) \neq 0$. Equivalently, if  $\det (A) = 0$, then $A$ is not invertible. 
%
%
%Now assume $\det (A) \neq 0$ and let $A = \mat{a_{ij}}$. From the definition of the determinant
%in terms of expansion along column $r$ and multiplying by $\det (A)^{-1}$\index{inverses and determinants}, we get:
%
%\begin{equation*}
%\sum_{i=1}^{n}a_{ir}
%\cof{A}{ir}\det (A)^{-1}=\det (A)\det (A)^{-1}=1
%\end{equation*}
%Consider the following similar expression for $k\neq r$. 
%\begin{equation*}
%\sum_{i=1}^{n}a_{ir}\cof{A}{ik}\det (A)^{-1}
%\end{equation*}
%
%\noindent By replacing the $k\th$ column of $A$ with its $r\th$ column, we
%obtain a matrix $B_{k}$ whose determinant equals zero by Theorem
%\ref{thm:switching-rows}.  However, now expanding this matrix $B_{k}$ along
%the $k\th$ column yields
%\begin{equation*}
%0=\det (B_{k}) \det (A) ^{-1}=\sum_{i=1}^{n}a_{ir}
%\cof{A}{ik}\det (A) ^{-1}
%\end{equation*}
%Summarizing,
%\begin{equation*}
%\sum_{i=1}^{n}a_{ir}\cof{A}{ik}\det (A)
%^{-1}=\delta _{rk} = \left\{
%\begin{array}{c}
%1\text{ if }r=k \\
%0\text{ if }r\neq k
%\end{array}
%\right. 
%\end{equation*}
%Now notice that 
%\begin{equation*}
%\sum_{i=1}^{n}a_{ir}\cof{A}{ik}=\sum_{i=1}^{n}a_{ir}
%\cof{A}{ki}^{T}
%\end{equation*}
%which is the $kr\th$ entry of $\func{cof}(A) ^{T}A$. Therefore rewriting the above we obtain:
%\begin{equation}
%\vspace{0.05in}\frac{\func{cof}(A) ^{T}}{\det (A) }
%A=I  \label{inverse-proof-eq1}
%\end{equation}
%and hence 
%\begin{equation}
%\vspace{0.05in}\frac{\func{adj}(A)}{\det (A) } A=I
%\end{equation}
%Using the other formula in Definition~\ref{def:n-by-n-determinant}, and similar reasoning we obtain 
%
%\begin{equation}
%\vspace{0.05in}A \frac{\func{adj}(A)}{\det (A) } =I
%\end{equation}
%
%Thus $A$ is invertible when $\det (A) \neq 0$ and 
%
%\begin{equation*}
%A^{-1} = \frac{1}{\det (A)} \func{adj}(A)
%\end{equation*}
%and the proof is complete. 
%\end{proof}

This method for finding the inverse of $A$ is useful in many contexts. In particular, it 
is useful with complicated matrices where the entries are functions, rather than numbers.

Consider the following example. 

\begin{example}{Inverse for non-constant matrix}{inverse-non-constant-matrix}
Suppose
\begin{equation*}
A(t) =\begin{mymatrix}{ccc}
e^{t} & 0 & 0 \\
0 & \cos t & \sin t \\
0 & -\sin t & \cos t
\end{mymatrix}
\end{equation*}
Show that $A(t) ^{-1}$ exists and then find it.
\end{example}

\begin{solution} First note $\det (A(t)) =
e^{t}(\cos^2 t + \sin^2 t) = e^{t}\neq 0$ so $A(t) ^{-1}$
exists.

The cofactor matrix is
\begin{equation*}
C(t) =\begin{mymatrix}{ccc}
1 & 0 & 0 \\
0 & e^{t}\cos t & e^{t}\sin t \\
0 & -e^{t}\sin t & e^{t}\cos t
\end{mymatrix}
\end{equation*}
and so the inverse is
\begin{equation*}
\vspace{0.05in}\frac{1}{e^{t}}\begin{mymatrix}{ccc}
1 & 0 & 0 \\
0 & e^{t}\cos t & e^{t}\sin t \\
0 & -e^{t}\sin t & e^{t}\cos t
\end{mymatrix} ^{T}= \begin{mymatrix}{ccc}
e^{-t} & 0 & 0 \\
0 & \cos t & -\sin t \\
0 & \sin t & \cos t
\end{mymatrix} 
\end{equation*}

\end{solution}
