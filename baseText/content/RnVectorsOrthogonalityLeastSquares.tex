\subsection{Least squares approximation}

It should not be surprising to hear that many problems do not have a
perfect solution, and in these cases the objective is always to try to
do the best possible. For example what does one do if there are no
solutions to a system of linear equations $A\vect{x}=\vect{b}$? It
turns out that what we do is find $\vect{x}$ such that $A\vect{x}$ is
as close to $\vect{b}$ as possible. A very important technique that
follows from orthogonal projections is that of the least square
approximation\index{least square approximation}, and allows us to do
exactly that.

We begin with a lemma. 

Recall that we can form the image of an $m \times n$ matrix $A$ by
$\func{im}\tup{A} = = \set{A\vect{x} : \vect{x} \in
\R^n }$. Rephrasing Theorem \ref{thm:orth-proj} using
the subspace $W=\func{im}\tup{A}$  gives the
equivalence of an orthogonality condition with a minimization
condition\index{orthogonality and minimization}. The following picture
illustrates this orthogonality condition and geometric meaning of this
theorem.

\begin{center}
\begin{tikzpicture}
\draw[fill=lightgray](-3,0,2)--(-2,0.75,0)--(2,0.75,0)--(1,0,2)--(-3,0,2);
\draw(-2,0,0) rectangle (-1.8, 0.2,0);
\draw[thick,->](0,0,0)--(-2,0,0);
\draw[thick,->](0,0,0)--(-2,2,0);
\draw[red,thick,->](-2,2,0)--(-2,0,0);
\draw[thick,->](0,0,0)--(-0.5,0,1);
\draw[blue, thick, ->](-2,2,0)--(-0.5,0,1);
\node[right] at (0,0,0){$0$};
\node[above] at (-1,0,0){$\vect{z}=A\vect{x}$};
\node[above right] at (-1,1,0){$\vect{y}$};
\node[left] at (-2,1,0){$\vect{y}-\vect{z}$};
\node[right] at (-0.25,0,1){$\vect{u}$};
\node at (1,0.5,0){$A(\R^n)$};
\end{tikzpicture}
\end{center}

\begin{theorem}{Existence of minimizers}{existence-minimizer-hs}\label{existence-minimizer-hs}
Let $\vect{y}\in \R^{m}$ and let $A$ be an $m\times n$ matrix.

Choose $\vect{z}\in W= \func{im}\tup{A}$ given by $\vect{z} =
\func{proj}_{W}\tup{\vect{y}}$, and let $\vect{x} \in \R^{n}$ such that $\vect{z}=A\vect{x}$.

Then
\begin{enumerate}
\item $\vect{y} - A\vect{x} \in W^{\perp}$
\item $\norm{\vect{y} - A\vect{x}} < \norm{\vect{y} - \vect{u}}$ for all $\vect{u} \neq \vect{z} \in W$
\end{enumerate}
\end{theorem}

We note a simple but useful observation.

\begin{lemma}{Transpose and dot product}{transpose-and-dot-prod}
Let $A$ be an $m\times n$ matrix. Then 
\begin{equation*}
A\vect{x} \dotprod \vect{y} = \vect{x}\dotprod A^T\vect{y} 
\end{equation*}
\end{lemma}

\begin{proof}
This follows from the definitions:
\[ A\vect{x} \dotprod \vect{y}=\sum_{i,j}a_{ij}x_{j} y_{i}
=\sum_{i,j}x_{j} a_{ji} y_{i}= \vect{x} \dotprod A^T\vect{y}
\] \end{proof}

The next corollary gives the technique of least squares.

\begin{corollary}{Least squares and normal equation}{normal-equation}
A specific value of $\vect{x}$ which solves the problem of Theorem 
\ref{thm:existenceminimizerhs} is obtained by solving the equation
\begin{equation*}
A^TA\vect{x}=A^T\vect{y}
\end{equation*}
Furthermore, there always exists a solution to this system of equations.
\end{corollary}

\begin{proof} 
For $\vect{x}$ the minimizer of Theorem \ref{thm:existenceminimizerhs}, $\tup{
\vect{y}-A\vect{x}} \dotprod A \vect{u} =0$ for all $\vect{u} \in \R^{n}$ and from
Lemma \ref{lem:transpose-and-dot-prod}, this is the same as saying
\begin{equation*}
A^T\tup{\vect{y}-A\vect{x}} \dotprod \vect{u}=0
\end{equation*}
for all $u \in \R^{n}.$ This implies 
\begin{equation*}
A^T\vect{y}-A^TA\vect{x}=\vect{0}.
\end{equation*}
Therefore, there is a solution to the equation of this corollary, and it
solves the minimization problem of Theorem \ref{thm:existenceminimizerhs}.
\end{proof}

Note that $\vect{x}$ might not be unique but $A\vect{x}$, the closest
point of $A\tup{\R^{n}}$ to $\vect{y}$ is unique as was shown in the
above argument. 

Consider the following example. 

\begin{example}{Least squares solution to a system}{}
Find a least squares solution to the system 
\begin{equation*}
\begin{mymatrix}{rr}
2 & 1 \\ 
-1 & 3 \\ 
4 & 5
\end{mymatrix} \begin{mymatrix}{c}
x \\ 
y
\end{mymatrix} =\begin{mymatrix}{c}
2 \\ 
1 \\ 
1
\end{mymatrix}
\end{equation*}
\end{example}

\begin{solution}
First, consider whether there exists a real solution. To do so, set up the augmented matrix given by
\begin{equation*}
\begin{mymatrix}{rr|r}
2 & 1 & 2 \\ 
-1 & 3 & 1 \\ 
4 & 5 & 1
\end{mymatrix}
\end{equation*}
The {\rref} of this augmented matrix is 
\begin{equation*}
\begin{mymatrix}{rr|r}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1
\end{mymatrix}
\end{equation*}

It follows that there is no real solution to this system. Therefore we wish to find the least squares solution. The normal equations are 
\begin{eqnarray*}
A^T A \vect{x} &=& A^T \vect{y} \\
\begin{mymatrix}{rrr}
2 & -1 & 4 \\ 
1 & 3 & 5
\end{mymatrix} \begin{mymatrix}{rr}
2 & 1 \\ 
-1 & 3 \\ 
4 & 5
\end{mymatrix} \begin{mymatrix}{c}
x \\ 
y
\end{mymatrix} &=&\begin{mymatrix}{rrr}
2 & -1 & 4 \\ 
1 & 3 & 5
\end{mymatrix} \begin{mymatrix}{c}
2 \\ 
1 \\ 
1
\end{mymatrix}
\end{eqnarray*}
and so we need to solve the system 
\begin{equation*}
\begin{mymatrix}{rr}
21 & 19 \\ 
19 & 35
\end{mymatrix} \begin{mymatrix}{c}
x \\ 
y
\end{mymatrix} =\begin{mymatrix}{r}
7 \\ 
10
\end{mymatrix}
\end{equation*}
This is a familiar exercise and the solution is 
\begin{equation*}
\begin{mymatrix}{c}
x \\ 
y
\end{mymatrix} =\begin{mymatrix}{c}
\vspace{0.05in}\frac{5}{34} \\ 
\vspace{0.05in}\frac{7}{34}
\end{mymatrix}
\end{equation*}
\end{solution}

Consider another example. 

\begin{example}{Least squares solution to a system}{}
Find a least squares solution to the system 
\begin{equation*}
\begin{mymatrix}{rr}
2 & 1 \\ 
-1 & 3 \\ 
4 & 5
\end{mymatrix} \begin{mymatrix}{c}
x \\ 
y
\end{mymatrix} =\begin{mymatrix}{c}
3 \\ 
2 \\ 
9
\end{mymatrix}
\end{equation*}
\end{example}

\begin{solution}
First, consider whether there exists a real solution. To do so, set up the augmented matrix given by
\begin{equation*}
\begin{mymatrix}{rr|r}
2 & 1 & 3 \\ 
-1 & 3 & 2 \\ 
4 & 5 & 9
\end{mymatrix}
\end{equation*}
The {\rref} of this augmented matrix is 
\begin{equation*}
\begin{mymatrix}{rr|r}
1 & 0 & 1 \\ 
0 & 1 & 1 \\ 
0 & 0 & 0
\end{mymatrix}
\end{equation*}

It follows that the system has a solution given by $x=y=1$. However we can also use the normal equations and find
the least squares solution. 
\begin{equation*}
\begin{mymatrix}{rrr}
2 & -1 & 4 \\ 
1 & 3 & 5
\end{mymatrix} \begin{mymatrix}{rr}
2 & 1 \\ 
-1 & 3 \\ 
4 & 5
\end{mymatrix} \begin{mymatrix}{c}
x \\ 
y
\end{mymatrix} =\begin{mymatrix}{rrr}
2 & -1 & 4 \\ 
1 & 3 & 5
\end{mymatrix} \begin{mymatrix}{r}
3 \\ 
2 \\ 
9
\end{mymatrix}
\end{equation*}
Then
\begin{equation*}
\begin{mymatrix}{rr}
21 & 19 \\ 
19 & 35
\end{mymatrix} \begin{mymatrix}{c}
x \\ 
y
\end{mymatrix} =\begin{mymatrix}{c}
40 \\ 
54
\end{mymatrix}
\end{equation*}

The least squares solution is  
\begin{equation*}
\begin{mymatrix}{c}
x \\ 
y
\end{mymatrix} =\begin{mymatrix}{c}
1 \\ 
1
\end{mymatrix}
\end{equation*}
which is the same as the solution found above. 
\end{solution}

An important application of Corollary \ref{cor:normal-equation} is the
problem of finding the least squares regression
line\index{regression line} in statistics.  Suppose you are given points in the $xy$ plane
\begin{equation*}
\set{\tup{x_{1},y_{1}},  \tup{x_{2},y_{2}}, \cdots,  \tup{x_{n},y_{n}}   }
\end{equation*}
and you would like to find constants $m$ and $b$ such that the line $\vect{y}=m\vect{x}+b$
goes through all these points. Of course this will be impossible in general.
Therefore, we try to find $m,b$ such that the line will be as close as possible. The desired system
is 
\begin{equation*}
\begin{mymatrix}{c}
y_{1} \\ 
\vdots \\ 
y_{n}
\end{mymatrix} =\begin{mymatrix}{cc}
x_{1} & 1 \\ 
\vdots & \vdots \\ 
x_{n} & 1
\end{mymatrix} \begin{mymatrix}{c}
m \\ 
b
\end{mymatrix}
\end{equation*}
which is of the form $\vect{y}=A\vect{x}$. It is desired to choose $m$
and $b$ to make 
\begin{equation*}
\left\norm{A\begin{mymatrix}{c}
m \\ 
b
\end{mymatrix} -\begin{mymatrix}{c}
y_{1} \\ 
\vdots \\ 
y_{n}
\end{mymatrix} \right} ^{2}
\end{equation*}
as small as possible. According to Theorem \ref{thm:existenceminimizerhs} and
Corollary \ref{cor:normal-equation}, the best values for $m$ and $b$ occur as the
solution to 
\begin{equation*}
A^{T}A\begin{mymatrix}{c}
m \\ 
b
\end{mymatrix} =A^{T}\begin{mymatrix}{c}
y_{1} \\ 
\vdots \\ 
y_{n}
\end{mymatrix} ,\ \;\mbox{where}\; A=\begin{mymatrix}{cc}
x_{1} & 1 \\ 
\vdots & \vdots \\ 
x_{n} & 1
\end{mymatrix} 
\end{equation*}
Thus, computing $A^{T}A,$ 
\begin{equation*}
\begin{mymatrix}{cc}
\sum_{i=1}^{n}x_{i}^{2} & \sum_{i=1}^{n}x_{i} \\ 
\sum_{i=1}^{n}x_{i} & n
\end{mymatrix} \begin{mymatrix}{c}
m \\ 
b
\end{mymatrix} =\begin{mymatrix}{c}
\sum_{i=1}^{n}x_{i}y_{i} \\ 
\sum_{i=1}^{n}y_{i}
\end{mymatrix}
\end{equation*}
Solving this system of equations for $m$ and $b$ (using Cramer's rule for example) yields: 
\begin{equation*}
m=
\frac{-\tup{\sum_{i=1}^{n}x_{i}} \tup{\sum_{i=1}^{n}y_{i}}
+\tup{\sum_{i=1}^{n}x_{i}y_{i}} n}{\tup{
\sum_{i=1}^{n}x_{i}^{2}} n-\tup{\sum_{i=1}^{n}x_{i}} ^{2}}
\end{equation*}
and 
\begin{equation*}
b=\frac{-\tup{\sum_{i=1}^{n}x_{i}} \sum_{i=1}^{n}x_{i}y_{i}+\tup{
\sum_{i=1}^{n}y_{i}} \sum_{i=1}^{n}x_{i}^{2}}{\tup{
\sum_{i=1}^{n}x_{i}^{2}} n-\tup{\sum_{i=1}^{n}x_{i}} ^{2}}.
\end{equation*}

Consider the following example.

\begin{example}{Least squares regression line}{least-squares-line}
Find the least squares regression line $\vect{y}=m\vect{x}+b$ for the following set of data points:
\[ \set{(0,1), (1,2), (2,2), (3,4), (4,5) } \]
\end{example}

\begin{solution}
In this case we have $n=5$ data points and we obtain:
\begin{equation*}
\begin{array}{ll}
\sum_{i=1}^{5}x_{i} = 10 & \sum_{i=1}^{5}y_{i} =  14 \\
\\
\sum_{i=1}^{5}x_{i}y_{i}  =  38 & \sum_{i=1}^{5}x_{i}^{2}  =   30\\
\end{array}
\end{equation*}
and hence
\begin{eqnarray*}
m &=& \frac{- 10 * 14 + 5*38}{5*30-10^2} = 1.00 \\
\\
b &=& \frac{- 10 * 38 + 14*30}{5*30-10^2} = 0.80 \\
\end{eqnarray*}

The  least squares regression line for the set of data points is:
\[ \vect{y} = \vect{x}+.8 \] 

One could use this line to approximate other values for the data. For
example for $x=6$ one could use $y(6)=6+.8=6.8$ as an approximate
value for the data.

The following diagram shows the data points and the corresponding regression line.

\begin{center}
\begin{tikzpicture}
\begin{axis}[
	xlabel=x,
	ylabel=y,
	xmin = -1,
	xmax = 5,
	ymin = -1,
	ymax = 6,
	axis lines = center,
	axis on top = true,
	domain = 0:5,
	legend style = {at={(1.03,-0.03)}},
	%legend style = {legend pos=south east}
	]

	\addplot [mark =none,color=blue, ultra thick] {x + 0.8};
	\addlegendentry{Regression Line}
	\addplot coordinates{
	(0,1)
	(1,2)
	(2,2)
	(3,4)
	(4,5)
	};
	\addlegendentry{Data Points}
\end{axis}
\end{tikzpicture}
\end{center}
\end{solution}

One could clearly do a least squares fit for curves of the form $
y=ax^{2}+bx+c$ in the same way. In this case you want to solve as well as
possible for $a,b,$ and $c$ the system 
\begin{equation*}
\begin{mymatrix}{ccc}
x_{1}^{2} & x_{1} & 1 \\ 
\vdots & \vdots & \vdots \\ 
x_{n}^{2} & x_{n} & 1
\end{mymatrix} \begin{mymatrix}{c}
a \\ 
b \\ 
c
\end{mymatrix} =\begin{mymatrix}{c}
y_{1} \\ 
\vdots \\ 
y_{n}
\end{mymatrix}
\end{equation*}
and one would use the same technique as above. Many other similar problems
are important, including many in higher dimensions and they are all solved
the same way.
