\subsection{Orthogonal diagonalization}

We begin this section by recalling some important definitions. Recall from Definition~\ref{def:orth-set} that non-zero vectors are called orthogonal if their dot product equals $0$.  A set is orthonormal if it is orthogonal and each vector is a unit vector. 

An orthogonal matrix $U$, from Definition~\ref{def:Ortho-Matrix}, is one in which $UU^{T} = I$. In other words, the transpose of an orthogonal matrix is equal to its inverse. A key characteristic of orthogonal matrices, which will be essential in this section, is that the columns of an orthogonal matrix form an orthonormal set. 

We now recall another important definition. 

\begin{definition}{Symmetric and antisymmetric matrices}{symmetric-antisymmetric-matrices}
A real $n\times n$-matrix $A$, is \textbf{symmetric }if $A^{T}=A$. If $%
A=-A^{T}$, then $A$ is called \textbf{antisymmetric. }
\index{symmetric matrix}
\end{definition}

Before proving an essential theorem, we first examine the following lemma which will be used below.

\begin{lemma}{The dot product}{dotproduct-lemma}
Let $A=\mat{a_{ij} }$ be a real symmetric $n \times n$-matrix, and let $\vect{x}, \vect{y} \in \R^n$. Then
\[
A\vect{x} \dotprod \vect{y} = \vect{x} \dotprod A \vect{y}
\]
\end{lemma}

\begin{proof}
This result follows from the definition of the dot product together with properties of matrix multiplication, as follows:
\begin{eqnarray*}
A\vect{x} \dotprod \vect{y} &=& \sum_{k,l}a_{kl}x_{l}y_{k} \\
&=&\sum_{k,l} (a_{lk})^Tx_{l}y_{k} \\
&=& \vect{x}\dotprod A^{T}\vect{y} \\
&=& \vect{x}\dotprod A \vect{y}
\end{eqnarray*}

The last step follows from $A^T = A$, since $A$ is symmetric. 
\end{proof}

We can now prove that the eigenvalues of a real symmetric matrix are real numbers.  Consider the following important theorem. 

\begin{theorem}{Orthogonal eigenvectors}{symmetric-orthogonal-eigenvectors}
Let $A$ be a real symmetric matrix. Then the eigenvalues of $A$ are real numbers and eigenvectors corresponding to distinct eigenvalues are orthogonal. 
\end{theorem}

\begin{proof}
Recall that for a complex number $a+ib$, the complex conjugate, denoted by $
\overline{a+ib}$ is given by $\overline{a+ib}=a-ib$. The
notation, $\overline{\vect{x}}$ will denote the vector which has every
entry replaced by its complex conjugate.

Suppose $A$ is a real symmetric matrix and $A\vect{x}=\lambda \vect{x}$.
Then 
\begin{equation*}
\overline{\lambda }\overline{\vect{x}}^{T}\vect{x}=\tup{\overline{A
\vect{x}}} ^{T}\vect{x}=\overline{\vect{x}}^{T}A^{T}\vect{x}=
\overline{\vect{x}}^{T}A\vect{x}=\lambda \overline{\vect{x}}^{T}
\vect{x}
\end{equation*}
Dividing by $\overline{\vect{x}}^{T}\vect{x}$ on both sides yields $
\overline{\lambda }=\lambda $ which says $\lambda $ is real. To do this, we need to ensure that $\overline{\vect{x}}^{T}\vect{x} \neq 0$. Notice that $\overline{\vect{x}}^{T}\vect{x} = 0$ if and only if $\vect{x} = \vect{0}$. Since we chose $\vect{x}$ such that $A\vect{x} = \lambda \vect{x}$, $\vect{x}$ is an eigenvector and therefore must be non-zero.  

Now suppose $A$ is real symmetric and $A\vect{x}=\lambda \vect{x}$, $A
\vect{y}=\mu \vect{y}$ where $\mu \neq \lambda$. Then since $A$
is symmetric, it follows from Lemma~\ref{lem:dotproduct-lemma} about the dot product that 
\begin{equation*}
\lambda \vect{x}\dotprod \vect{y}=A\vect{x}\dotprod \vect{y}=\vect{x}\dotprod A\vect{y}=\vect{x}\dotprod \mu \vect{y}=\mu \vect{x}\dotprod \vect{y}
\end{equation*}
Hence $\tup{\lambda -\mu } \vect{x}\dotprod \vect{y}=0$. It follows that,
since $\lambda -\mu \neq 0$, it must be that $\vect{x}\dotprod \vect{y}=0$. Therefore the eigenvectors form an orthogonal set. 
\end{proof}

The following theorem is proved in a similar manner.

\begin{theorem}{Eigenvalues of antisymmetric matrix}{antisymmetric-eigen}
The eigenvalues of a real antisymmetric matrix are either equal to $0$ or are pure imaginary numbers.
\end{theorem}

\begin{proof}
First, note that if $A=0$ is the zero matrix, then $A$ is antisymmetric and has eigenvalues equal to $0$. 

Suppose $A=-A^{T}$ so $A$ is antisymmetric and $A\vect{x}=\lambda 
\vect{x}$. Then 
\begin{equation*}
\overline{\lambda \vect{x}}^{T}\vect{x}=\tup{\overline{A
\vect{x}}} ^{T}\vect{x}=\overline{\vect{x}}^{T}A^{T}\vect{x}=-
\overline{\vect{x}}^{T}A\vect{x}=-\lambda \overline{\vect{x}}^{T}
\vect{x}
\end{equation*}
and so, dividing by $\overline{\vect{x}}^{T}\vect{x}$ as before, $
\overline{\lambda }=-\lambda$. Letting $\lambda =a+ib$, this means $
a-ib=-a-ib$ and so $a=0$. Thus $\lambda $ is pure imaginary. 
\end{proof}

Consider the following example. 

\begin{example}{Eigenvalues of an antisymmetric matrix}{antisymmetric-eigenvalues}
Let $A=\begin{mymatrix}{rr}
0 & -1 \\
1 & 0
\end{mymatrix}$.  Find its eigenvalues.
\end{example}

\begin{solution}
First notice that $A$ is antisymmetric. By Theorem~\ref{thm:antisymmetric-eigen}, the eigenvalues will either equal $0$ or be pure imaginary.  The eigenvalues of $A$ are obtained by solving the usual equation 
\[
\det (\eigenVar I - A ) = 
\det \begin{mymatrix}{rr}
\eigenVar & 1 \\ 
-1 & \eigenVar
\end{mymatrix} =\eigenVar ^{2}+1=0
\]

Hence the eigenvalues are $\pm i$, pure
imaginary.
\end{solution}

Consider the following example.

\begin{example}{Eigenvalues of a symmetric matrix}{eigenvalues-symmetric}
Let $A=\begin{mymatrix}{rr}
1 & 2 \\
2 & 3
\end{mymatrix}$. Find its eigenvalues.
\end{example}

\begin{solution}
First, notice that $A$ is symmetric. By Theorem~\ref{thm:symmetric-orthogonal-eigenvectors}, the eigenvalues will all be real. The eigenvalues of $A$ are obtained by solving the usual equation 
\[
\det (\eigenVar I - A) = 
\det \begin{mymatrix}{rr}
\eigenVar - 1 & -2 \\ 
-2 & \eigenVar - 3 
\end{mymatrix} = \eigenVar^2 -4\eigenVar -1=0
\]
The eigenvalues are given by $\lambda_1 =2+
\sqrt{5}$ and $\lambda_2 =2-\sqrt{5}$ which are both real. 
\end{solution}

Recall that a diagonal matrix $D=\mat{d_{ij} }$ is one in which $d_{ij} = 0$ whenever $i \neq j$. In other words, all numbers not on the main diagonal are equal to zero. 

Consider the following important theorem.

\begin{theorem}{Orthogonal diagonalization}{orth-diag}
Let $A$ be a real symmetric matrix. Then there exists an
orthogonal matrix $U$ such that 
\[
U^{T}AU = D
\]
where $D$ is a diagonal matrix. Moreover,
the diagonal entries of $D$ are the eigenvalues of $A$.
\end{theorem}

We can use this theorem to diagonalize a symmetric matrix, using orthogonal matrices. Consider the following corollary.

\begin{corollary}{Orthonormal set of eigenvectors}{orthonormal-eigenvectors}
If $A$ is a real $n\times n$ symmetric matrix, then there exists an
orthonormal set of eigenvectors, $\set{\vect{u}_{1},\ldots,\vect{u}
_{n}}$.
\end{corollary}

\begin{proof}
Since $A$ is symmetric, then by Theorem~\ref{thm:orth-diag},
there exists an orthogonal matrix $U$ such that $U^{T}AU=D$, a diagonal
matrix whose diagonal entries are the eigenvalues of $A$. Therefore, since $
A $ is symmetric and all the matrices are real, 
\begin{equation*}
\overline{D}=\overline{D^{T}}=\overline{U^{T}A^{T}U}=U^{T}A^{T}U=U^{T}AU=D
\end{equation*}
showing $D$ is real because each entry of $D$ equals its complex conjugate.

Now let 
\begin{equation*}
U=\begin{mymatrix}{cccc}
\vect{u}_{1} & \vect{u}_{2} & \cdots & \vect{u}_{n}
\end{mymatrix}
\end{equation*}
where the $\vect{u}_{i}$ denote the columns of $U$ and 
\begin{equation*}
D=\begin{mymatrix}{ccc}
\lambda _{1} &  & 0 \\ 
& \ddots &  \\ 
0 &  & \lambda _{n}
\end{mymatrix}
\end{equation*}
The equation, $U^{T}AU=D$ implies $AU = UD$ and 
\begin{eqnarray*}
AU &=&\begin{mymatrix}{cccc}
A\vect{u}_{1} & A\vect{u}_{2} & \cdots & A\vect{u}_{n}%
\end{mymatrix} \\
&=&\begin{mymatrix}{cccc}
\lambda _{1}\vect{u}_{1} & \lambda _{2}\vect{u}_{2} & \cdots & \lambda
_{n}\vect{u}_{n}
\end{mymatrix} \\
&=& UD
\end{eqnarray*}
where the entries denote the columns of $AU$ and $UD$ respectively.
Therefore, $A\vect{u}_{i}=\lambda _{i}\vect{u}_{i}$.  Since the matrix $U$
is orthogonal, the $ij^{th}$ entry of $U^{T}U$ equals $\delta _{ij}$ and so 
\begin{equation*}
\delta _{ij}=\vect{u}_{i}^{T}\vect{u}_{j}=\vect{u}_{i}\dotprod \vect{u}
_{j}
\end{equation*}
This proves the corollary because it shows the vectors $\set{\vect{u}
_{i}} $ form an orthonormal set.
\end{proof}

\begin{definition}{Principal axes}{principal-axes}
Let $A$ be an $n \times n$-matrix. Then the principal axes of $A$ is a set of orthonormal eigenvectors of $A$.
\index{principal axes}
\end{definition}

In the next example, we examine how to find such a set of orthonormal eigenvectors.

\begin{example}{Find an orthonormal set of eigenvectors}{orth-basis-eigenvectors}
Find an orthonormal set of eigenvectors for the symmetric matrix
\begin{equation*}
A = \begin{mymatrix}{rrr}
17 & -2 & -2 \\
-2 & 6 & 4 \\
-2 & 4 & 6
\end{mymatrix}
\end{equation*}
\end{example}

\begin{solution}
Recall Procedure~\ref{proc:find-eigenvalues-vectors} for finding the eigenvalues and eigenvectors of a matrix. You can verify that the eigenvalues are $18,9,2$. First find the eigenvector for $18$ by solving the equation $(18I-A)X = 0$. 
The appropriate augmented matrix is given by  
\begin{equation*}
\begin{mymatrix}{ccc|c}
18-17 & 2 & 2 & 0 \\ 
2 & 18-6 & -4 & 0 \\ 
2 & -4 & 18-6 & 0
\end{mymatrix}
\end{equation*}
The {\rref} is 
\begin{equation*}
\begin{mymatrix}{rrr|r}
1 & 0 & 4 & 0 \\ 
0 & 1 & -1 & 0 \\ 
0 & 0 & 0 & 0
\end{mymatrix}
\end{equation*}
Therefore an eigenvector is 
\begin{equation*}
\begin{mymatrix}{r}
-4 \\ 
1 \\ 
1
\end{mymatrix}
\end{equation*}
Next find the eigenvector for $\lambda =9$. The augmented matrix and resulting {\rref} are 
\begin{equation*}
\begin{mymatrix}{ccc|c}
9-17 & 2 & 2 & 0 \\ 
2 & 9-6 & -4 & 0 \\ 
2 & -4 & 9-6 & 0
\end{mymatrix}
\sim\ldots\sim
\begin{mymatrix}{rrr|r}
1 & 0 & -\vspace{0.05in}\frac{1}{2} & 0 \\ 
0 & 1 & -1 & 0 \\ 
0 & 0 & 0 & 0
\end{mymatrix}
\end{equation*}
Thus an eigenvector for $\lambda =9$ is 
\begin{equation*}
\begin{mymatrix}{r}
1 \\ 
2 \\ 
2
\end{mymatrix}
\end{equation*}
Finally find an eigenvector for $\lambda =2$. The appropriate augmented
matrix and {\rref} are 
\begin{equation*}
\begin{mymatrix}{ccc|c}
2-17 & 2 & 2 & 0 \\ 
2 & 2-6 & -4 & 0 \\ 
2 & -4 & 2-6 & 0
\end{mymatrix}
\sim\ldots\sim
\begin{mymatrix}{rrr|r}
1 & 0 & 0 & 0 \\ 
0 & 1 & 1 & 0 \\ 
0 & 0 & 0 & 0
\end{mymatrix}
\end{equation*}
Thus an eigenvector for $\lambda =2$ is 
\begin{equation*}
\begin{mymatrix}{r}
0 \\ 
-1 \\ 
1
\end{mymatrix}
\end{equation*}

The set of eigenvectors for $A$ is given by 
\[
\set{
\begin{mymatrix}{r}
-4 \\ 
1 \\ 
1
\end{mymatrix},
\begin{mymatrix}{r}
1 \\ 
2 \\ 
2
\end{mymatrix},
\begin{mymatrix}{r}
0 \\ 
-1 \\ 
1
\end{mymatrix}
}
\]
You can verify that these eigenvectors form an orthogonal set. By dividing each eigenvector by its magnitude, we obtain an orthonormal set: 
\begin{equation*}
\set{\frac{1}{\sqrt{18}}\begin{mymatrix}{r}
-4 \\ 
1 \\ 
1
\end{mymatrix} ,\frac{1}{3}\begin{mymatrix}{r}
1 \\ 
2 \\ 
2
\end{mymatrix} ,\frac{1}{\sqrt{2}}\begin{mymatrix}{r}
0 \\ 
-1 \\ 
1
\end{mymatrix} }
\end{equation*}
\end{solution}

Consider the following example.

\begin{example}{Repeated eigenvalues}{}
Find an orthonormal set of three eigenvectors for the matrix
\begin{equation*}
A = \begin{mymatrix}{rrr}
10 & 2 & 2 \\
2 & 13 & 4 \\
2 & 4 & 13
\end{mymatrix}
\end{equation*}
\end{example}

\begin{solution}
You can verify that the eigenvalues of $A$ are $9$ (with multiplicity two) and $18$ (with multiplicity one). Consider the
eigenvectors corresponding to $\lambda =9$. The appropriate augmented matrix
and {\rref} are given by  
\begin{equation*}
\begin{mymatrix}{ccc|c}
9-10 & -2 & -2 & 0 \\ 
-2 & 9-13 & -4 & 0 \\ 
-2 & -4 & 9-13 & 0
\end{mymatrix}
\sim\ldots\sim 
\begin{mymatrix}{rrr|r}
1 & 2 & 2 & 0 \\ 
0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0
\end{mymatrix}
\end{equation*}
and so eigenvectors are of the form 
\begin{equation*}
\begin{mymatrix}{c}
-2y-2z \\ 
y \\ 
z
\end{mymatrix}
\end{equation*}
We need to find two of these which are orthogonal. Let one be given by setting $z=0$ and $y=1$, giving $
\begin{mymatrix}{r}
-2 \\ 
1 \\ 
0
\end{mymatrix}$. 

In order to find an eigenvector orthogonal to this one, we need to satisfy 
\begin{equation*}
\begin{mymatrix}{r}
-2 \\ 
1 \\ 
0
\end{mymatrix} \dotprod \begin{mymatrix}{c}
-2y-2z \\ 
y \\ 
z
\end{mymatrix} =5y+4z=0
\end{equation*}
The values $y=-4$ and $z=5$ satisfy this equation, giving another eigenvector
corresponding to $\lambda=9$ as 
\begin{equation*}
\begin{mymatrix}{c}
-2\tup{-4} -2\tup{5} \\ 
\tup{-4} \\ 
5
\end{mymatrix} =\begin{mymatrix}{r}
-2 \\ 
-4 \\ 
5
\end{mymatrix}
\end{equation*}
Next find the eigenvector for $\lambda =18$. The augmented matrix and the resulting {\rref} are given by  
\begin{equation*}
\begin{mymatrix}{ccc|c}
18-10 & -2 & -2 & 0 \\ 
-2 & 18-13 & -4 & 0 \\ 
-2 & -4 & 18-13 & 0
\end{mymatrix}
\sim\ldots\sim
\begin{mymatrix}{rrr|r}
1 & 0 & -\vspace{0.05in}\frac{1}{2} & 0 \\ 
0 & 1 & -1 & 0 \\ 
0 & 0 & 0 & 0
\end{mymatrix}
\end{equation*}
and so an eigenvector is 
\begin{equation*}
\begin{mymatrix}{r}
1 \\ 
2 \\ 
2
\end{mymatrix}
\end{equation*}

Dividing each eigenvector by its length, the orthonormal set is 
\begin{equation*}
\set{\frac{1}{\sqrt{5}} \begin{mymatrix}{r}
-2\\ 
1 \\ 
0
\end{mymatrix} , \frac{\sqrt{5}}{15} \begin{mymatrix}{r}
-2 \\ 
-4 \\ 
5
\end{mymatrix} , \frac{1}{3}\begin{mymatrix}{r}
1 \\ 
2 \\ 
2
\end{mymatrix} }
\end{equation*}

\end{solution}

In the above solution, the repeated eigenvalue implies that there would have been many other
orthonormal bases which could have been obtained. While we chose to
take $z=0, y=1$, we could just as easily have taken $y=0$
or even $y=z=1$. Any such change would have resulted in a different
orthonormal set. 

Recall the following definition.

\begin{definition}{Diagonalizable}{non-defective}
An $n\times n$-matrix $A$ is said to be \textbf{non-defective}
\index{non-defective}or \textbf{diagonalizable}
\index{diagonalizable} if there exists an invertible matrix $P$ such that $
P^{-1}AP=D$ where $D$ is a diagonal matrix.
\end{definition}

As indicated in Theorem~\ref{thm:orth-diag} if $A$ is a real symmetric matrix, there exists an
orthogonal matrix $U$ such that $U^{T}AU=D$ where $D$ is a diagonal matrix. Therefore,
every symmetric matrix is diagonalizable because if $U$ is an orthogonal
matrix, it is invertible and its inverse is $U^{T}$. In this case, we say that $A$ is \textbf{orthogonally diagonalizable}\index{orthogonally diagonalizable}. Therefore every symmetric matrix is in fact orthogonally diagonalizable. The next theorem provides another way to determine if a matrix is orthogonally diagonalizable. 

\begin{theorem}{Orthogonally diagonalizable}{orth-diagonalizable}
Let $A$ be an $n \times n$-matrix. Then $A$ is orthogonally diagonalizable if and only if $A$ has an orthonormal set of eigenvectors. 
\end{theorem}

Recall from Corollary~\ref{cor:orthonormal-eigenvectors} that every symmetric matrix has an orthonormal set of eigenvectors. In fact these three conditions are equivalent. 

In the following example, the orthogonal
matrix $U$ will be found to orthogonally diagonalize a matrix. 

\begin{example}{Diagonalize a symmetric matrix}{orth-diag}
Let $A=\begin{mymatrix}{rrr}
1 & 0 & 0 \\
0 &
\vspace{0.05in}\frac{3}{2} & \vspace{0.05in}\frac{1}{2} \\
0 & \vspace{0.05in}\frac{1}{2} & \vspace{0.05in}\frac{3}{2}
\end{mymatrix}$. Find an orthogonal matrix $U$ such that $U^{T}AU$ is a diagonal
matrix.
\end{example}

\begin{solution}
In this case, the eigenvalues are $2$ (with multiplicity one) and $1$ (with multiplicity two). First
we will find an eigenvector for the eigenvalue $2$. The appropriate augmented matrix and resulting {\rref} are given by 
\begin{equation*}
\begin{mymatrix}{ccc|c}
2-1 & 0 & 0 &  0 \\ 
0 & 2-\vspace{0.05in}\frac{3}{2} & -\vspace{0.05in}\frac{1}{2} & 0 \\ 
0 & -\vspace{0.05in}\frac{1}{2} & 2-\vspace{0.05in}\frac{3}{2} &  0
\end{mymatrix}
\sim\ldots\sim
\begin{mymatrix}{rrr|r}
1 & 0 & 0 & 0 \\ 
0 & 1 & -1 & 0 \\ 
0 & 0 & 0 & 0
\end{mymatrix}
\end{equation*}
and so an eigenvector is 
\begin{equation*}
\begin{mymatrix}{r}
0 \\ 
1 \\ 
1
\end{mymatrix} 
\end{equation*}
However, it is desired that the eigenvectors be unit vectors
and so dividing this vector by its length gives 
\begin{equation*}
\begin{mymatrix}{c}
0 \\ 
\vspace{0.05in}\frac{1}{\sqrt{2}} \\ 
\vspace{0.05in}\frac{1}{\sqrt{2}}
\end{mymatrix}
\end{equation*}
Next find the eigenvectors corresponding to the eigenvalue equal to $1$. The appropriate augmented matrix and resulting {\rref} are given by:
\begin{equation*}
\begin{mymatrix}{ccc|c}
1-1 & 0 & 0 & 0 \\ 
0 & 1-\vspace{0.05in}\frac{3}{2} & -\vspace{0.05in}\frac{1}{2} & 0 \\ 
0 & -\vspace{0.05in}\frac{1}{2} & 1-\vspace{0.05in}\frac{3}{2} & 0
\end{mymatrix}
\sim\ldots\sim
\begin{mymatrix}{rrr|r}
0 & 1 & 1 & 0 \\ 
0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0
\end{mymatrix}
\end{equation*}
Therefore, the eigenvectors are of the form 
\begin{equation*}
\begin{mymatrix}{r}
s \\ 
-t \\ 
t
\end{mymatrix}
\end{equation*}
Two of these which are orthonormal are $\begin{mymatrix}{c}
1 \\ 
0 \\ 
0
\end{mymatrix}$, choosing $s=1$ and $t=0$, and $\begin{mymatrix}{c}
0 \\ 
-\vspace{0.05in}\frac{1}{\sqrt{2}} \\ 
\vspace{0.05in}\frac{1}{\sqrt{2}}
\end{mymatrix}$, letting $s=0$, $t= 1 $ and normalizing the resulting vector.

To obtain the desired orthogonal matrix, we let the orthonormal eigenvectors computed above be the columns. 
\begin{equation*}
\begin{mymatrix}{rrr}
0 & 1 & 0 \\ 
-\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}} \\ 
\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}
\end{mymatrix}
\end{equation*}

To verify, compute $U^{T}AU$ as follows:
\begin{equation*}
U^{T}AU = 
\begin{mymatrix}{rrr}
0 & -\vspace{0.05in}\frac{1}{\sqrt{2}} & \vspace{0.05in}\frac{1}{\sqrt{2}}
\\ 
1 & 0 & 0 \\ 
0 & \vspace{0.05in}\frac{1}{\sqrt{2}} & \vspace{0.05in}\frac{1}{\sqrt{2}}
\end{mymatrix} \begin{mymatrix}{rrr}
1 & 0 & 0 \\ 
0 & \vspace{0.05in}\frac{3}{2} & \vspace{0.05in}\frac{1}{2} \\ 
0 & \vspace{0.05in}\frac{1}{2} & \vspace{0.05in}\frac{3}{2}
\end{mymatrix} \begin{mymatrix}{rrr}
0 & 1 & 0 \\ 
-\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}} \\ 
\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}}
\end{mymatrix}
\end{equation*}
\begin{equation*}
=\begin{mymatrix}{rrr}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 2
\end{mymatrix} = D 
\end{equation*}
the desired diagonal matrix. Notice that the eigenvectors, which construct the columns of $U$, are in the same order as the eigenvalues in $D$. 
\end{solution}

We conclude this section with a Theorem that generalizes earlier results.

\begin{theorem}{Triangulation of a matrix}{triangulation}
Let $A$ be an $n \times n$-matrix. If $A$ has $n$ real eigenvalues, then an orthogonal matrix $U$ can be found to result in the upper triangular matrix $U^T A U$. 
\end{theorem}

This Theorem provides a useful Corollary.

\begin{corollary}{Determinant and trace}{det-trace}
Let $A$ be an $n \times n$-matrix with eigenvalues $\lambda_1,\ldots, \lambda_n$. Then it follows that $\det(A)$ is equal to the product of the $\lambda_1$, while $\func{trace}(A)$ is equal to the sum of the $\lambda_i$. 
\end{corollary}

\begin{proof}
By Theorem~\ref{thm:triangulation}, there exists an orthogonal matrix $U$ such
that $U^TAU=P$, where $P$ is an upper triangular matrix.
Since $P$ is similar to $A$, the eigenvalues
of $P$ are $\lambda_1, \lambda_2, \ldots, \lambda_n$.
Furthermore, since $P$ is (upper) triangular, the entries on the
main diagonal of $P$ are its eigenvalues, so
$\det(P)=\lambda_1 \lambda_2 \cdots \lambda_n$ and
$\func{trace}(P)=\lambda_1 + \lambda_2 + \ldots + \lambda_n$.
Since $P$ and $A$ are similar, $\det(A)=\det(P)$ and $\func{trace}(A)=\func{trace}(P)$,
and therefore the results follow.
\end{proof}
